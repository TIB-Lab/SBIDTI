{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb18ae19-e0fe-4709-ab21-3e37904bb7f2",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9519e2c1-0012-4d7a-b061-27132b93197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dill as pickle\n",
    "import os\n",
    "import tqdm.auto as tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import lognorm,norm\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from scipy.special import i0e\n",
    "\n",
    "from scipy.optimize import least_squares\n",
    "from numpy.linalg import inv\n",
    "\n",
    "\n",
    "# Define font properties\n",
    "font = {\n",
    "    'family': 'sans-serif',  # Use sans-serif family\n",
    "    'sans-serif': ['Helvetica'],  # Specify Helvetica as the sans-serif font\n",
    "    'size': 14  # Set the default font size\n",
    "}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# Set tick label sizes\n",
    "plt.rc('ytick', labelsize=24)\n",
    "plt.rc('xtick', labelsize=24)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n",
    "# Customize axes spines and legend appearance\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "\n",
    "from sbi import analysis as analysis\n",
    "from sbi import utils as utils\n",
    "from sbi.inference import SNPE, simulate_for_sbi\n",
    "from sbi.inference.potentials.posterior_based_potential import posterior_estimator_based_potential\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")\n",
    "from sbi.utils import process_prior,BoxUniform\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Categorical,Normal\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "from dipy.sims.voxel import single_tensor\n",
    "from dipy.data import get_fnames\n",
    "from dipy.io.gradients import read_bvals_bvecs\n",
    "from dipy.core.gradients import gradient_table\n",
    "from dipy.reconst.dti import (decompose_tensor, from_lower_triangular)\n",
    "from dipy.io.image import load_nifti\n",
    "from dipy.segment.mask import median_otsu\n",
    "import dipy.reconst.dti as dti\n",
    "import dipy.reconst.dki as dki\n",
    "from dipy.align.reslice import reslice\n",
    "from dipy.core.sphere import disperse_charges, Sphere, HemiSphere\n",
    "\n",
    "import pymatreader as pmt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import i0\n",
    "\n",
    "from dwMRI_BasicFuncs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd9471-a93e-4d76-b0b9-70e2b9e97b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoxPlots(y_data, positions, colors, colors2, ax,hatch = False,scatter=False,scatter_alpha=0.5, **kwargs):\n",
    "\n",
    "    GREY_DARK = \"#747473\"\n",
    "    jitter = 0.02\n",
    "    # Clean data to remove NaNs column-wise\n",
    "    if(np.ndim(y_data) == 1):\n",
    "        cleaned_data = y_data[~np.isnan(y_data)]\n",
    "    else:\n",
    "        cleaned_data = [d[~np.isnan(d)] for d in y_data]\n",
    "    \n",
    "    # Define properties for the boxes (patch objects)\n",
    "    boxprops = dict(\n",
    "        linewidth=2, \n",
    "        facecolor='none',       # use facecolor for filling (set to 'none' if you want no fill)\n",
    "        edgecolor='turquoise'   # edgecolor for the outline\n",
    "    )\n",
    "\n",
    "    # Define properties for the medians (Line2D objects)\n",
    "    # Ensure GREY_DARK is defined (or replace it with a color string)\n",
    "    medianprops = dict(\n",
    "        linewidth=2, \n",
    "        color=GREY_DARK,\n",
    "        solid_capstyle=\"butt\"\n",
    "    )\n",
    "\n",
    "    # For whiskers, since they are Line2D objects, use 'color'\n",
    "    whiskerprops = dict(\n",
    "        linewidth=2, \n",
    "        color='turquoise'\n",
    "    )\n",
    "\n",
    "    bplot = ax.boxplot(\n",
    "        cleaned_data,\n",
    "        positions=positions, \n",
    "        showfliers=False,\n",
    "        showcaps = False,\n",
    "        medianprops=medianprops,\n",
    "        whiskerprops=whiskerprops,\n",
    "        boxprops=boxprops,\n",
    "        patch_artist=True,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Update the color of each box (these are patch objects)\n",
    "    for i, box in enumerate(bplot['boxes']):\n",
    "        box.set_edgecolor(colors[i])\n",
    "        if(hatch):\n",
    "            box.set_hatch('/')\n",
    "    \n",
    "    \n",
    "    # Update the color of the whiskers (each box has 2 whiskers)\n",
    "    for i in range(len(positions)):\n",
    "        bplot['whiskers'][2*i].set_color(colors[i])\n",
    "        bplot['whiskers'][2*i+1].set_color(colors[i])\n",
    "    \n",
    "    # If caps are enabled, update their color (Line2D objects)\n",
    "    if 'caps' in bplot:\n",
    "        for i, cap in enumerate(bplot['caps']):\n",
    "            cap.set_color(colors[i//2])  # two caps per box\n",
    "\n",
    "    if(scatter):\n",
    "        if(np.ndim(cleaned_data) == 1):\n",
    "            x_data = np.array([positions] * len(cleaned_data))\n",
    "            x_jittered = x_data + stats.t(df=6, scale=jitter).rvs(len(x_data))\n",
    "            ax.scatter(x_data, cleaned_data, s=100, color=colors2, alpha=scatter_alpha)\n",
    "        else:\n",
    "            x_data = [np.array([positions[i]] * len(d)) for i, d in enumerate(cleaned_data)]\n",
    "            x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "            # Plot the scatter points with jitter (using colors2)\n",
    "            for x, y, c in zip(x_jittered, cleaned_data, colors2):\n",
    "                ax.scatter(x, y, s=100, color=c, alpha=scatter_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b697281-85b3-42e8-b615-e85017ed41bf",
   "metadata": {},
   "source": [
    "## Basc functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8423d48-4f48-4276-b0dd-0c84e84bb321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vals_to_mat(dt):\n",
    "    DTI = np.zeros((3,3))\n",
    "    DTI[0,0] = dt[0]\n",
    "    DTI[0,1],DTI[1,0] =  dt[1],dt[1]\n",
    "    DTI[1,1] =  dt[2]\n",
    "    DTI[0,2],DTI[2,0] =  dt[3],dt[3]\n",
    "    DTI[1,2],DTI[2,1] =  dt[4],dt[4]\n",
    "    DTI[2,2] =  dt[5]\n",
    "    return DTI\n",
    "\n",
    "def mat_to_vals(DTI):\n",
    "    dt = np.zeros(6)\n",
    "    dt[0] = DTI[0,0]\n",
    "    dt[1] = DTI[0,1]\n",
    "    dt[2] = DTI[1,1]\n",
    "    dt[3] = DTI[0,2]\n",
    "    dt[4] = DTI[1,2]\n",
    "    dt[5] = DTI[2,2]\n",
    "    return dt\n",
    "\n",
    "def fill_lower_diag(a):\n",
    "    b = [a[0],a[3],a[1],a[4],a[5],a[2]]\n",
    "    n = 3\n",
    "    mask = np.tri(n,dtype=bool) \n",
    "    out = np.zeros((n,n),dtype=float)\n",
    "    out[mask] = b\n",
    "    return out\n",
    "\n",
    "\n",
    "def ForceLowFA(dt):\n",
    "    # Modify the matrix to ensure low FA (more isotropic)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(dt)\n",
    "    \n",
    "    # Make the eigenvalues more similar to enforce low FA\n",
    "    mean_eigenvalue = np.mean(eigenvalues)\n",
    "\n",
    "    adjusted_eigenvalues = np.clip(eigenvalues, mean_eigenvalue * np.random.rand(), mean_eigenvalue * 1.0)\n",
    "    \n",
    "    # Reconstruct the matrix with the adjusted eigenvalues\n",
    "    dt_low_fa = eigenvectors @ np.diag(adjusted_eigenvalues) @ eigenvectors.T\n",
    "    \n",
    "    return dt_low_fa\n",
    "    \n",
    "def FracAni(evals,MD):\n",
    "    numerator = np.sqrt(3 * np.sum((evals - MD) ** 2))\n",
    "    denominator = np.sqrt(2) * np.sqrt(np.sum(evals ** 2))\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "def clip_negative_eigenvalues(matrix):\n",
    "    # Perform eigenvalue decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "    \n",
    "    # Clip negative eigenvalues to 0\n",
    "    clipped_eigenvalues = np.maximum(eigenvalues, 1e-5)\n",
    "    \n",
    "    # Reconstruct the matrix with the clipped eigenvalues\n",
    "    clipped_matrix = eigenvectors @ np.diag(clipped_eigenvalues) @ np.linalg.inv(eigenvectors)\n",
    "    \n",
    "    return clipped_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc19bd7-c67c-4646-8767-c1efce4ca0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FitDT(Dat,seed=1):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    # DT_abc\n",
    "    data = Dat[:,0]\n",
    "    shape,loc,scale = lognorm.fit(data)\n",
    "    \n",
    "    dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "\n",
    "    #DT_rest\n",
    "    data = Dat[:,1]\n",
    "    loc,scale = norm.fit(data)\n",
    "    \n",
    "    # Compute the fitted PDF\n",
    "    dti2_fitted = stats.norm(loc=loc, scale=scale)\n",
    "\n",
    "    return dti1_fitted,dti2_fitted\n",
    "\n",
    "def FitKT(Dat,seed=1):\n",
    "    np.random.seed(seed)    \n",
    "    # Fitting x4\n",
    "    data = Dat[:,0]\n",
    "    shape,loc,scale = lognorm.fit(data)\n",
    "    x4_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "    \n",
    "    # Fitting R1\n",
    "    data = Dat[:,3]\n",
    "    loc,scale = norm.fit(data)\n",
    "    R1_fitted = norm(loc,scale)\n",
    "    \n",
    "    # Fitting x2\n",
    "    data = Dat[:,9]\n",
    "    shape,loc,scale = lognorm.fit(data)\n",
    "    x2_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "\n",
    "    # Fitting R2\n",
    "    data = Dat[:,12]\n",
    "    loc,scale = norm.fit(data)\n",
    "    R2_fitted = norm(loc,scale)\n",
    "\n",
    "\n",
    "    return x4_fitted,R1_fitted,x2_fitted,R2_fitted\n",
    "\n",
    "def GenDTKT(DT_Fits,KT_Fits,seed,size):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    DT = np.zeros([size,6])\n",
    "    KT = np.zeros([size,15])\n",
    "\n",
    "    DT[:,0] = DT_Fits[0].rvs(size)\n",
    "    DT[:,2] = DT_Fits[0].rvs(size)\n",
    "    DT[:,5] = DT_Fits[0].rvs(size)\n",
    "\n",
    "    DT[:,1] = DT_Fits[1].rvs(size)\n",
    "    DT[:,3] = DT_Fits[1].rvs(size)\n",
    "    DT[:,4] = DT_Fits[1].rvs(size)\n",
    "\n",
    "    for k in range(3):\n",
    "        KT[:,k] = KT_Fits[0].rvs(size)\n",
    "    for k in range(3,9):\n",
    "        KT[:,k] = KT_Fits[1].rvs(size)\n",
    "    for k in range(9,12):\n",
    "        KT[:,k] = KT_Fits[2].rvs(size)\n",
    "    for k in range(12,15):\n",
    "        KT[:,k] = KT_Fits[3].rvs(size)\n",
    "\n",
    "    return DT,KT\n",
    "    \n",
    "def DKIMetrics(dt,kt,analytical=True):\n",
    "    if(dt.ndim == 1):\n",
    "        dt = vals_to_mat(dt)\n",
    "    evals,evecs = np.linalg.eigh(dt)\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evals = evals[idx]\n",
    "    evecs = evecs[:, idx]\n",
    "    \n",
    "    params = np.concatenate([evals,np.hstack(evecs),kt])\n",
    "    params2 = np.concatenate([evals,np.hstack(evecs),-kt])\n",
    "\n",
    "    mk = dki.mean_kurtosis(params,analytical=analytical,min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    ak = dki.axial_kurtosis(params,analytical=analytical,min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    rk = dki.radial_kurtosis(params,analytical=analytical,min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    mkt = dki.mean_kurtosis_tensor(params, min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    kfa = kurtosis_fractional_anisotropy_test(params)\n",
    "\n",
    "    return mk,ak,rk,mkt,kfa\n",
    "    \n",
    "def kurtosis_fractional_anisotropy_test(dki_params):\n",
    "    r\"\"\"Compute the anisotropy of the kurtosis tensor (KFA).\n",
    "\n",
    "    See :footcite:p:`Glenn2015` and :footcite:p:`NetoHenriques2021` for further\n",
    "    details about the method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dki_params : ndarray (x, y, z, 27) or (n, 27)\n",
    "        All parameters estimated from the diffusion kurtosis model.\n",
    "        Parameters are ordered as follows:\n",
    "            1) Three diffusion tensor's eigenvalues\n",
    "            2) Three lines of the eigenvector matrix each containing the first,\n",
    "                second and third coordinates of the eigenvector\n",
    "            3) Fifteen elements of the kurtosis tensor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kfa : array\n",
    "        Calculated mean kurtosis tensor.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The KFA is defined as :footcite:p:`Glenn2015`:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "         KFA \\equiv\n",
    "         \\frac{||\\mathbf{W} - MKT \\mathbf{I}^{(4)}||_F}{||\\mathbf{W}||_F}\n",
    "\n",
    "    where $W$ is the kurtosis tensor, MKT the kurtosis tensor mean, $I^{(4)}$ is\n",
    "    the fully symmetric rank 2 isotropic tensor and $||...||_F$ is the tensor's\n",
    "    Frobenius norm :footcite:p:`Glenn2015`.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. footbibliography::\n",
    "\n",
    "    \"\"\"\n",
    "    Wxxxx = dki_params[..., 12]\n",
    "    Wyyyy = dki_params[..., 13]\n",
    "    Wzzzz = dki_params[..., 14]\n",
    "    Wxxxy = dki_params[..., 15]\n",
    "    Wxxxz = dki_params[..., 16]\n",
    "    Wxyyy = dki_params[..., 17]\n",
    "    Wyyyz = dki_params[..., 18]\n",
    "    Wxzzz = dki_params[..., 19]\n",
    "    Wyzzz = dki_params[..., 20]\n",
    "    Wxxyy = dki_params[..., 21]\n",
    "    Wxxzz = dki_params[..., 22]\n",
    "    Wyyzz = dki_params[..., 23]\n",
    "    Wxxyz = dki_params[..., 24]\n",
    "    Wxyyz = dki_params[..., 25]\n",
    "    Wxyzz = dki_params[..., 26]\n",
    "\n",
    "\n",
    "    W = 1.0 / 5.0 * (Wxxxx + Wyyyy + Wzzzz + 2 * Wxxyy + 2 * Wxxzz + 2 * Wyyzz)\n",
    "    # Compute's equation numerator\n",
    "    A = (\n",
    "        (Wxxxx - W) ** 2\n",
    "        + (Wyyyy - W) ** 2\n",
    "        + (Wzzzz - W) ** 2\n",
    "        + 4 * (Wxxxy**2 + Wxxxz**2 + Wxyyy**2 + Wyyyz**2 + Wxzzz**2 + Wyzzz**2)\n",
    "        + 6 * ((Wxxyy - W / 3) ** 2 + (Wxxzz - W / 3) ** 2 + (Wyyzz - W / 3) ** 2)\n",
    "        + 12 * (Wxxyz**2 + Wxyyz**2 + Wxyzz**2)\n",
    "    )\n",
    "    # Compute's equation denominator\n",
    "    B = (\n",
    "        Wxxxx**2\n",
    "        + Wyyyy**2\n",
    "        + Wzzzz**2\n",
    "        + 4 * (Wxxxy**2 + Wxxxz**2 + Wxyyy**2 + Wyyyz**2 + Wxzzz**2 + Wyzzz**2)\n",
    "        + 6 * (Wxxyy**2 + Wxxzz**2 + Wyyzz**2)\n",
    "        + 12 * (Wxxyz**2 + Wxyyz**2 + Wxyzz**2)\n",
    "    )\n",
    "\n",
    "    # Compute KFA\n",
    "    KFA = np.zeros(A.shape)\n",
    "    KFA = np.sqrt(A / B)\n",
    "\n",
    "    return KFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf6922-00cf-416f-b578-09734f57f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomSimulator(Mat,gtab,S0,snr=None):\n",
    "    evals,evecs = np.linalg.eigh(Mat)\n",
    "    signal = single_tensor(gtab, S0=S0, evals=evals, evecs=evecs)\n",
    "    if(snr is None):\n",
    "        return signal\n",
    "    else:\n",
    "        return AddNoise(signal,S0,snr)\n",
    "\n",
    "def Simulator(gtab,S0,params,SNR):\n",
    "\n",
    "    dt = ComputeDTI(params)\n",
    "    signal_dti = CustomSimulator(dt,gtab,S0,SNR)\n",
    "    \n",
    "    return signal_dti\n",
    "\n",
    "\n",
    "def GenRicciNoise(signal,S0,snr):\n",
    "\n",
    "    size = signal.shape\n",
    "    sigma = S0 / snr\n",
    "    noise1 = np.random.normal(0, sigma, size=size)\n",
    "    noise2 = np.random.normal(0, sigma, size=size)\n",
    "\n",
    "    return np.sqrt((signal+noise1) ** 2 + noise2 ** 2)\n",
    "\n",
    "\n",
    "def AddNoise(signal,S0,snr):\n",
    "    \n",
    "    return GenRicciNoise(signal,S0,snr)\n",
    "\n",
    "def CustomDKISimulator(dt,kt,gtab,S0,snr=None):\n",
    "    if(dt.ndim == 1):\n",
    "        dt = vals_to_mat(dt)\n",
    "    evals,evecs = np.linalg.eigh(dt)\n",
    "    params = np.concatenate([evals,np.hstack(evecs),kt])\n",
    "    signal = dki.dki_prediction(params,gtab,S0)\n",
    "    if(snr is None):\n",
    "        return signal\n",
    "    else:\n",
    "        return AddNoise(signal,S0,snr)\n",
    "\n",
    "def CustomDKISimulator2(params,kt,gtab,S0,snr=None):\n",
    "    dt = ComputeDTI(params)\n",
    "    evals,evecs = np.linalg.eigh(dt)\n",
    "    combined_set = np.concatenate([evals,np.hstack(evecs),kt])\n",
    "    signal = dki.dki_prediction(combined_set,gtab,S0)\n",
    "    if(np.isnan(signal).any() or np.isinf(signal).any() or (signal>1e15).any()):\n",
    "        pass#import pdb;pdb.set_trace()\n",
    "    if(snr is None):\n",
    "        return signal\n",
    "    else:\n",
    "        return AddNoise(signal,S0,snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83c1bd-2b6f-4f84-809b-2585ff5e4c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_path = '../Networks/'\n",
    "image_path   = './Images/'\n",
    "if not os.path.exists(image_path):\n",
    "    os.makedirs(image_path)\n",
    "NoiseLevels = [None,20,10,5,2]\n",
    "\n",
    "TrainingSamples = 50000\n",
    "InferSamples    = 500\n",
    "\n",
    "lower_abs,upper_abs = -0.07,0.07\n",
    "lower_rest,upper_rest = -0.015,0.015\n",
    "lower_S0 = 25\n",
    "upper_S0 = 2000\n",
    "Save = True\n",
    "\n",
    "TrueCol  = 'k'\n",
    "NoisyCol = 'k'\n",
    "NLLSFit   = np.array([225,190,106])/255\n",
    "SBIFit   = np.array([64,176,166])/255\n",
    "WLSFit = np.array([140, 100, 200]) / 255  # muted violet\n",
    "MLEFit = np.array([210, 80, 140]) / 255\n",
    "BayFit = np.array([100, 120, 220]) / 255 \n",
    "\n",
    "Errors_name = ['MD comparison','FA comparison','eig. comparison','Frobenius','Signal comparison','Correlation','Signal comparison','Correlation2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859126b-389c-479b-8270-cc2d92a7cef6",
   "metadata": {},
   "source": [
    "## DKI Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3594c07f-91f7-4dbd-b3f2-659adc0db55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 1\n",
    "fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "\n",
    "data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "# Get the indices of True values\n",
    "true_indices = np.argwhere(mask)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = true_indices.min(axis=0)\n",
    "max_coords = true_indices.max(axis=0)\n",
    "\n",
    "maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "FlatTD = TestData.reshape(maskdata.shape[0]*maskdata.shape[1],138)\n",
    "FlatTD = FlatTD[FlatTD[:,:69].sum(axis=-1)>0]\n",
    "FlatTD = FlatTD[~np.array(FlatTD<0).any(axis=-1)]\n",
    "\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabExt)\n",
    "tenfit = dkimodel.fit(FlatTD)\n",
    "DKIHCP = tenfit.kt\n",
    "DTIHCP = tenfit.lower_triangular()\n",
    "DKIFull = np.array(DKIHCP)\n",
    "DTIFull = np.array(DTIHCP)\n",
    "\n",
    "\n",
    "DTIFilt1 = DTIFull[(abs(DKIFull)<10).all(axis=1)]\n",
    "DKIFilt1 = DKIFull[(abs(DKIFull)<10).all(axis=1)]\n",
    "DTIFilt = DTIFilt1[(DKIFilt1>-3/7).all(axis=1)]\n",
    "DKIFilt = DKIFilt1[(DKIFilt1>-3/7).all(axis=1)]\n",
    "\n",
    "TrueMets = []\n",
    "FA       = []\n",
    "for (dt,kt) in tqdm.tqdm(zip(DTIFilt,DKIFilt)):\n",
    "    TrueMets.append(DKIMetrics(dt,kt))\n",
    "    FA.append(FracAni(np.linalg.eigh(vals_to_mat(dt))[0],np.mean(np.linalg.eigh(vals_to_mat(dt))[0])))\n",
    "TrueMets = np.array(TrueMets)\n",
    "TrueFA = np.array(FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d569553f-12db-4bdd-bb79-81ce90408cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full fit\n",
    "DT1_full,DT2_full = FitDT(DTIFilt,1)\n",
    "x4_full,R1_full,x2_full,R2_full = FitKT(DKIFilt,1)\n",
    "\n",
    "# LowFA Fit\n",
    "DT1_lfa,DT2_lfa = FitDT(DTIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "x4_lfa,R1_lfa,x2_lfa,R2_lfa = FitKT(DKIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "\n",
    "# HighFA Fit\n",
    "DT1_hfa,DT2_hfa = FitDT(DTIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "x4_hfa,R1_hfa,x2_hfa,R2_hfa = FitKT(DKIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "\n",
    "# UltraLowFA Fit\n",
    "DT1_ulfa,DT2_ulfa = FitDT(DTIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa = FitKT(DKIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "\n",
    "# HigherAK Fit\n",
    "DT1_hak,DT2_hak = FitDT(DTIFilt[TrueMets[:,1]>0.9,:],1)\n",
    "x4_hak,R1_hak,x2_hak,R2_hak = FitKT(DKIFilt[TrueMets[:,1]>0.9,:],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fdf23-348f-477a-b516-71b6d7664b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "se = 14\n",
    "torch.manual_seed(se)\n",
    "np.random.seed(se)\n",
    "j = 1\n",
    "vL = torch.tensor([0.2*j])\n",
    "vS = torch.tensor([0.01*j])  \n",
    "\n",
    "kk = np.random.randint(0,4)\n",
    "if(kk==0):\n",
    "    DT_guess,KT_guess= GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],se,1)\n",
    "elif(kk==1):\n",
    "    DT_guess,KT_guess = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],se,1)\n",
    "elif(kk==2):\n",
    "    DT_guess,KT_guess = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],se,1)\n",
    "elif(kk==3):\n",
    "    DT_guess,KT_guess = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],se,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3159e7d-1223-4008-a549-6fe99268100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatFolder = '/Users/maximilianeggl/Dropbox/PostDoc/Silvia/SBIDTIPaper2/Code/SavedDat/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20520c02-878a-47ca-9763-512a411b4399",
   "metadata": {},
   "source": [
    "# MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb11a85-7e61-40df-bcbf-1bd65b890087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invertComputeDTI(A, scale_factor=1e-3):\n",
    "    # 1) ensure perfect symmetry\n",
    "    A = (A + A.T) / 2.0\n",
    "\n",
    "    # 2) Cholesky → L lower‑triangular with positive diagonals\n",
    "    L = np.linalg.cholesky(A)\n",
    "\n",
    "    # 3) recover params:\n",
    "    #    - diagonal params = log(L[ii,ii] / scale_factor)\n",
    "    #    - off‑diagonals = the corresponding L entries\n",
    "    return np.array([\n",
    "        np.log(L[0,0] / scale_factor),  # p0\n",
    "        np.log(L[1,1] / scale_factor),  # p2\n",
    "        np.log(L[2,2] / scale_factor),   # p5\n",
    "        L[1,0],                         # p1\n",
    "        L[2,0],                         # p3\n",
    "        L[2,1]                         # p4\n",
    "\n",
    "    ])\n",
    "def ComputeDTI(params, scale_factor=1e-3):\n",
    "    L = fill_lower_diag(params)\n",
    "    diag_indices = np.diag_indices_from(L)\n",
    "    L[diag_indices] = np.exp(L[diag_indices]) * scale_factor\n",
    "    A = L @ L.T\n",
    "    return A\n",
    "    \n",
    "def rician_nll_DKI(params,S_obs,gtab,S0):\n",
    "    dt        = params[:6]\n",
    "    kt        = params[6:-1]\n",
    "    log_sigma = params[-1]\n",
    "\n",
    "    S_model  = CustomDKISimulator2(dt,kt,gtab,S0)\n",
    "\n",
    "    sigma2 = np.exp(2 * log_sigma)  # ensure positivity\n",
    "    # Avoid log(0) with clipping\n",
    "    S_obs_clipped = np.clip(S_obs, 1e-10, None)\n",
    "    S_model_clipped = np.clip(S_model, 1e-10, 1e10)\n",
    "\n",
    "    bessel_arg = (S_obs_clipped * S_model_clipped) / sigma2\n",
    "\n",
    "    # use the scaled Bessel to avoid overflow:\n",
    "    log_bessel = np.log(i0e(bessel_arg)) + np.abs(bessel_arg)\n",
    "\n",
    "    nll = (\n",
    "        np.log(sigma2)\n",
    "        - np.log(S_obs_clipped)\n",
    "        + (S_obs_clipped**2 + S_model_clipped**2) / (2 * sigma2)\n",
    "        - log_bessel\n",
    "    )\n",
    "    if(np.isnan(nll).any() or np.isinf(nll).any()):\n",
    "        import pdb;pdb.set_trace()\n",
    "    return np.sum(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73675d3b-bb23-4366-adc3-1f515815ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(params,gtab,y):\n",
    "\n",
    "    dt        = params[:6]\n",
    "    kt        = params[6:-1]\n",
    "    log_S0 = params[-1]\n",
    "    \n",
    "    Signal = CustomDKISimulator2(dt,kt,gtab,np.exp(log_S0))\n",
    "    return y - Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447517c-5515-4676-a6f7-8ca9d3cdf09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = 3\n",
    "fdwi = './HCP_data/Pat'+str(kk)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(kk)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(kk)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(kk)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(kk)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(kk)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "_, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "\n",
    "\n",
    "data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "# Get the indices of True values\n",
    "true_indices = np.argwhere(mask)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = true_indices.min(axis=0)\n",
    "max_coords = true_indices.max(axis=0)\n",
    "\n",
    "maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "axial_middle = maskdata.shape[2] // 2\n",
    "maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f80ea3-6d93-4384-a608-212eea500969",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bounds = [-0.0] * 3 + [-0.015]*3 + [0]*3 + [-1]*9 + [-0.25]*3 + [np.log(2)]\n",
    "\n",
    "upper_bounds = [5] * 3 + [0.015]*3 + [5]*3 + [1]*9 + [0.25]*3 + [np.log(20)]\n",
    "bounds = list(zip(lower_bounds, upper_bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b3566d-a697-4ece-aae7-cc29bf09f476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for kk in tqdm.tqdm(range(1,33)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "\n",
    "    ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "    NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "    torch.manual_seed(10)\n",
    "    DT_guess_new = invertComputeDTI(vals_to_mat(DT_guess.squeeze()))\n",
    "    KT_guess_new = KT_guess.squeeze()\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                tObs = TestData4D[i,j,axial_middle,:]\n",
    "                LS_x = least_squares(residuals, x0=np.hstack([DT_guess_new,KT_guess_new,np.log(tObs[gtabExt.bvals==0].mean())]), args=(gtabExt, tObs)).x\n",
    "                res = minimize(\n",
    "                        rician_nll_DKI,\n",
    "                        np.hstack([LS_x[:6],LS_x[6:21],np.log(10)]),\n",
    "                        args=(tObs, gtabExt, np.exp(LS_x[-1])),\n",
    "                        options={'disp':0,'maxfun': 100000,   # ← raise the cap on f+g evaluations\n",
    "                                'maxiter': 20000,   # ← (optional) raise the cap on iterations   # ← raise the cap on f+g evaluations   # ← raise the cap on f+g evaluations\n",
    "                                'gtol': 1e-6,\n",
    "                                'ftol': 1e-6,},\n",
    "                            bounds=bounds\n",
    "                    )\n",
    "                NoiseEst[i,j] = res.x\n",
    "                #DT_guess_new,KT_guess_new = LS_x[:6],LS_x[6:21]\n",
    "    NoiseEstInv = np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEstInv[i,j] = np.hstack([mat_to_vals(ComputeDTI(NoiseEst[i,j,:6])),NoiseEst[i,j,6:21],NoiseEst[i,j,-1]])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEstInv[i,j]))),NoiseEstInv[i,j,6:]])\n",
    "    \n",
    "    MK_SBIFull  = np.zeros(ArrShape)\n",
    "    AK_SBIFull  = np.zeros(ArrShape)\n",
    "    RK_SBIFull  = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK_SBIFull[i,j] = Metrics[0]\n",
    "            AK_SBIFull[i,j] = Metrics[1]\n",
    "            RK_SBIFull[i,j] = Metrics[2]\n",
    "    np.save(DatFolder+'Full_MK_MLE_'+str(kk),np.array(MK_SBIFull,dtype=object))\n",
    "    np.save(DatFolder+'Full_AK_MLE_'+str(kk),np.array(AK_SBIFull,dtype=object))\n",
    "    np.save(DatFolder+'Full_RK_MLE_'+str(kk),np.array(RK_SBIFull,dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74175788-e5ea-4d45-83eb-28926206b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kk in tqdm.tqdm(range(23,33)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    \n",
    "    ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "    NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "    torch.manual_seed(10)\n",
    "    DT_guess_new = invertComputeDTI(vals_to_mat(DT_guess.squeeze()))\n",
    "    KT_guess_new = KT_guess.squeeze()\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                tObs = TestData4D[i,j,axial_middle,:]\n",
    "                avg_s0 = tObs[gtabExt.bvals==0].mean()\n",
    "                if(avg_s0<0):\n",
    "                    tObs = tObs + np.abs(tObs.min())\n",
    "                try:\n",
    "                    LS_x = least_squares(residuals, x0=np.hstack([DT_guess_new,KT_guess_new,np.log(tObs[gtabExt.bvals==0].mean())]), args=(gtabExt, tObs)).x\n",
    "                    try:\n",
    "                        res = minimize(\n",
    "                                    rician_nll_DKI,\n",
    "                                    np.hstack([LS_x[:6],LS_x[6:21],np.log(10)]),\n",
    "                                    args=(tObs, gtabExt, np.exp(LS_x[-1])),\n",
    "                                    options={'disp':0,'maxfun': 100000,   # ← raise the cap on f+g evaluations\n",
    "                                            'maxiter': 20000,   # ← (optional) raise the cap on iterations   # ← raise the cap on f+g evaluations   # ← raise the cap on f+g evaluations\n",
    "                                            'gtol': 1e-6,\n",
    "                                            'ftol': 1e-6,},\n",
    "                                        bounds=bounds\n",
    "                                )\n",
    "                    except:\n",
    "                        res = minimize(\n",
    "                                    rician_nll_DKI,\n",
    "                                    np.hstack([DT_guess_new,KT_guess_new,np.log(tObs[gtabExt.bvals==0].mean())]),\n",
    "                                    args=(tObs, gtabExt, np.exp(LS_x[-1])),\n",
    "                                    options={'disp':0,'maxfun': 100000,   # ← raise the cap on f+g evaluations\n",
    "                                            'maxiter': 20000,   # ← (optional) raise the cap on iterations   # ← raise the cap on f+g evaluations   # ← raise the cap on f+g evaluations\n",
    "                                            'gtol': 1e-6,\n",
    "                                            'ftol': 1e-6,},\n",
    "                                        bounds=bounds\n",
    "                        )\n",
    "                    NoiseEst[i,j] = res.x\n",
    "                except:\n",
    "                    NoiseEst[i,j] = 0\n",
    "                \n",
    "                #DT_guess_new,KT_guess_new = LS_x[:6],LS_x[6:21]\n",
    "    NoiseEstInv = np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEstInv[i,j] = np.hstack([mat_to_vals(ComputeDTI(NoiseEst[i,j,:6])),NoiseEst[i,j,6:21],NoiseEst[i,j,-1]])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEstInv[i,j]))),NoiseEstInv[i,j,6:]])\n",
    "    \n",
    "    MK_SBIFull  = np.zeros(ArrShape)\n",
    "    AK_SBIFull  = np.zeros(ArrShape)\n",
    "    RK_SBIFull  = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK_SBIFull[i,j] = Metrics[0]\n",
    "            AK_SBIFull[i,j] = Metrics[1]\n",
    "            RK_SBIFull[i,j] = Metrics[2]\n",
    "    np.save(DatFolder+'Full_MK_MLE_'+str(kk),np.array(MK_SBIFull,dtype=object))\n",
    "    np.save(DatFolder+'Full_AK_MLE_'+str(kk),np.array(AK_SBIFull,dtype=object))\n",
    "    np.save(DatFolder+'Full_RK_MLE_'+str(kk),np.array(RK_SBIFull,dtype=object))\n",
    "\n",
    "    fig,ax = plt.subplots(1,3)\n",
    "    ax[0].imshow(MK_SBIFull,vmin=0,vmax=1)\n",
    "    ax[1].imshow(AK_SBIFull,vmin=0,vmax=1)\n",
    "    ax[2].imshow(RK_SBIFull,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d473a86-d10e-4a69-9072-9f04e03189b1",
   "metadata": {},
   "source": [
    "#### Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d939f4-773d-49cb-b289-5a0f343c841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices7 = [0]+selected_indices\n",
    "\n",
    "bvalsHCP7_1 = bvalsHCP[selected_indices7]\n",
    "bvecsHCP7_1 = bvecsHCP[selected_indices7]\n",
    "\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(14):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx = []\n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "selected_indices7 = selected_indices7+[t+69 for t in true_indx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7d2e7-939d-46fd-8553-d73fe2ec5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kk in tqdm.tqdm(range(1,33)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "    gtabHCP7 = gradient_table(gtabExt.bvals[selected_indices7],gtabExt.bvecs[selected_indices7])\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    \n",
    "    ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "    NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "    torch.manual_seed(10)\n",
    "    DT_guess_new = invertComputeDTI(vals_to_mat(DT_guess.squeeze()))\n",
    "    KT_guess_new = KT_guess.squeeze()\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                tObs = TestData4D[i,j,axial_middle,selected_indices7]\n",
    "                avg_s0 = tObs[gtabHCP7.bvals==0].mean()\n",
    "                if(avg_s0<0):\n",
    "                    tObs = tObs + np.abs(tObs.min())\n",
    "                try:\n",
    "                    LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze(),np.log(100)]), args=(gtabHCP7, tObs)).x\n",
    "                    \n",
    "                    try:\n",
    "                        res = minimize(\n",
    "                                    rician_nll_DKI,\n",
    "                                    np.hstack([LS_x[:6],LS_x[6:21],np.log(10)]),\n",
    "                                    args=(tObs, gtabHCP7, np.exp(LS_x[-1])),\n",
    "                                    options={'disp':0,'maxfun': 100000,   # ← raise the cap on f+g evaluations\n",
    "                                            'maxiter': 20000,   # ← (optional) raise the cap on iterations   # ← raise the cap on f+g evaluations   # ← raise the cap on f+g evaluations\n",
    "                                            'gtol': 1e-6,\n",
    "                                            'ftol': 1e-6,},\n",
    "                                        bounds=bounds\n",
    "                                )\n",
    "                    except:\n",
    "                        res = minimize(\n",
    "                                    rician_nll_DKI,\n",
    "                                    np.hstack([DT_guess_new,KT_guess_new,np.log(tObs[gtabExt.bvals==0].mean())]),\n",
    "                                    args=(tObs, gtabHCP7, np.exp(LS_x[-1])),\n",
    "                                    options={'disp':0,'maxfun': 100000,   # ← raise the cap on f+g evaluations\n",
    "                                            'maxiter': 20000,   # ← (optional) raise the cap on iterations   # ← raise the cap on f+g evaluations   # ← raise the cap on f+g evaluations\n",
    "                                            'gtol': 1e-6,\n",
    "                                            'ftol': 1e-6,},\n",
    "                                        bounds=bounds\n",
    "                        )\n",
    "                    NoiseEst[i,j] = res.x\n",
    "                except:\n",
    "                    NoiseEst[i,j] = 0\n",
    "                #DT_guess_new,KT_guess_new = LS_x[:6],LS_x[6:21]\n",
    "    NoiseEstInv = np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEstInv[i,j] = np.hstack([mat_to_vals(ComputeDTI(NoiseEst[i,j,:6])),NoiseEst[i,j,6:21],NoiseEst[i,j,-1]])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEstInv[i,j]))),NoiseEstInv[i,j,6:]])\n",
    "    \n",
    "    MK_SBIFull  = np.zeros(ArrShape)\n",
    "    AK_SBIFull  = np.zeros(ArrShape)\n",
    "    RK_SBIFull  = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK_SBIFull[i,j] = Metrics[0]\n",
    "            AK_SBIFull[i,j] = Metrics[1]\n",
    "            RK_SBIFull[i,j] = Metrics[2]\n",
    "    np.save(DatFolder+'Min_MK_MLE_'+str(kk),np.array(MK_SBIFull,dtype=object))\n",
    "    np.save(DatFolder+'Min_AK_MLE_'+str(kk),np.array(AK_SBIFull,dtype=object))\n",
    "    np.save(DatFolder+'Min_RK_MLE_'+str(kk),np.array(RK_SBIFull,dtype=object))\n",
    "\n",
    "    fig,ax = plt.subplots(1,3)\n",
    "    ax[0].imshow(MK_SBIFull,vmin=0,vmax=1)\n",
    "    ax[1].imshow(AK_SBIFull,vmin=0,vmax=1)\n",
    "    ax[2].imshow(RK_SBIFull,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b13f05-4ca4-4a41-a456-bae1a0b0d0be",
   "metadata": {},
   "source": [
    "# Variational Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec0dc9a-1f1d-4e92-8ff8-6fcc6f3cae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invertComputeDTI_exp(A, scale_factor=1e-3):\n",
    "    # 1) ensure perfect symmetry\n",
    "    A = (A + A.T) / 2.0\n",
    "\n",
    "    # 2) Cholesky → L lower‑triangular with positive diagonals\n",
    "    L = np.linalg.cholesky(A)\n",
    "\n",
    "    # 3) recover params:\n",
    "    #    - diagonal params = log(L[ii,ii] / scale_factor)\n",
    "    #    - off‑diagonals = the corresponding L entries\n",
    "    return np.array([\n",
    "        np.log(L[0,0] / scale_factor),  # p0\n",
    "        np.log(L[1,1] / scale_factor),  # p2\n",
    "        np.log(L[2,2] / scale_factor),   # p5\n",
    "        L[1,0],                         # p1\n",
    "        L[2,0],                         # p3\n",
    "        L[2,1]                         # p4\n",
    "\n",
    "    ])\n",
    "def ComputeDTI_exp(params, scale_factor=1e-3):\n",
    "    L = fill_lower_diag(params)\n",
    "    diag_indices = np.diag_indices_from(L)\n",
    "    L[diag_indices] = np.exp(L[diag_indices]) * scale_factor\n",
    "    A = L @ L.T\n",
    "    return A\n",
    "\n",
    "def CustomDKISimulator_exp(params,kt,gtab,S0,snr=None):\n",
    "    dt = ComputeDTI_exp(params)\n",
    "    evals,evecs = np.linalg.eigh(dt)\n",
    "    combined_set = np.concatenate([evals,np.hstack(evecs),kt])\n",
    "    signal = dki.dki_prediction(combined_set,gtab,S0)\n",
    "    if(np.isnan(signal).any() or np.isinf(signal).any() or (signal>1e15).any()):\n",
    "        pass#import pdb;pdb.set_trace()\n",
    "    if(snr is None):\n",
    "        return signal\n",
    "    else:\n",
    "        return AddNoise(signal,S0,snr)\n",
    "\n",
    "def residuals(params,gtab,y):\n",
    "\n",
    "    dt        = params[:6]\n",
    "    kt        = params[6:-1]\n",
    "    log_S0 = params[-1]\n",
    "    \n",
    "    Signal = CustomDKISimulator_exp(dt,kt,gtab,np.exp(log_S0))\n",
    "    return y - Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c33eb-3bed-44b3-aec8-72029c862e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(theta, gtab):\n",
    "    \"\"\"DKI signal model for *one* voxel, given params theta.\"\"\"\n",
    "    dt        = theta[:6]\n",
    "    kt        = theta[6:-1]\n",
    "    S0        = np.exp(theta[-1])          # log-S0 in parameters\n",
    "    return CustomDKISimulator_exp(dt, kt, gtab, S0)   # (Ngrad,)\n",
    "\n",
    "# Finite-difference Jacobian (6 dt + K kt + 1 S0 parameters)\n",
    "def jacobian(theta, gtab, eps=1e-5):\n",
    "    J = np.zeros((len(gtab.bvals), len(theta)))\n",
    "    f0 = f(theta, gtab)\n",
    "    for i in range(len(theta)):\n",
    "        t_eps        = theta.copy()\n",
    "        t_eps[i]    += eps\n",
    "        J[:, i]      = (f(t_eps, gtab) - f0) / eps\n",
    "    return J, f0\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def vb_gauss_one_voxel(y, gtab, theta0,\n",
    "                       mu0=None, V0=None,  # Gaussian prior\n",
    "                       a0=1e-3, b0=1e-3,   # Gamma prior\n",
    "                       max_iter=100, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Mean-field VB (Gaussian q(theta), Gamma q(1/σ²))\n",
    "    for one voxel with Gaussian noise.\n",
    "    \"\"\"\n",
    "    D = len(theta0)                       # #parameters\n",
    "    if mu0 is None:\n",
    "        mu0 = np.zeros(D)\n",
    "    if V0 is None:\n",
    "        V0 = np.eye(D) * 1e2              # very vague prior\n",
    "    V0_inv = inv(V0)\n",
    "\n",
    "    m  = theta0.copy()                    # variational mean\n",
    "    S  = V0.copy()                        # variational cov\n",
    "    a  = a0 + len(y)/2.\n",
    "    b  = b0 + 1.0                         # will be updated\n",
    "    for it in range(max_iter):\n",
    "        # --- E[precision] ----------------\n",
    "        lam = a / b                       # <1/σ²>\n",
    "\n",
    "        # --- linearise model around current mean\n",
    "        J, f_m = jacobian(m, gtab)\n",
    "        r      = y - f_m                  # residual\n",
    "\n",
    "        # --- Gauss-Newton-style VB updates\n",
    "        S_new  = inv(V0_inv + lam * J.T @ J)\n",
    "        m_new  = m + S_new @ (V0_inv @ (mu0 - m) + lam * J.T @ r)\n",
    "\n",
    "        # --- update Gamma factors\n",
    "        quad   = (r @ r\n",
    "                  + np.trace(J @ S_new @ J.T))\n",
    "        b_new  = b0 + 0.5 * quad\n",
    "\n",
    "        # --- convergence check\n",
    "        if np.linalg.norm(m_new - m) < tol:\n",
    "            m, S, b = m_new, S_new, b_new\n",
    "            break\n",
    "\n",
    "        m, S, b = m_new, S_new, b_new\n",
    "\n",
    "    return m, S, a/b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb5e7a-3417-4850-b768-9b6b32ece8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors_SD =  [1/0.001633,    1/.5e-5,    1/0.001633,    1/.5e-5,   1/ 7.5e-5, 1/0.001633] + [1e2]*15 + [1e2]\n",
    "mu0 = np.zeros(22)\n",
    "\n",
    "V0  = np.diag(priors_SD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25a54e-cdc5-442e-8214-cc4beb47eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(params,gtab,y):\n",
    "\n",
    "    dt        = params[:6]\n",
    "    kt        = params[6:-1]\n",
    "    log_S0 = params[-1]\n",
    "    \n",
    "    Signal = CustomDKISimulator2(dt,kt,gtab,np.exp(log_S0))\n",
    "    return y - Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68752e13-68f6-4b05-b5c6-b884b629d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kk in tqdm.tqdm(range(10,33)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    \n",
    "    ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "    NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "    torch.manual_seed(10)\n",
    "    DT_guess_new = invertComputeDTI(vals_to_mat(DT_guess.squeeze()))\n",
    "    KT_guess_new = KT_guess.squeeze()\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                tObs = TestData4D[i,j,axial_middle,:]\n",
    "                avg_s0 = tObs[gtabExt.bvals==0].mean()\n",
    "                if(avg_s0<0):\n",
    "                    tObs = tObs + np.abs(tObs.min())\n",
    "                try:\n",
    "                    LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze(),np.log(tObs[gtabExt.bvals==0].mean())]), args=(gtabExt, tObs)).x\n",
    "                    try:\n",
    "                        m, S, prec = vb_gauss_one_voxel(tObs, gtabExt, LS_x,mu0=mu0, V0=V0)\n",
    "                    except:\n",
    "                        m = 0\n",
    "                    NoiseEst[i,j] = m\n",
    "                except:\n",
    "                    NoiseEst[i,j] = 0\n",
    "                \n",
    "                #DT_guess_new,KT_guess_new = LS_x[:6],LS_x[6:21]\n",
    "    NoiseEstInv = np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEstInv[i,j] = np.hstack([mat_to_vals(ComputeDTI(NoiseEst[i,j,:6])),NoiseEst[i,j,6:21],NoiseEst[i,j,-1]])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEstInv[i,j]))),NoiseEstInv[i,j,6:]])\n",
    "    \n",
    "    MK_SBIFull  = np.zeros(ArrShape)\n",
    "    AK_SBIFull  = np.zeros(ArrShape)\n",
    "    RK_SBIFull  = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK_SBIFull[i,j] = Metrics[0]\n",
    "            AK_SBIFull[i,j] = Metrics[1]\n",
    "            RK_SBIFull[i,j] = Metrics[2]\n",
    "    np.save(DatFolder+'Full_MK_Bay_'+str(kk),np.array(MK_SBIFull,dtype=object))\n",
    "    np.save(DatFolder+'Full_AK_Bay_'+str(kk),np.array(AK_SBIFull,dtype=object))\n",
    "    np.save(DatFolder+'Full_RK_Bay_'+str(kk),np.array(RK_SBIFull,dtype=object))\n",
    "\n",
    "    fig,ax = plt.subplots(1,3)\n",
    "    ax[0].imshow(MK_SBIFull,vmin=0,vmax=1)\n",
    "    ax[1].imshow(AK_SBIFull,vmin=0,vmax=1)\n",
    "    ax[2].imshow(RK_SBIFull,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccccaa1f-e378-4fe5-8651-040941d1faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices7 = [0]+selected_indices\n",
    "\n",
    "bvalsHCP7_1 = bvalsHCP[selected_indices7]\n",
    "bvecsHCP7_1 = bvecsHCP[selected_indices7]\n",
    "\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(14):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx = []\n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "selected_indices7 = selected_indices7+[t+69 for t in true_indx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a83e6-8263-453e-8b4e-d2793c666d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kk in tqdm.tqdm(range(9,33)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "\n",
    "    gtabHCP7 = gradient_table(gtabExt.bvals[selected_indices7],gtabExt.bvecs[selected_indices7])\n",
    "\n",
    "    ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "    NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "    torch.manual_seed(10)\n",
    "    DT_guess_new = invertComputeDTI(vals_to_mat(DT_guess.squeeze()))\n",
    "    KT_guess_new = KT_guess.squeeze()\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                tObs = TestData4D[i,j,axial_middle,selected_indices7]\n",
    "                avg_s0 = tObs[gtabHCP7.bvals==0].mean()\n",
    "                if(avg_s0<0):\n",
    "                    tObs = tObs + np.abs(tObs.min())\n",
    "                try:\n",
    "                    LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze(),np.log(100)]), args=(gtabHCP7, tObs)).x\n",
    "                    try:\n",
    "                        m, S, prec = vb_gauss_one_voxel(tObs, gtabHCP7, LS_x,mu0=mu0, V0=V0)\n",
    "                    except:\n",
    "                        m = 0\n",
    "                    NoiseEst[i,j] = m\n",
    "                except:\n",
    "                    NoiseEst[i,j] = 0\n",
    "                \n",
    "                #DT_guess_new,KT_guess_new = LS_x[:6],LS_x[6:21]\n",
    "    NoiseEstInv = np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEstInv[i,j] = np.hstack([mat_to_vals(ComputeDTI(NoiseEst[i,j,:6])),NoiseEst[i,j,6:21],NoiseEst[i,j,-1]])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEstInv[i,j]))),NoiseEstInv[i,j,6:]])\n",
    "    \n",
    "    MK_SBIFull  = np.zeros(ArrShape)\n",
    "    AK_SBIFull  = np.zeros(ArrShape)\n",
    "    RK_SBIFull  = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK_SBIFull[i,j] = Metrics[0]\n",
    "            AK_SBIFull[i,j] = Metrics[1]\n",
    "            RK_SBIFull[i,j] = Metrics[2]\n",
    "    np.save(DatFolder+'Min_MK_Bay_'+str(kk),np.array(MK_SBIFull,dtype=object))\n",
    "    np.save(DatFolder+'Min_AK_Bay_'+str(kk),np.array(AK_SBIFull,dtype=object))\n",
    "    np.save(DatFolder+'Min_RK_Bay_'+str(kk),np.array(RK_SBIFull,dtype=object))\n",
    "\n",
    "    fig,ax = plt.subplots(1,3)\n",
    "    ax[0].imshow(MK_SBIFull,vmin=0,vmax=1)\n",
    "    ax[1].imshow(AK_SBIFull,vmin=0,vmax=1)\n",
    "    ax[2].imshow(RK_SBIFull,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432eccd5-af31-4088-b7d0-c59fb2bec4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"say 'Other methods done'\") # or '\\7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9a7ad-b7aa-4ebd-8e8b-3c396bc72966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(3.2,4.8))\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "\n",
    "\n",
    "y_data = np.array(Prec7_NLLS)\n",
    "g_pos = np.array([2.5])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "#ax1.yaxis.set_ticks(np.arange(0.0005, 0.006, 0.002))\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.sca(ax2)\n",
    "\n",
    "y_data = np.array(PrecFull_SBI)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_SBI)\n",
    "g_pos = np.array([1.0])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI)\n",
    "g_pos = np.array([1.35])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(PrecFull_NLLS)\n",
    "g_pos = np.array([1.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_NLLS)\n",
    "g_pos = np.array([2.15])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_NLLS)\n",
    "g_pos = np.array([2.5])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "#ax1.set_ylim(0.4, 1)\n",
    "#ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax1.yaxis.set_ticks([0.4,0.7,1])\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "#ax2.set_ylim(0.0,0.3)\n",
    "#ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "#ax2.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "ax2.set_xlim([0.3,2.7])\n",
    "ax1.set_xlim(ax2.get_xlim())\n",
    "# Show plot\n",
    "ax2.set_ylim(0,0.9)\n",
    "ax2.set_yticks([0,0.4,0.8])\n",
    "ax1.set_ylim(1,13)\n",
    "\n",
    "x = np.arange(1.7,2.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS)[~np.isnan(PrecFull_NLLS)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS)[~np.isnan(PrecFull_NLLS)], 77)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.5,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI)[~np.isnan(PrecFull_SBI)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI)[~np.isnan(PrecFull_SBI)], 77)\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "plt.xticks([0.65,1,1.35,1.8,2.15,2.5],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "#if Save: plt.savefig(FigLoc+'DKI_MK_Prec.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f5305d-2bf5-4015-84a9-0aead2e0a3f4",
   "metadata": {},
   "source": [
    "# WLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dcefba-79fc-496e-a1c8-21c166114354",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullWLArr = []\n",
    "RKFullWLArr = []\n",
    "AKFullWLArr = []\n",
    "MKTFullWLArr = []\n",
    "KFAFullWLArr = []\n",
    "for kk in tqdm.tqdm(range(1,33)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gtabExt,fit_method='WLS')\n",
    "    dkifitNL = dkimodelNL.fit(TestData[:,:,:])\n",
    "\n",
    "    ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "    MK_SBIFull  = np.zeros(ArrShape)\n",
    "    AK_SBIFull  = np.zeros(ArrShape)\n",
    "    RK_SBIFull  = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Metrics = DKIMetrics(dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt)\n",
    "            MK_SBIFull[i,j] = Metrics[0]\n",
    "            AK_SBIFull[i,j] = Metrics[1]\n",
    "            RK_SBIFull[i,j] = Metrics[2]\n",
    "    MKFullWLArr.append(MK_SBIFull)\n",
    "    AKFullWLArr.append(AK_SBIFull)\n",
    "    RKFullWLArr.append(RK_SBIFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab298f95-a1b7-4faf-adac-a3274fe1e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "\n",
    "temp_bvecs = bvecsHCP[bvalsHCP>0]\n",
    "temp_bvals = bvalsHCP[bvalsHCP>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(18):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "temp = selected_indices\n",
    "\n",
    "bvalsHCP7_1 = np.insert(temp_bvals[temp],0,0)\n",
    "bvecsHCP7_1 = np.insert(temp_bvecs[temp],0,[0,0,0],axis=0)\n",
    "\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(27):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP20 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx_one = []\n",
    "for b in bvecsHCP7_1:\n",
    "    true_indx_one.append(np.linalg.norm(b-bvecsHCP,axis=1).argmin())\n",
    "true_indx = []        \n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "selected_indices20 = true_indx_one+[t+69 for t in true_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4dd85-ca3e-4214-a60c-4310a6ccc9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TD = []\n",
    "axial_middles = []\n",
    "masks = []\n",
    "WMs = []\n",
    "for kk in tqdm.tqdm(range(32)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk+1)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk+1)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk+1)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middles.append(axial_middle)\n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    TD.append(TestData4D)\n",
    "    masks.append(mask[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,axial_middle])\n",
    "    WM, affine, img = load_nifti('./flipped/c2Pat'+str(kk+1)+'_FP.nii', return_img=True)\n",
    "    WMs.append(np.fliplr(WM[:,:,axial_middle]>0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd694eaf-92da-4fb5-a8c5-0f91843b2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "gTabsF = []\n",
    "gTabs7 = []\n",
    "gTabs20 = []\n",
    "\n",
    "FullDat   = []\n",
    "\n",
    "for i in tqdm.tqdm(range(1,33)):\n",
    "    fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    gTabsF.append(gtabExt)\n",
    "    \n",
    "    bvalsHCP7 = gtabExt.bvals[selected_indices7]\n",
    "    bvecsHCP7 = gtabExt.bvecs[selected_indices7]\n",
    "    gtabHCP7 = gradient_table(bvalsHCP7, bvecsHCP7)\n",
    "    gTabs7.append(gtabHCP7)\n",
    "\n",
    "    bvalsHCP20 = gtabExt.bvals[selected_indices20]\n",
    "    bvecsHCP20 = gtabExt.bvecs[selected_indices20]\n",
    "    gtabHCP20 = gradient_table(bvalsHCP20, bvecsHCP20)\n",
    "    gTabs20.append(gtabHCP20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00df257-1e02-4c2f-b6b6-fa6438acbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullWLArr = []\n",
    "RKFullWLArr = []\n",
    "AKFullWLArr = []\n",
    "MKTFullWLArr = []\n",
    "KFAFullWLArr = []\n",
    "for kk in tqdm.tqdm(range(32)):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabsF[kk],fit_method='WLS')\n",
    "    dkifitNL = dkimodelNL.fit(TD[kk][:,:,axial_middles[kk],:])\n",
    "    ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7[i,j] = Metrics[0]\n",
    "            AK_NL7[i,j] = Metrics[1]\n",
    "            RK_NL7[i,j] = Metrics[2]\n",
    "            MKT_NL7[i,j] = Metrics[3]\n",
    "            KFA_NL7[i,j] = Metrics[4]\n",
    "    MKFullWLArr.append(MK_NL7)\n",
    "    RKFullWLArr.append(RK_NL7)\n",
    "    AKFullWLArr.append(AK_NL7)\n",
    "    MKTFullWLArr.append(MKT_NL7)\n",
    "    KFAFullWLArr.append(KFA_NL7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd9822-ce89-4811-b83e-9854c0de1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKMinWLArr = []\n",
    "RKMinWLArr = []\n",
    "AKMinWLArr = []\n",
    "MKTMinWLArr = []\n",
    "KFAMinWLArr = []\n",
    "for kk in tqdm.tqdm(range(32)):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabs7[kk],fit_method='WLS')\n",
    "    dkifitNL = dkimodelNL.fit(TD[kk][:,:,axial_middles[kk],selected_indices7])\n",
    "    ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7[i,j] = Metrics[0]\n",
    "            AK_NL7[i,j] = Metrics[1]\n",
    "            RK_NL7[i,j] = Metrics[2]\n",
    "            MKT_NL7[i,j] = Metrics[3]\n",
    "            KFA_NL7[i,j] = Metrics[4]\n",
    "    MKMinWLArr.append(MK_NL7)\n",
    "    RKMinWLArr.append(RK_NL7)\n",
    "    AKMinWLArr.append(AK_NL7)\n",
    "    MKTMinWLArr.append(MKT_NL7)\n",
    "    KFAMinWLArr.append(KFA_NL7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c92265-84fc-48eb-abe0-1a5e91eacbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullNLArr = []\n",
    "RKFullNLArr = []\n",
    "AKFullNLArr = []\n",
    "MKTFullNLArr = []\n",
    "KFAFullNLArr = []\n",
    "for kk in tqdm.tqdm(range(32)):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabsF[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(TD[kk][:,:,axial_middles[kk],:])\n",
    "    ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7[i,j] = Metrics[0]\n",
    "            AK_NL7[i,j] = Metrics[1]\n",
    "            RK_NL7[i,j] = Metrics[2]\n",
    "            MKT_NL7[i,j] = Metrics[3]\n",
    "            KFA_NL7[i,j] = Metrics[4]\n",
    "    MKFullNLArr.append(MK_NL7)\n",
    "    RKFullNLArr.append(RK_NL7)\n",
    "    AKFullNLArr.append(AK_NL7)\n",
    "    MKTFullNLArr.append(MKT_NL7)\n",
    "    KFAFullNLArr.append(KFA_NL7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b980b2-c1f6-4fc5-99b1-5db81e505756",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKMinNLArr = []\n",
    "RKMinNLArr = []\n",
    "AKMinNLArr = []\n",
    "MKTMinNLArr = []\n",
    "KFAMinNLArr = []\n",
    "for kk in tqdm.tqdm(range(32)):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabs7[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(TD[kk][:,:,axial_middles[kk],selected_indices7])\n",
    "    ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7[i,j] = Metrics[0]\n",
    "            AK_NL7[i,j] = Metrics[1]\n",
    "            RK_NL7[i,j] = Metrics[2]\n",
    "            MKT_NL7[i,j] = Metrics[3]\n",
    "            KFA_NL7[i,j] = Metrics[4]\n",
    "    MKMinNLArr.append(MK_NL7)\n",
    "    RKMinNLArr.append(RK_NL7)\n",
    "    AKMinNLArr.append(AK_NL7)\n",
    "    MKTMinNLArr.append(MKT_NL7)\n",
    "    KFAMinNLArr.append(KFA_NL7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa0726-a07b-46ad-bbc2-a074077b1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullNLArr = []\n",
    "RKFullNLArr = []\n",
    "AKFullNLArr = []\n",
    "MKTFullNLArr = []\n",
    "KFAFullNLArr = []\n",
    "for kk in tqdm.tqdm(range(1,33)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gtabExt,fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(TestData[:,:,:])\n",
    "\n",
    "    ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "    MK_SBIFull  = np.zeros(ArrShape)\n",
    "    AK_SBIFull  = np.zeros(ArrShape)\n",
    "    RK_SBIFull  = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Metrics = DKIMetrics(dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt)\n",
    "            MK_SBIFull[i,j] = Metrics[0]\n",
    "            AK_SBIFull[i,j] = Metrics[1]\n",
    "            RK_SBIFull[i,j] = Metrics[2]\n",
    "    MKFullNLArr.append(MK_SBIFull)\n",
    "    AKFullNLArr.append(AK_SBIFull)\n",
    "    RKFullNLArr.append(RK_SBIFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed4c7e-da43-422c-933c-9ec609e1a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK_Bay_Full = []\n",
    "AK_Bay_Full = []\n",
    "RK_Bay_Full = []\n",
    "for kk in range(1,33):\n",
    "    MK_Bay_Full.append(np.load(DatFolder+'Full_MK_Bay_'+str(kk)+'.npy',allow_pickle=True))\n",
    "    AK_Bay_Full.append(np.load(DatFolder+'Full_AK_Bay_'+str(kk)+'.npy',allow_pickle=True))\n",
    "    RK_Bay_Full.append(np.load(DatFolder+'Full_RK_Bay_'+str(kk)+'.npy',allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fba9ffb-63f7-41b0-ae3c-b71099f7fce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MK_Bay_Min = []\n",
    "AK_Bay_Min = []\n",
    "RK_Bay_Min = []\n",
    "for kk in range(1,33):\n",
    "    MK_Bay_Min.append(np.load(DatFolder+'Min_MK_Bay_'+str(kk)+'.npy',allow_pickle=True))\n",
    "    AK_Bay_Min.append(np.load(DatFolder+'Min_AK_Bay_'+str(kk)+'.npy',allow_pickle=True))\n",
    "    RK_Bay_Min.append(np.load(DatFolder+'Min_RK_Bay_'+str(kk)+'.npy',allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516b69c-b9c5-4f54-b58d-6a689cab979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK_Bay_Full = np.load(DatFolder+'Full_MK_HCP_Bay.npy',allow_pickle=True)\n",
    "AK_Bay_Full = np.load(DatFolder+'Full_AK_HCP_Bay.npy',allow_pickle=True)\n",
    "RK_Bay_Full = np.load(DatFolder+'Full_RK_HCP_Bay.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39cc4cf-ba62-45b5-b1fa-aff1bbd82c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK_MLE_Min = np.load(DatFolder+'Min_MK_HCP_MLE.npy',allow_pickle=True)\n",
    "AK_MLE_Min = np.load(DatFolder+'Min_AK_HCP_MLE.npy',allow_pickle=True)\n",
    "RK_MLE_Min = np.load(DatFolder+'Min_RK_HCP_MLE.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135369f-e603-44c6-967e-38a369ee01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK_MLE_Full = np.load(DatFolder+'Full_MK_HCP_MLE.npy',allow_pickle=True)\n",
    "AK_MLE_Full = np.load(DatFolder+'Full_AK_HCP_MLE.npy',allow_pickle=True)\n",
    "RK_MLE_Full = np.load(DatFolder+'Full_RK_HCP_MLE.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b308bc-822d-466c-8c63-d20647a6fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(DatFolder+'Min_MK_HCP_Bay',np.array(MK_Bay_Min,dtype=object))\n",
    "np.save(DatFolder+'Min_AK_HCP_Bay',np.array(AK_Bay_Min,dtype=object))\n",
    "np.save(DatFolder+'Min_RK_HCP_Bay',np.array(RK_Bay_Min,dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb0033a-3f3d-458c-996b-63985f6e4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKMinArr = np.load(DatFolder+'Min_MK_HCP.npy',allow_pickle=True)\n",
    "MKMidArr = np.load(DatFolder+'Mid_MK_HCP.npy',allow_pickle=True)\n",
    "MKFullArr = np.load(DatFolder+'Full_MK_HCP.npy',allow_pickle=True)\n",
    "\n",
    "AKMinArr = np.load(DatFolder+'Min_AK_HCP.npy',allow_pickle=True)\n",
    "AKMidArr = np.load(DatFolder+'Mid_AK_HCP.npy',allow_pickle=True)\n",
    "AKFullArr = np.load(DatFolder+'Full_AK_HCP.npy',allow_pickle=True)\n",
    "\n",
    "RKMinArr = np.load(DatFolder+'Min_RK_HCP.npy',allow_pickle=True)\n",
    "RKMidArr = np.load(DatFolder+'Mid_RK_HCP.npy',allow_pickle=True)\n",
    "RKFullArr = np.load(DatFolder+'Full_RK_HCP.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec2750-fcca-4b46-92fa-d1d797450aee",
   "metadata": {},
   "source": [
    "# Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3dc7f-9a34-4610-bc7e-d226ab40a011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52062669-7fc1-4ca4-90c7-f13524a9e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './Figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d8522-8907-49d2-9388-0f6bc051340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S11/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e0098-70b2-4c46-966c-dd4448884959",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = 1\n",
    "fdwi = './HCP_data/Pat'+str(kk)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(kk)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(kk)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(kk)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(kk)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(kk)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "_, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=True, dilate=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68869240-6bce-4608-a85b-8db253526338",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5cb76-9c5d-447a-8634-cd1324075744",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_other = []\n",
    "for kk in tqdm.tqdm(range(1,33)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    masks_other.append(mask2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e0a7a-d4dc-4e52-be72-189ce6828082",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_bay = []\n",
    "for i in range(8):\n",
    "    NS1 = MK_Bay_Min[i].astype(np.float32)\n",
    "    NS2 = MK_Bay_Full[i].astype(np.float32)\n",
    "    Ma = masks[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM_bay.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca179998-050b-4e2b-9314-dbc09ee1e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_bay = []\n",
    "for i in range(32):\n",
    "    NS1 = MK_Bay_Min[i].astype(np.float32)\n",
    "    NS2 = MK_Bay_Full[i].astype(np.float32)\n",
    "    Ma = masks[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_bay.append(result)\n",
    "\n",
    "SSIM_MLE = []\n",
    "for i in range(32):\n",
    "    NS1 = MK_MLE_Min[i].astype(np.float32)\n",
    "    NS2 = MK_MLE_Full[i].astype(np.float32)\n",
    "    Ma = masks[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_MLE.append(result)\n",
    "\n",
    "SSIM_NL = []\n",
    "for i in range(32):\n",
    "    NS1 = MKMinNLArr[i]\n",
    "    NS2 = MKFullNLArr[i]\n",
    "    Ma = masks_other[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_NL.append(result)\n",
    "\n",
    "SSIM_WL = []\n",
    "for i in range(32):\n",
    "    NS1 = MKMinWLArr[i]\n",
    "    NS2 = MKFullWLArr[i]\n",
    "    Ma = masks_other[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_WL.append(result)\n",
    "\n",
    "SSIM_SBI= []\n",
    "for i in range(32):\n",
    "    NS1 = MKMinArr[i]\n",
    "    NS2 = MKFullArr[i]\n",
    "    Ma = masks_other[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_SBI.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fbd2b-4e0b-4af0-bd21-a43de3f8e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLLSFit   = np.array([225,190,106])/255\n",
    "SBIFit   = np.array([64,176,166])/255\n",
    "WLSFit = np.array((192,108,132)) / 255  # muted violet\n",
    "MLEFit = np.array([70,100,150]) / 255\n",
    "BayFit = np.array([140,165,200])/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c59f4-68f0-4ab6-a70e-18e0390b9671",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "y_data = np.array(SSIM_SBI)\n",
    "g_pos = np.array([1])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM_NL)\n",
    "g_pos = np.array([2])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM_WL)\n",
    "g_pos = np.array([3])\n",
    "colors = [WLSFit]\n",
    "colors2 = ['lightsalmon']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "y_data = np.array(SSIM_MLE)\n",
    "g_pos = np.array([4])\n",
    "colors = [MLEFit]\n",
    "colors2 = ['lightsteelblue']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "y_data = np.array(SSIM_bay)\n",
    "g_pos = np.array([5])\n",
    "colors = [BayFit]\n",
    "colors2 = ['lavender']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "plt.yticks([0,0.2,0.4,0.6,0.8])\n",
    "plt.xticks([1,2,3,4,5],['SBI','NLLS','WLS','MLE','Bay.'],fontsize=32,rotation=90)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_MK_Other.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8ff4f-3e59-449b-bcdc-948e4302e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c93f15-543c-4760-ba03-7a90f8d7efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_bay = []\n",
    "for i in range(32):\n",
    "    NS1 = AK_Bay_Min[i].astype(np.float32)\n",
    "    NS2 = AK_Bay_Full[i].astype(np.float32)\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = masks[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_bay.append(result)\n",
    "\n",
    "SSIM_MLE = []\n",
    "for i in range(32):\n",
    "    NS1 = AK_MLE_Min[i].astype(np.float32)\n",
    "    NS2 = AK_MLE_Full[i].astype(np.float32)\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = masks[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_MLE.append(result)\n",
    "\n",
    "SSIM_NL = []\n",
    "for i in range(32):\n",
    "    NS1 = AKMinNLArr[i]\n",
    "    NS2 = AKFullNLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = masks_other[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_NL.append(result)\n",
    "\n",
    "SSIM_WL = []\n",
    "for i in range(32):\n",
    "    NS1 = AKMinWLArr[i]\n",
    "    NS2 = AKFullWLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = masks_other[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_WL.append(result)\n",
    "\n",
    "SSIM_SBI= []\n",
    "for i in range(32):\n",
    "    NS1 =AKMinArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =AKFullArr[i]\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = masks_other[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM_SBI.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75a6ac-2821-4423-8109-550b5d415fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "y_data = np.array(SSIM_SBI)\n",
    "g_pos = np.array([1])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM_NL)\n",
    "g_pos = np.array([2])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM_WL)\n",
    "g_pos = np.array([3])\n",
    "colors = [WLSFit]\n",
    "colors2 = ['lightsalmon']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "y_data = np.array(SSIM_MLE)\n",
    "g_pos = np.array([4])\n",
    "colors = [MLEFit]\n",
    "colors2 = ['lightsteelblue']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "y_data = np.array(SSIM_bay)\n",
    "g_pos = np.array([5])\n",
    "colors = [BayFit]\n",
    "colors2 = ['lavender']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "plt.yticks([0,0.2,0.4,0.6,0.8])\n",
    "plt.ylim([0,1])\n",
    "plt.xticks([1,2,3,4,5],['SBI','NLLS','WLS','MLE','Bay.'],fontsize=32,rotation=90)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_AK_Other.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86039d9b-40b9-47fc-a22d-1853aefd8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_bay = []\n",
    "for i in range(32):\n",
    "    NS1 = RK_Bay_Min[i].astype(np.float32)\n",
    "    NS2 = RK_Bay_Full[i].astype(np.float32)\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    Ma = masks[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_bay.append(result)\n",
    "\n",
    "SSIM_MLE = []\n",
    "for i in range(32):\n",
    "    NS1 = RK_MLE_Min[i].astype(np.float32)\n",
    "    NS2 = RK_MLE_Full[i].astype(np.float32)\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    Ma = masks[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_MLE.append(result)\n",
    "\n",
    "SSIM_NL = []\n",
    "for i in range(32):\n",
    "    NS1 = RKMinNLArr[i]\n",
    "    NS2 = RKFullNLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    Ma = masks_other[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_NL.append(result)\n",
    "\n",
    "SSIM_WL = []\n",
    "for i in range(32):\n",
    "    NS1 = RKMinWLArr[i]\n",
    "    NS2 = RKFullWLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    Ma = masks_other[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_WL.append(result)\n",
    "\n",
    "SSIM_SBI= []\n",
    "for i in range(32):\n",
    "    NS1 =RKMinArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =RKFullArr[i]\n",
    "    Ma = masks_other[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM_SBI.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dd77c6-580e-41d3-8913-0c145366a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "y_data = np.array(SSIM_SBI)\n",
    "g_pos = np.array([1])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM_NL)\n",
    "g_pos = np.array([2])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM_WL)\n",
    "g_pos = np.array([3])\n",
    "colors = [WLSFit]\n",
    "colors2 = ['lightsalmon']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "y_data = np.array(SSIM_MLE)\n",
    "g_pos = np.array([4])\n",
    "colors = [MLEFit]\n",
    "colors2 = ['lightsteelblue']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "y_data = np.array(SSIM_bay)\n",
    "g_pos = np.array([5])\n",
    "colors = [BayFit]\n",
    "colors2 = ['lavender']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "plt.yticks([0,0.2,0.4,0.6,0.8])\n",
    "plt.ylim([-0.1,1])\n",
    "plt.xticks([1,2,3,4,5],['SBI','NLLS','WLS','MLE','Bay.'],fontsize=32,rotation=90)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_RK_Other.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca056b-2d8e-40c5-8d37-396818feab8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
