{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d9056a-243c-40b6-9f22-d95ec3a5516d",
   "metadata": {},
   "source": [
    "#  Frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd004c5b-fe75-44b4-8e7e-4d4431f78841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.ticker as ticker\n",
    "from dwMRI_BasicFuncs import *\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.ndimage import gaussian_filter,binary_dilation\n",
    "\n",
    "# Define font properties\n",
    "font = {\n",
    "    'family': 'sans-serif',  # Use sans-serif family\n",
    "    'sans-serif': ['Helvetica'],  # Specify Helvetica as the sans-serif font\n",
    "    'size': 14  # Set the default font size\n",
    "}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# Set tick label sizes\n",
    "plt.rc('ytick', labelsize=24)\n",
    "plt.rc('xtick', labelsize=24)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n",
    "# Customize axes spines and legend appearance\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582716e0-036f-42a6-b902-c7d083d46ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_path = './Networks/'\n",
    "NoiseLevels = [None,20,10,5,2]\n",
    "\n",
    "TrainingSamples = 50000\n",
    "InferSamples    = 500\n",
    "\n",
    "lower_abs,upper_abs = -0.07,0.07\n",
    "lower_rest,upper_rest = -0.015,0.015\n",
    "lower_S0 = 25\n",
    "upper_S0 = 2000\n",
    "Save = True\n",
    "\n",
    "TrueCol  = 'k'\n",
    "NoisyCol = 'k'\n",
    "WLSFit   = 'sandybrown'\n",
    "SBIFit   = np.array([64,176,166])/255\n",
    "\n",
    "Errors_name = ['MD comparison','FA comparison','eig. comparison','Frobenius','Signal comparison','Correlation','Signal comparison','Correlation2']\n",
    "\n",
    "DatFolder = './SavedDat/'\n",
    "MSDir = './MS_data/'\n",
    "Save = False\n",
    "\n",
    "ChunkSize = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201f599-c463-45d0-805e-64537e9e03f8",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37a7c0-c69e-4f8c-9989-c4c7f811f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viol_plot(A,col,hatch=False,**kwargs):\n",
    "    A_T = np.transpose(A)\n",
    "    filtered_A = []\n",
    "    for column in A_T:\n",
    "        # Remove NaNs\n",
    "        column = column[~np.isnan(column)]\n",
    "        # Identify outliers using Z-score\n",
    "        z_scores = stats.zscore(column)\n",
    "        abs_z_scores = np.abs(z_scores)\n",
    "        # Filter data within 3 standard deviations\n",
    "        filtered_entries = (abs_z_scores < 1000)\n",
    "        filtered_column = column[filtered_entries]\n",
    "        filtered_A.append(filtered_column)\n",
    "    \n",
    "    vp = plt.violinplot(filtered_A,showmeans=True,**kwargs)  \n",
    "    for v in vp['bodies']:\n",
    "        v.set_facecolor(col)\n",
    "    vp['cbars'].set_color(col)\n",
    "    vp['cmins'].set_color(col)\n",
    "    vp['cmaxes'].set_color(col)\n",
    "    vp['cmeans'].set_color('black')\n",
    "    if(hatch):\n",
    "        vp['bodies'][0].set_hatch('//')\n",
    "def BoxPlots(y_data, positions, colors, colors2, ax,hatch = False,scatter=False,scatter_alpha=0.5, **kwargs):\n",
    "\n",
    "    GREY_DARK = \"#747473\"\n",
    "    jitter = 0.02\n",
    "    # Clean data to remove NaNs column-wise\n",
    "    if(np.ndim(y_data) == 1):\n",
    "        cleaned_data = y_data[~np.isnan(y_data)]\n",
    "    else:\n",
    "        cleaned_data = [d[~np.isnan(d)] for d in y_data]\n",
    "    \n",
    "    # Define properties for the boxes (patch objects)\n",
    "    boxprops = dict(\n",
    "        linewidth=2, \n",
    "        facecolor='none',       # use facecolor for filling (set to 'none' if you want no fill)\n",
    "        edgecolor='turquoise'   # edgecolor for the outline\n",
    "    )\n",
    "\n",
    "    # Define properties for the medians (Line2D objects)\n",
    "    # Ensure GREY_DARK is defined (or replace it with a color string)\n",
    "    medianprops = dict(\n",
    "        linewidth=2, \n",
    "        color=GREY_DARK,\n",
    "        solid_capstyle=\"butt\"\n",
    "    )\n",
    "\n",
    "    # For whiskers, since they are Line2D objects, use 'color'\n",
    "    whiskerprops = dict(\n",
    "        linewidth=2, \n",
    "        color='turquoise'\n",
    "    )\n",
    "\n",
    "    bplot = ax.boxplot(\n",
    "        cleaned_data,\n",
    "        positions=positions, \n",
    "        showfliers=False,\n",
    "        showcaps = False,\n",
    "        medianprops=medianprops,\n",
    "        whiskerprops=whiskerprops,\n",
    "        boxprops=boxprops,\n",
    "        patch_artist=True,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Update the color of each box (these are patch objects)\n",
    "    for i, box in enumerate(bplot['boxes']):\n",
    "        box.set_edgecolor(colors[i])\n",
    "        if(hatch):\n",
    "            box.set_hatch('/')\n",
    "    \n",
    "    \n",
    "    # Update the color of the whiskers (each box has 2 whiskers)\n",
    "    for i in range(len(positions)):\n",
    "        bplot['whiskers'][2*i].set_color(colors[i])\n",
    "        bplot['whiskers'][2*i+1].set_color(colors[i])\n",
    "    \n",
    "    # If caps are enabled, update their color (Line2D objects)\n",
    "    if 'caps' in bplot:\n",
    "        for i, cap in enumerate(bplot['caps']):\n",
    "            cap.set_color(colors[i//2])  # two caps per box\n",
    "\n",
    "    if(scatter):\n",
    "        if(np.ndim(cleaned_data) == 1):\n",
    "            x_data = np.array([positions] * len(cleaned_data))\n",
    "            x_jittered = x_data + stats.t(df=6, scale=jitter).rvs(len(x_data))\n",
    "            ax.scatter(x_data, cleaned_data, s=100, color=colors2, alpha=scatter_alpha)\n",
    "        else:\n",
    "            x_data = [np.array([positions[i]] * len(d)) for i, d in enumerate(cleaned_data)]\n",
    "            x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "            # Plot the scatter points with jitter (using colors2)\n",
    "            for x, y, c in zip(x_jittered, cleaned_data, colors2):\n",
    "                ax.scatter(x, y, s=100, color=c, alpha=scatter_alpha)\n",
    "def BoxPlots2(y_data, positions, colors, colors2, ax,hatch = False):\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "\n",
    "    jitter = 0.02\n",
    "    x_data = [np.array([positions[i]] * len(d)) for i, d in enumerate(y_data)]\n",
    "    x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    # Define properties for the boxes (patch objects)\n",
    "    boxprops = dict(\n",
    "        linewidth=2, \n",
    "        facecolor='none',       # use facecolor for filling (set to 'none' if you want no fill)\n",
    "        edgecolor='turquoise'   # edgecolor for the outline\n",
    "    )\n",
    "\n",
    "    # Define properties for the medians (Line2D objects)\n",
    "    # Ensure GREY_DARK is defined (or replace it with a color string)\n",
    "    medianprops = dict(\n",
    "        linewidth=2, \n",
    "        color='dimgray',  # Replace 'GREY_DARK' with an actual color if needed\n",
    "        solid_capstyle=\"butt\"\n",
    "    )\n",
    "\n",
    "    # For whiskers, since they are Line2D objects, use 'color'\n",
    "    whiskerprops = dict(\n",
    "        linewidth=2, \n",
    "        color='turquoise'\n",
    "    )\n",
    "\n",
    "    bplot = ax.boxplot(\n",
    "        y_data,\n",
    "        positions=positions, \n",
    "        showfliers=False,\n",
    "        showcaps=False,\n",
    "        showmeans=True,\n",
    "        medianprops=medianprops,\n",
    "        whiskerprops=whiskerprops,\n",
    "        boxprops=boxprops,\n",
    "        patch_artist=True\n",
    "    )\n",
    "\n",
    "    # Update the color of each box (these are patch objects)\n",
    "    for i, box in enumerate(bplot['boxes']):\n",
    "        box.set_edgecolor(colors[i])\n",
    "        if(hatch):\n",
    "            box.set_hatch('/')\n",
    "    \n",
    "    # Update the color of the medians (Line2D objects)\n",
    "    for i, median in enumerate(bplot['medians']):\n",
    "        median.set_color(colors[i])\n",
    "    \n",
    "    # Update the color of the whiskers (each box has 2 whiskers)\n",
    "    for i in range(len(positions)):\n",
    "        bplot['whiskers'][2*i].set_color(colors[i])\n",
    "        bplot['whiskers'][2*i+1].set_color(colors[i])\n",
    "    \n",
    "    # If caps are enabled, update their color (Line2D objects)\n",
    "    if 'caps' in bplot:\n",
    "        for i, cap in enumerate(bplot['caps']):\n",
    "            cap.set_color(colors[i//2])  # two caps per box\n",
    "\n",
    "    # Plot the scatter points with jitter (using colors2)\n",
    "    for x, y, c in zip(x_jittered, y_data, colors2):\n",
    "        ax.scatter(x, y, s=100, color=c, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ebb122-f0f7-4241-bd6b-7311ea641291",
   "metadata": {},
   "source": [
    "## DKI Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c43864-45fe-4508-bf25-169da1d71185",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 1\n",
    "fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "\n",
    "data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "# Get the indices of True values\n",
    "true_indices = np.argwhere(mask)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = true_indices.min(axis=0)\n",
    "max_coords = true_indices.max(axis=0)\n",
    "\n",
    "maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "FlatTD = TestData.reshape(maskdata.shape[0]*maskdata.shape[1],138)\n",
    "FlatTD = FlatTD[FlatTD[:,:69].sum(axis=-1)>0]\n",
    "FlatTD = FlatTD[~np.array(FlatTD<0).any(axis=-1)]\n",
    "\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabExt)\n",
    "tenfit = dkimodel.fit(FlatTD)\n",
    "DKIHCP = tenfit.kt\n",
    "DTIHCP = tenfit.lower_triangular()\n",
    "DKIFull = np.array(DKIHCP)\n",
    "DTIFull = np.array(DTIHCP)\n",
    "\n",
    "\n",
    "DTIFilt1 = DTIFull[(abs(DKIFull)<10).all(axis=1)]\n",
    "DKIFilt1 = DKIFull[(abs(DKIFull)<10).all(axis=1)]\n",
    "DTIFilt = DTIFilt1[(DKIFilt1>-3/7).all(axis=1)]\n",
    "DKIFilt = DKIFilt1[(DKIFilt1>-3/7).all(axis=1)]\n",
    "\n",
    "TrueMets = []\n",
    "FA       = []\n",
    "for (dt,kt) in tqdm(zip(DTIFilt,DKIFilt)):\n",
    "    TrueMets.append(DKIMetrics(dt,kt))\n",
    "    FA.append(FracAni(np.linalg.eigh(vals_to_mat(dt))[0],np.mean(np.linalg.eigh(vals_to_mat(dt))[0])))\n",
    "TrueMets = np.array(TrueMets)\n",
    "TrueFA = np.array(FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72411e4-ae6e-4159-acfc-6d9a6ae612bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full fit\n",
    "DT1_full,DT2_full = FitDT(DTIFilt,1)\n",
    "x4_full,R1_full,x2_full,R2_full = FitKT(DKIFilt,1)\n",
    "\n",
    "# LowFA Fit\n",
    "DT1_lfa,DT2_lfa = FitDT(DTIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "x4_lfa,R1_lfa,x2_lfa,R2_lfa = FitKT(DKIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "\n",
    "# HighFA Fit\n",
    "DT1_hfa,DT2_hfa = FitDT(DTIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "x4_hfa,R1_hfa,x2_hfa,R2_hfa = FitKT(DKIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "\n",
    "# UltraLowFA Fit\n",
    "DT1_ulfa,DT2_ulfa = FitDT(DTIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa = FitKT(DKIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "\n",
    "# HigherAK Fit\n",
    "DT1_hak,DT2_hak = FitDT(DTIFilt[TrueMets[:,1]>0.9,:],1)\n",
    "x4_hak,R1_hak,x2_hak,R2_hak = FitKT(DKIFilt[TrueMets[:,1]>0.9,:],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b5446-66fa-485d-8c0d-41128f3c4783",
   "metadata": {},
   "source": [
    "# Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e8ddf-4666-4712-8064-617b8ec7cf7d",
   "metadata": {},
   "source": [
    "## DTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768f1ee-1ac0-43bf-8efe-aae895b56b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prior = DTIPriorS0(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0)\n",
    "priorS0, *_ = process_prior(custom_prior) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2143a9d-1824-4c5e-a9d1-9935d9b8d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial = HemiSphere(xyz=bvecs[1:])\n",
    "hsph_initial7 = HemiSphere(xyz=bvecs[1:7])\n",
    "hsph_updated,potentials = disperse_charges(hsph_initial,5000)\n",
    "hsph_updated7,potentials = disperse_charges(hsph_initial7,5000)\n",
    "\n",
    "gtabSimF = gradient_table(np.array([0]+[1000]*64).squeeze(), np.vstack([[0,0,0],hsph_updated.vertices]))\n",
    "gtabSim7 = gradient_table(np.array([0]+[1000]*6).squeeze(), np.vstack([[0,0,0],hsph_updated7.vertices]))\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "gTabs = [gtabSimF]\n",
    "for _ in range(4):\n",
    "    x = np.random.permutation(np.arange(65))\n",
    "    bvecs_shuffle = gtabSimF.bvecs[x]\n",
    "    bvals_shuffle = gtabSimF.bvals[x]\n",
    "    \n",
    "    gTabs.append(gradient_table(bvals_shuffle, bvecs_shuffle))\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "params = priorS0.sample()\n",
    "dtTruth = ComputeDTI(params)\n",
    "dtTruth = ForceLowFA(dtTruth)\n",
    "Truth = CustomSimulator(dtTruth,gtabSimF,S0=200,snr=None)\n",
    "\n",
    "    \n",
    "dt_evals,dt_evecs = np.linalg.eigh(dtTruth)\n",
    "\n",
    "SNR = [CustomSimulator(dtTruth,gtabSimF, S0=200,snr=scale) for scale in NoiseLevels[1:]]\n",
    "    \n",
    "SNR = np.array(SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef731e0-6f40-42b4-8551-a0090d14620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "SNR20 = np.vstack([CustomSimulator(dtTruth,gtabSimF, S0=200,snr=20) for k in range(200)])\n",
    "SNR10 = np.vstack([CustomSimulator(dtTruth,gtabSimF, S0=200,snr=10) for k in range(200)])\n",
    "SNR5 = np.vstack([CustomSimulator(dtTruth,gtabSimF, S0=200,snr=5) for k in range(200)])\n",
    "SNR2 = np.vstack([CustomSimulator(dtTruth,gtabSimF, S0=200,snr=2) for k in range(200)])\n",
    "\n",
    "tenmodel = dti.TensorModel(gtabSimF,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(SNR20)\n",
    "FA20 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD20 = dti.mean_diffusivity(tenfit.evals)\n",
    "tenfit = tenmodel.fit(SNR10)\n",
    "FA10 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD10 = dti.mean_diffusivity(tenfit.evals)\n",
    "tenfit = tenmodel.fit(SNR5)\n",
    "FA5 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD5 = dti.mean_diffusivity(tenfit.evals)\n",
    "tenfit = tenmodel.fit(SNR2)\n",
    "FA2 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD2 = dti.mean_diffusivity(tenfit.evals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fec708a-309d-4fa7-b6df-c4a783544e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimFull.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSimF,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTISimFull.pickle\"):\n",
    "        with open(f\"{network_path}/DTISimFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimMin.pickle\", \"rb\") as handle:\n",
    "        posterior7 = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSim7,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior7 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTISimMin.pickle\"):\n",
    "        with open(f\"{network_path}/DTISimMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfefd201-09c0-489c-a70e-99056f4a923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "MD20_SBI = []\n",
    "FA20_SBI = []\n",
    "for S in tqdm(SNR20):\n",
    "    posterior_samples_1 = posteriorFull.sample((InferSamples,), x=S,show_progress_bars=False)\n",
    "    Guess = vals_to_mat(posterior_samples_1.mean(axis=0))\n",
    "    Guess_clean = clip_negative_eigenvalues(Guess)\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess_clean)\n",
    "    MD20_SBI.append(np.mean(evals_guess_raw))\n",
    "    FA20_SBI.append(FracAni(evals_guess_raw,MD20_SBI[-1]))\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "MD10_SBI = []\n",
    "FA10_SBI = []\n",
    "for S in tqdm(SNR10):\n",
    "    posterior_samples_1 = posteriorFull.sample((InferSamples,), x=S,show_progress_bars=False)\n",
    "    Guess = vals_to_mat(posterior_samples_1.mean(axis=0))\n",
    "    Guess_clean = clip_negative_eigenvalues(Guess)\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess_clean)\n",
    "    if((evals_guess_raw<0).any()): print(True)\n",
    "    MD10_SBI.append(np.mean(evals_guess_raw))\n",
    "    FA10_SBI.append(FracAni(evals_guess_raw,MD10_SBI[-1]))\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "MD5_SBI = []\n",
    "\n",
    "\n",
    "FA5_SBI = []\n",
    "for S in tqdm(SNR5):\n",
    "    posterior_samples_1 = posteriorFull.sample((InferSamples,), x=S,show_progress_bars=False)\n",
    "    Guess = vals_to_mat(posterior_samples_1.mean(axis=0))\n",
    "    Guess_clean = clip_negative_eigenvalues(Guess)\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess_clean)\n",
    "    if((evals_guess_raw<0).any()): print(True)\n",
    "    MD5_SBI.append(np.mean(evals_guess_raw))\n",
    "    FA5_SBI.append(FracAni(evals_guess_raw,MD5_SBI[-1]))\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "MD2_SBI = []\n",
    "FA2_SBI = []\n",
    "for S in tqdm(SNR2):\n",
    "    posterior_samples_1 = posteriorFull.sample((InferSamples,), x=S,show_progress_bars=False)\n",
    "    Guess = vals_to_mat(posterior_samples_1.mean(axis=0))\n",
    "    Guess_clean = clip_negative_eigenvalues(Guess)\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess_clean)\n",
    "    if((evals_guess_raw<0).any()): print(True)\n",
    "    MD2_SBI.append(np.mean(evals_guess_raw))\n",
    "    FA2_SBI.append(FracAni(evals_guess_raw,MD2_SBI[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb2040-3b99-49ba-b02c-986448fe1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "Samples  = []\n",
    "DTISim = []\n",
    "S0Sim    = []\n",
    "\n",
    "params = priorS0.sample([500])\n",
    "for i in tqdm(range(500)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim.append(dt)\n",
    "    S0Sim.append(params[i,-1])\n",
    "    Samples.append([CustomSimulator(dt,gtabSimF, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples = np.array(Samples).squeeze()\n",
    "Samples = np.moveaxis(Samples, 0, -1)\n",
    "\n",
    "Samples7  = []\n",
    "DTISim7 = []\n",
    "S0Sim7    = []\n",
    "\n",
    "params = priorS0.sample([500])\n",
    "for i in tqdm(range(500)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim7.append(dt)\n",
    "    S0Sim7.append(params[i,-1])\n",
    "    Samples7.append([CustomSimulator(dt,gtabSim7, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples7 = np.array(Samples7).squeeze()\n",
    "Samples7 = np.moveaxis(Samples7, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf407048-5c6d-4263-adc2-da6b083f13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "ErrorFull = []\n",
    "NoiseApproxFull = []\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim[i])\n",
    "        tObs = Samples[k,:,i]\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSimF, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "        mat_guess = clip_negative_eigenvalues(mat_guess)\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSimF,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApproxFull.append(ENoise)\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "NoiseApproxFull = np.array(NoiseApproxFull)    \n",
    "\n",
    "Error_s = []\n",
    "for k,gtab,Samps,DTIS in zip([65,7],[gtabSimF,gtabSim7],[Samples,Samples7],[DTISim,DTISim7]):\n",
    "    tenmodel = dti.TensorModel(gtab,fit_method='NLLS')\n",
    "    Error_n = []\n",
    "    for S,Noise in zip(Samps,NoiseLevels):\n",
    "        Error = []\n",
    "        for i in range(500):\n",
    "            tenfit = tenmodel.fit(S[:,i])\n",
    "            tensor_vals = dti.lower_triangular(tenfit.quadratic_form)\n",
    "            DT_test = vals_to_mat(tensor_vals)\n",
    "            Error.append(Errors(DT_test,DTIS[i],gtab,Samps[0][:,i],S[:,i]))\n",
    "        Error_n.append(Error)\n",
    "    Error_s.append(Error_n)\n",
    "Error_s = np.array(Error_s)\n",
    "Error_s = np.swapaxes(Error_s,0,1)\n",
    "\n",
    "torch.manual_seed(10)\n",
    "Error7 = []\n",
    "NoiseApprox7 = []\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim7[i])\n",
    "        tObs = Samples7[k,:,i]\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSim7, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posterior7.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSim7,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApprox7.append(ENoise)\n",
    "    Error7.append(ErrorN2)\n",
    "\n",
    "NoiseApprox7 = np.array(NoiseApprox7)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569bc1f8-afaa-4f9a-8781-57016bfffbb5",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad238656-d10f-4c31-9c68-9c0718480175",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(Truth,'k',lw=2,label='True signal')\n",
    "plt.plot(SNR[0],'gray',lw=2,ls='--',label='Noisy signal')\n",
    "plt.axis('off')\n",
    "legend= plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1.03,1.7),fontsize=26,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "for handle in legend.get_lines():\n",
    "    handle.set_linewidth(6)  # Set desired linewidth\n",
    "if Save: plt.savefig(FigLoc+'EgSig20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(Truth,'k',lw=2)\n",
    "plt.plot(SNR[1],'gray',lw=2,ls='--')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'EgSig10.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(Truth,'k',lw=2)\n",
    "plt.plot(SNR[2],'gray',lw=2,ls='--')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'EgSig5.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(Truth,'k',lw=2)\n",
    "plt.plot(SNR[3],'gray',lw=2,ls='--')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'EgSig2.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1b29f4-0dbd-48a5-a615-b0afa8a8a094",
   "metadata": {},
   "source": [
    "### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21098222-6731-427a-9b20-36a26af39f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(6.4,2.4))\n",
    "y_data = np.array([FA20_SBI,FA10_SBI,FA5_SBI,FA2_SBI])\n",
    "g_pos = np.array([1.3,2.3,3.3,4.3])\n",
    "\n",
    "colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.3,scatter=True)\n",
    "\n",
    "g_pos = np.array([1,2,3,4])\n",
    "colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "y_data = np.array([FA20,FA10,FA5,FA2])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.3,scatter=True)\n",
    "\n",
    "l = plt.axhline(FracAni(dt_evals,np.mean(dt_evals)),c='k',lw=3,ls='--',label='True FA')\n",
    "plt.xticks([1,2,3,4],[20,10,5,2],fontsize=28)\n",
    "plt.xticks(fontsize=28)\n",
    "#plt.xlabel('SNR',fontsize=32)\n",
    "#plt.ylabel('FA',fontsize=32)\n",
    "leg_patch1 = mpatches.Patch(color='lightseagreen', label='SBI Fit')\n",
    "leg_patch2 = mpatches.Patch(color='sandybrown', label='NLLS Fit')\n",
    "ax.legend(\n",
    "    handles=[leg_patch1],\n",
    "    loc='upper left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=1,\n",
    "fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,bbox_to_anchor=(0,1))\n",
    "plt.yticks([0,1])\n",
    "if Save: plt.savefig(FigLoc+'EgNoiseFA.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c098a3-71d5-45b5-9a7c-e40af83890a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(6.4,2.4))\n",
    "y_data = np.array([MD20_SBI,MD10_SBI,MD5_SBI,MD2_SBI])\n",
    "g_pos = np.array([1.3,2.3,3.3,4.3])\n",
    "\n",
    "colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.3,scatter=True)\n",
    "\n",
    "g_pos = np.array([1,2,3,4])\n",
    "colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "y_data = np.array([MD20,MD10,MD5,MD2])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.3,scatter=True)\n",
    "\n",
    "l = plt.axhline(np.mean(dt_evals),c='k',lw=3,ls='--',label='True MD')\n",
    "plt.xticks([])\n",
    "#plt.xticks(fontsize=28)\n",
    "#plt.xlabel('SNR',fontsize=32)\n",
    "#plt.ylabel('MD',fontsize=32)\n",
    "leg_patch2 = mpatches.Patch(color='sandybrown', label='NLLS Fit')\n",
    "ax.legend(\n",
    "    handles=[leg_patch2],\n",
    "    loc='upper left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=1,\n",
    "fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,bbox_to_anchor=(0,0.5))\n",
    "plt.yticks([0,0.001,0.002])\n",
    "plt.ylim((0, 0.0025))\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "if Save: plt.savefig(FigLoc+'EgNoiseMD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad5211-cbca-4c65-81ba-bc5a3f8f51c1",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fc835-6c5a-4973-911c-de7d300917c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(9,3),constrained_layout=True)\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(ErrorFull).T,Errors_name)):\n",
    "    y_data = E[:,1:]\n",
    "    g_pos = np.array([1.3,2.3,3.3,4.3])\n",
    "    colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "    colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "    \n",
    "    BoxPlots(y_data.T,g_pos,colors,colors2,a,widths=0.3,scatter=False)\n",
    "    y_data = Error_s[1:,0,:,ll].T\n",
    "    g_pos = np.array([1,2,3,4])\n",
    "    colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "    colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "    \n",
    "    BoxPlots(y_data.T,g_pos,colors,colors2,a,widths=0.3,scatter=False)\n",
    "\n",
    "\n",
    "    plt.sca(a)\n",
    "    if(ll>3):\n",
    "        plt.xlabel('SNR', fontsize=24)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:])\n",
    "    #ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.yticks(fontsize=32)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "\n",
    "    if(ll==0):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(ll==1):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=WLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "        plt.ylim([-0.05,1])\n",
    "        plt.yticks([0,1])\n",
    "    plt.grid()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'SimDatDTIErrors1.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1984e507-205a-47ff-8f84-51f59d507ac5",
   "metadata": {},
   "source": [
    "### d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81cc83-bdff-4a94-a2bd-d6bde9b04ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(9,3),constrained_layout=True)\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(Error7).T,Errors_name)):\n",
    "    y_data = E[:,1:]\n",
    "    g_pos = np.array([1.3,2.3,3.3,4.3])\n",
    "    colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "    colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "    \n",
    "    BoxPlots(y_data.T,g_pos,colors,colors2,a,widths=0.3,scatter=False)\n",
    "    y_data = Error_s[1:,-1,:,ll].T\n",
    "    g_pos = np.array([1,2,3,4])\n",
    "    colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "    colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "    \n",
    "    BoxPlots(y_data.T,g_pos,colors,colors2,a,widths=0.3,scatter=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.sca(a)\n",
    "    if(ll>3):\n",
    "        plt.xlabel('SNR', fontsize=24)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:])\n",
    "    #ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.yticks(fontsize=32)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "\n",
    "    if(ll==0):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(ll==1):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=WLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "        plt.ylim([-0.05,1])\n",
    "        plt.yticks([0,1])\n",
    "    plt.grid()\n",
    "#plt.tight_layout()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'SimDatDTIErrors2.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88c76da-a62c-4af6-98a3-529438c888b5",
   "metadata": {},
   "source": [
    "## DKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f5ad03-cb8d-4455-a84b-fadbe455d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial = HemiSphere(xyz=bvecs[1:])\n",
    "hsph_updated,_ = disperse_charges(hsph_initial,5000)\n",
    "bvecsExt = np.vstack([[0,0,0],hsph_updated.vertices])\n",
    "bvalsExt = np.hstack([bvals, 3000*np.ones_like(bvals)])\n",
    "bvecsExt = np.vstack([bvecsExt, bvecsExt])\n",
    "bvalsExt[65] = 0\n",
    "gtabSim = gradient_table(bvalsExt, bvecsExt)\n",
    "\n",
    "\n",
    "hsph_initial15 = HemiSphere(xyz=bvecs[1:16])\n",
    "hsph_initial7 = HemiSphere(xyz=bvecs[1:7])\n",
    "hsph_updated15,_ = disperse_charges(hsph_initial15,5000)\n",
    "hsph_updated7,_ = disperse_charges(hsph_initial7,5000)\n",
    "gtabSimSub = gradient_table(np.array([0]+[1000]*6+[3000]*15).squeeze(), np.vstack([[0,0,0],hsph_updated7.vertices,hsph_updated15.vertices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097fdf7-af71-4ea7-8c2d-7c733aff68d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "j = 1\n",
    "vL = torch.tensor([0.2*j])\n",
    "vS = torch.tensor([0.01*j])  \n",
    "\n",
    "kk = np.random.randint(0,4)\n",
    "if(kk==0):\n",
    "    DT,KT = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],2,1)\n",
    "elif(kk==1):\n",
    "    DT,KT = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],2,1)\n",
    "elif(kk==2):\n",
    "    DT,KT = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],2,1)\n",
    "elif(kk==3):\n",
    "    DT,KT = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],2,1)\n",
    "\n",
    "tObs = CustomDKISimulator(DT.squeeze(),KT.squeeze(),gtabSim,200,20)\n",
    "tObs7 = CustomDKISimulator(np.squeeze(DT),np.squeeze(KT),gtabSimSub,200,20)\n",
    "tTrue = CustomDKISimulator(DT.squeeze(),KT.squeeze(),gtabSim,200,None)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],1,50)\n",
    "DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],1,50)\n",
    "DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,100)\n",
    "\n",
    "SampsDT = np.vstack([DT2,DT3,DT5])\n",
    "SampsKT = np.vstack([KT2,KT3,KT5])\n",
    "\n",
    "Samples  = []\n",
    "for Sd,Sk in zip(SampsDT,SampsKT):\n",
    "    Samples.append([CustomDKISimulator(Sd,Sk,gtabSim, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "\n",
    "Samples = np.array(Samples)\n",
    "\n",
    "Samples7  = []\n",
    "for Sd,Sk in zip(SampsDT,SampsKT):\n",
    "    Samples7.append([CustomDKISimulator(Sd,Sk,gtabSimSub, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "\n",
    "Samples7 = np.array(Samples7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05b039-e999-45e1-8427-f23c25734da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DKISimFull.pickle\"):\n",
    "    with open(f\"{network_path}/DKISimFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*6000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*2000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*6000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*6000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(2.5*6000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabSim.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabSim,200,np.random.rand()*30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>800).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKISimFull.pickle\"):\n",
    "        with open(f\"{network_path}/DKISimFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n",
    "\n",
    "if os.path.exists(f\"{network_path}/DKISimMin.pickle\"):\n",
    "    with open(f\"{network_path}/DKISimMin.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*6000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*2000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*6000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*6000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(2.5*6000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabSimSub.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabSimSub,200,np.random.rand()*30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>800).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKISimMin.pickle\"):\n",
    "        with open(f\"{network_path}/DKISimMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorMin, handle)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c051f3-ce37-4f50-bd36-ee0f81cf4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "ErrorFull = []\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=Samples[i,k,:],show_progress_bars=False)\n",
    "        GuessSBI = posterior_samples_1.mean(axis=0)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(GuessSBI[:6],GuessSBI[6:],SampsDT[i],SampsKT[i]))\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "Error_s = []\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabSim,fit_method='NLLS')\n",
    "\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tenfit = dkimodel.fit(Samples[i,k,:])\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(tenfit.lower_triangular(),tenfit.kt,SampsDT[i],SampsKT[i]))\n",
    "    Error_s.append(ErrorN2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9338f539-73e2-4118-ad21-fd0f2e316405",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "ErrorMin = []\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        posterior_samples_1 = posteriorMin.sample((InferSamples,), x=Samples7[i,k,:],show_progress_bars=False)\n",
    "        GuessSBI = posterior_samples_1.mean(axis=0)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(GuessSBI[:6],GuessSBI[6:],SampsDT[i],SampsKT[i]))\n",
    "    ErrorMin.append(ErrorN2)\n",
    "\n",
    "Error_s_min = []\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tenfit = dkimodel.fit(Samples7[i,k,:])\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(tenfit.lower_triangular(),tenfit.kt,SampsDT[i],SampsKT[i]))\n",
    "    Error_s_min.append(ErrorN2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe4d9e-4f9d-4c7f-91d4-05d7b11cc9ee",
   "metadata": {},
   "source": [
    "### e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80207429-7e0c-47a9-ad66-81849dea6a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=True)\n",
    "GuessDKI = posterior_samples_1.mean(axis=0)\n",
    "GuessSig = CustomDKISimulator(GuessDKI[:6],GuessDKI[6:],gtabSim,200)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k',label='True signal')\n",
    "plt.plot(GuessSig,lw=2,c=SBIFit,ls='--',label='SBI Recon.')\n",
    "plt.axis('off')\n",
    "legend = plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1,1.95),fontsize=26,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "for handle in legend.get_lines():\n",
    "    handle.set_linewidth(6)  # Set desired linewidth\n",
    "if Save: plt.savefig(FigLoc+'FullReconSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2ddf2-7d84-446a-980c-d8270f044791",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodel = dki.DiffusionKurtosisModel(gtabSim,fit_method='NLLS')\n",
    "tenfit = dkimodel.fit(tObs)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k')\n",
    "plt.plot(tenfit.predict(gtabSim,200),lw=2,c=WLSFit,ls='--',label='NLLS Recon.')\n",
    "plt.axis('off')\n",
    "legend = plt.legend(ncols=2,loc=1,bbox_to_anchor =  (0.9,1.95),fontsize=26,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "for handle in legend.get_lines():\n",
    "    handle.set_linewidth(6)  # Set desired linewidth\n",
    "if Save: plt.savefig(FigLoc+'FullReconWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ea351-ffd5-4f2f-a393-1871cf9bf9aa",
   "metadata": {},
   "source": [
    "### f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc667d4-bee1-49ac-adbf-d5c001ce769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorFull = np.array(ErrorFull)\n",
    "Error_s = np.array(Error_s)\n",
    "ErrorNames = ['MK Error', 'AK Error', 'RK Error', 'MKT Error', 'KFA Error']\n",
    "fig,ax = plt.subplots(1,3,figsize=(13.5,3))\n",
    "for i in range(3):\n",
    "    plt.sca(ax[i])\n",
    "    g_pos = np.array([1.3,2.3,3.3,4.3])\n",
    "    colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "    colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "    BoxPlots(ErrorFull[1:,:,i],g_pos,colors,colors2,ax[i],widths=0.3,scatter=False)\n",
    "    g_pos = np.array([1,2,3,4])\n",
    "    colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "    colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "    BoxPlots(Error_s[1:,:,i],g_pos,colors,colors2,ax[i],widths=0.3,scatter=False)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    plt.yticks(fontsize=32)\n",
    "    \n",
    "    if(i==0):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),Line2D([0], [0], color=WLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(-0.1,1.1),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'ErrorsFull.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06777e3-b322-4db1-8340-fa5735e2c411",
   "metadata": {},
   "source": [
    "### g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44879b7d-ea19-4df5-b8da-fd515d5f71e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "posterior_samples_1 = posteriorMin.sample((InferSamples,), x=tObs7,show_progress_bars=True)\n",
    "GuessDKI = posterior_samples_1.mean(axis=0)\n",
    "GuessSig = CustomDKISimulator(GuessDKI[:6],GuessDKI[6:],gtabSim,200)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k',label='True signal')\n",
    "plt.plot(GuessSig,lw=2,c=SBIFit,ls='--',label='SBI Recon.')\n",
    "plt.axis('off')\n",
    "plt.fill_betweenx(np.arange(0,500,50),0*np.ones(10),7*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.fill_betweenx(np.arange(0,500,50),64*np.ones(10),79*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.ylim(-9.996985449425491, 209.99985644997255)\n",
    "legend = plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1,1.95),fontsize=26,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "for handle in legend.get_lines():\n",
    "    handle.set_linewidth(6)  # Set desired linewidth\n",
    "if Save: plt.savefig(FigLoc+'7ReconSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c9c28-e59a-4a22-9045-652fe144f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "tenfit = dkimodel.fit(tObs7)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k')\n",
    "plt.plot(tenfit.predict(gtabSim,200),lw=2,c=WLSFit,ls='--',label='NLLS Recon.')\n",
    "plt.axis('off')\n",
    "legend = plt.legend(ncols=2,loc=1,bbox_to_anchor =  (0.9,1.95),fontsize=26,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "for handle in legend.get_lines():\n",
    "    handle.set_linewidth(6)  # Set desired linewidth\n",
    "plt.fill_betweenx(np.arange(0,500,50),0*np.ones(10),7*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.fill_betweenx(np.arange(0,500,50),64*np.ones(10),79*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.ylim(-9.996985449425491, 209.99985644997255)\n",
    "if Save: plt.savefig(FigLoc+'7ReconWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be06a36-7964-4d21-bf83-1efa173e2794",
   "metadata": {},
   "source": [
    "### h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb4edd-c239-492e-b817-ab1d7a7da933",
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorFull = np.array(ErrorMin)\n",
    "Error_s = np.array(Error_s_min)\n",
    "ErrorNames = ['MK Error', 'AK Error', 'RK Error', 'MKT Error', 'KFA Error']\n",
    "fig,ax = plt.subplots(1,3,figsize=(13.5,3))\n",
    "for i in range(3):\n",
    "    plt.sca(ax[i])\n",
    "    g_pos = np.array([1.3,2.3,3.3,4.3])\n",
    "    colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "    colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "    BoxPlots(ErrorFull[1:,:,i],g_pos,colors,colors2,ax[i],widths=0.3,scatter=False)\n",
    "    g_pos = np.array([1,2,3,4])\n",
    "    colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "    colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "    BoxPlots(Error_s[1:,:,i],g_pos,colors,colors2,ax[i],widths=0.3,scatter=False)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    plt.yticks(fontsize=32)\n",
    "    if(i==0):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),Line2D([0], [0], color=WLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(-0.1,1.1),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,labelspacing=0.1)\n",
    "if Save: plt.savefig(FigLoc+'ErrorsMin.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72e512-9b8b-4419-801f-394adfca2496",
   "metadata": {},
   "source": [
    "# Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6353a0ae-086d-44eb-8760-d4ad77a66bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prior = DTIPriorS0Direc(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0)\n",
    "priorDirec, *_ = process_prior(custom_prior) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1312e820-77b8-4d05-878c-9a01b6b04365",
   "metadata": {},
   "source": [
    "## HCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff9e02-c011-442f-ac1f-0f44c58f390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdwi = './HCP_data/Pat'+str(1)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(1)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(1)+'/bvecs_1k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "mask_cutout = np.copy(mask[:,:,axial_middle])\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices_alt = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices_alt))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices_alt], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices_alt.append(next_index)\n",
    "\n",
    "selected_indices_alt = [0]+selected_indices_alt\n",
    "\n",
    "bvalsHCP7_alt = bvalsHCP[selected_indices_alt]\n",
    "bvecsHCP7_alt = bvecsHCP[selected_indices_alt]\n",
    "gtabHCP7_alt = gradient_table(bvalsHCP7_alt, bvecsHCP7_alt)\n",
    "\n",
    "custom_prior = DTIPriorS0Noise(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0,0,30)\n",
    "priorS0Noise, *_ = process_prior(custom_prior) \n",
    "\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(6):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices20 = [0]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(19):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices20))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices20], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices20.append(next_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0118b-9010-466a-b6c2-36105e10e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIHCPFull.pickle\"):\n",
    "    with open(f\"{network_path}/DTIHCPFull.pickle\", \"rb\") as handle:\n",
    "        posterior2 = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    bvals = gtabHCP.bvals\n",
    "    bvecs = gtabHCP.bvecs\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorS0.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        dt = ForceLowFA(dt)\n",
    "        Obs.append(CustomSimulator(dt,gtabHCP,params[-1],np.random.rand()*30 + 20))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior2 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{save_path}/DTIHCPFull.pickle\"):\n",
    "        with open(f\"{save_path}/DTIHCPFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior2, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175d3fa-f8d0-40f0-abbb-ee81b9b1562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "def optimize_chunk(pixels):\n",
    "    results = []\n",
    "    for i, j in pixels:\n",
    "        posterior_samples_1 = posterior2.sample((1000,), x=maskdata[i, j,axial_middle, :91],show_progress_bars=False)\n",
    "        results.append((i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])))\n",
    "    return results\n",
    "\n",
    "chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    ")\n",
    "\n",
    "# Initialize array with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "NoiseEst = np.zeros(list(ArrShape) + [7])\n",
    "\n",
    "# Assign the optimization results to InferredParams\n",
    "for chunk in results:\n",
    "    for i, j, x in chunk:\n",
    "        NoiseEst[i, j] = x\n",
    "\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(55):\n",
    "    for j in range(64):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1]])\n",
    "MD_SBIFull = np.zeros([55,64])\n",
    "FA_SBIFull = np.zeros([55,64])\n",
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "        MD_SBIFull[i,j] = np.mean(Eigs)\n",
    "        FA_SBIFull[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "FA_SBIFull[np.isnan(FA_SBIFull)] = 0\n",
    "\n",
    "tenmodel = dti.TensorModel(gtabHCP,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(maskdata[:,:,axial_middle])\n",
    "FAFull = dti.fractional_anisotropy(tenfit.evals)\n",
    "MDFull = dti.mean_diffusivity(tenfit.evals)\n",
    "\n",
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            FAFull[i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399f080-b577-45e8-a69f-986aa4d7728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIHCPMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTIHCPMin.pickle\", \"rb\") as handle:\n",
    "        posterior7_2 = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    bvals = gtabHCP.bvals\n",
    "    bvecs = gtabHCP.bvecs\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorS0.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        dt = ForceLowFA(dt)\n",
    "        Obs.append(CustomSimulator(dt,gtabHCP7,params[-1],np.random.rand()*30 + 20))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior7_2 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{save_path}/DTIHCPMin.pickle\"):\n",
    "        with open(f\"{save_path}/DTIHCPMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7_2, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e994cc46-b5a4-44f4-b33c-63111a1cf19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel(i, j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posterior7_2.sample((1000,), x=maskdata[i, j,axial_middle, selected_indices_alt],show_progress_bars=False)\n",
    "    return i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "\n",
    "# Initialize array with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "NoiseEst = np.zeros(list(ArrShape) + [7])\n",
    "\n",
    "# Assign the optimization results to InferredParams\n",
    "for i, j, x in results:\n",
    "    NoiseEst[i, j] = x\n",
    "\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(55):\n",
    "    for j in range(64):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1]])\n",
    "MD_SBI7 = np.zeros([55,64])\n",
    "FA_SBI7 = np.zeros([55,64])\n",
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "        MD_SBI7[i,j] = np.mean(Eigs)\n",
    "        FA_SBI7[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "FA_SBI7[np.isnan(FA_SBI7)] = 0\n",
    "\n",
    "tenmodel = dti.TensorModel(gtabHCP7_alt,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(maskdata[:,:,axial_middle,selected_indices_alt])\n",
    "FA7 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD7 = dti.mean_diffusivity(tenfit.evals)\n",
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            FA7[i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f64ba-006f-4696-9f7b-6115b96aef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Masks = []\n",
    "maskdatas = []\n",
    "axial_middles = []\n",
    "WMs = []\n",
    "\n",
    "gTabsF = []\n",
    "gTabs7 = []\n",
    "gTabs20 = []\n",
    "\n",
    "FullDat   = []\n",
    "for kk in tqdm(range(32)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    gTabsF.append(gtabHCP)\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, _ = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "    Masks.append(mask)\n",
    "    maskdatas.append(maskdata[:,:,axial_middle])\n",
    "    axial_middles.append(axial_middle)\n",
    "\n",
    "    WM, affine, img = load_nifti('./HCP_data/WM_Masks/c2Pat'+str(kk+1)+'_FP.nii', return_img=True)\n",
    "    WMs.append(np.fliplr(WM[:,:,axial_middles[kk]]>0.8))\n",
    "\n",
    "\n",
    "    \n",
    "    bvalsHCP7 = bvalsHCP[selected_indices]\n",
    "    bvecsHCP7 = bvecsHCP[selected_indices]\n",
    "    gtabHCP7 = gradient_table(bvalsHCP7, bvecsHCP7)\n",
    "\n",
    "    gTabs7.append(gtabHCP7)\n",
    "\n",
    "    bvalsHCP20 = bvalsHCP[selected_indices20]\n",
    "    bvecsHCP20 = bvecsHCP[selected_indices20]\n",
    "    gtabHCP20 = gradient_table(bvalsHCP20, bvecsHCP20)\n",
    "\n",
    "    gTabs20.append(gtabHCP20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed6a510-df45-4835-a780-6075a5bfc993",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIMultiHCPFull_300.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMultiHCPFull_300.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm(range(TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-2])\n",
    "        dt = ForceLowFA(dt)\n",
    "        cG = gTabsF[int(params[-1])]\n",
    "        Obs.append(np.hstack([CustomSimulator(dt,cG,params[-2],50),params[-1]]))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-2]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs=100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMultiHCPFull_300.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMultiHCPFull_300.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n",
    "\n",
    "if os.path.exists(f\"{network_path}/DTIMultiHCPMin_300.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMultiHCPMin_300.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-2])\n",
    "        dt = ForceLowFA(dt)\n",
    "        cG = gTabs7[int(params[-1])]\n",
    "        Obs.append(np.hstack([CustomSimulator(dt,cG,params[-2],50),params[-1]]))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-2]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs=100)\n",
    "    posteriorMin = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMultiHCPMin_300.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMultiHCPMin_300.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorMin, handle)\n",
    "\n",
    "\n",
    "if os.path.exists(f\"{network_path}/DTIMultiHCPMid_300.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMultiHCPMid_300.pickle\", \"rb\") as handle:\n",
    "        posteriorMid = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-2])\n",
    "        dt = ForceLowFA(dt)\n",
    "        cG = gTabs20[int(params[-1])]\n",
    "        Obs.append(np.hstack([CustomSimulator(dt,cG,params[-2],50),params[-1]]))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-2]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs=100)\n",
    "    posteriorMid = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMultiHCPMid_300.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMultiHCPMid_300.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorMid, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541eb2ff-d328-44bb-bd7a-ebbca3ad7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argwhere(mask)\n",
    "def optimize_chunk(pixels):\n",
    "    results = []\n",
    "    for i, j in pixels:\n",
    "        posterior_samples_1 = posterior2.sample((1000,), x=maskdata[i, j,axial_middle, :91],show_progress_bars=False)\n",
    "        results.append((i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])))\n",
    "    return results\n",
    "\n",
    "chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    ")\n",
    "\n",
    "# Initialize array with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "NoiseEst = np.zeros(list(ArrShape) + [7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10532646-f49c-48c6-a894-9146b4830df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kk in tqdm(range(32)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, _ = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    Arr = maskdata[:,:,axial_middle, selected_indices]\n",
    "    indices = np.argwhere(mask)\n",
    "    def optimize_chunk(pixels):\n",
    "        results = []\n",
    "        for i, j in pixels:\n",
    "            posterior_samples_1 = posteriorMin.sample((500,), x=np.hstack([Arr[i,j],kk]),show_progress_bars=False)\n",
    "            results.append((i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])))\n",
    "        return results\n",
    "    \n",
    "    chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    "    )\n",
    "    \n",
    "    # Initialize array with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    NoiseEst = np.zeros(list(ArrShape) + [7])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for chunk in results:\n",
    "        for i, j, x in chunk:\n",
    "            NoiseEst[i, j] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a01e28-4659-4b51-bc7e-0282e3ef2e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{DatFolder}/Full_MD_HCP.npy\"):\n",
    "    MDFullArr = np.load(f\"{DatFolder}/Full_MD_HCP.npy\",allow_pickle=True)\n",
    "    FAFullArr = np.load(f\"{DatFolder}/Full_FA_HCP.npy\",allow_pickle=True)\n",
    "else:\n",
    "    MDFullArr = []\n",
    "    FAFullArr = []\n",
    "    for kk in tqdm(range(32)):\n",
    "        fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "        bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "        bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "        \n",
    "        bvalsHCP = np.loadtxt(bvalloc)\n",
    "        bvecsHCP = np.loadtxt(bvecloc)\n",
    "        gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "        \n",
    "        data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "        data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "        axial_middle = data.shape[2] // 2\n",
    "        maskdata, _ = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                     numpass=1, autocrop=True, dilate=2)\n",
    "        # Compute the mask where the sum is not zero\n",
    "        mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "        \n",
    "        # Get the indices where mask is True\n",
    "        indices = np.argwhere(mask)\n",
    "        def optimize_chunk(pixels):\n",
    "            results = []\n",
    "            for i, j in pixels:\n",
    "                posterior_samples_1 = posteriorFull.sample((500,), x=np.hstack([maskdata[i,j,axial_middle, :],kk]),show_progress_bars=False)\n",
    "                results.append((i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])))\n",
    "            return results\n",
    "        \n",
    "        chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "        results = Parallel(n_jobs=8)(\n",
    "            delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    "        )\n",
    "        \n",
    "        # Initialize array with the appropriate shape\n",
    "        ArrShape = mask.shape\n",
    "        NoiseEst = np.zeros(list(ArrShape) + [7])\n",
    "        \n",
    "        # Assign the optimization results to NoiseEst\n",
    "        for chunk in results:\n",
    "            for i, j, x in chunk:\n",
    "                NoiseEst[i, j] = x\n",
    "        \n",
    "        NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]):    \n",
    "                NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1:]])\n",
    "        MD_SBIFull = np.zeros(ArrShape)\n",
    "        FA_SBIFull = np.zeros(ArrShape)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]): \n",
    "                Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "                MD_SBIFull[i,j] = np.mean(Eigs)\n",
    "                FA_SBIFull[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "        FA_SBIFull[np.isnan(FA_SBIFull)] = 0\n",
    "        MDFullArr.append(MD_SBIFull)\n",
    "        FAFullArr.append(FA_SBIFull)\n",
    "if os.path.exists(f\"{DatFolder}/Min_MD_HCP.npy\"):\n",
    "    MDMinArr = np.load(f\"{DatFolder}/Min_MD_HCP.npy\",allow_pickle=True)\n",
    "    FAMinArr = np.load(f\"{DatFolder}/Min_FA_HCP.npy\",allow_pickle=True)\n",
    "else:\n",
    "    MDMinArr = []\n",
    "    FAMinArr = []\n",
    "    for kk in tqdm(range(32)):\n",
    "        fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "        bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "        bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "        \n",
    "        bvalsHCP = np.loadtxt(bvalloc)\n",
    "        bvecsHCP = np.loadtxt(bvecloc)\n",
    "        gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "        \n",
    "        data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "        data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "        axial_middle = data.shape[2] // 2\n",
    "        maskdata, _ = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                     numpass=1, autocrop=True, dilate=2)\n",
    "        # Compute the mask where the sum is not zero\n",
    "        mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "        \n",
    "        # Get the indices where mask is True\n",
    "        Arr = maskdata[:,:,axial_middle, selected_indices]\n",
    "        indices = np.argwhere(mask)\n",
    "        def optimize_chunk(pixels):\n",
    "            results = []\n",
    "            for i, j in pixels:\n",
    "                posterior_samples_1 = posteriorMin.sample((500,), x=np.hstack([Arr[i,j],kk]),show_progress_bars=False)\n",
    "                results.append((i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])))\n",
    "            return results\n",
    "        \n",
    "        chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "        results = Parallel(n_jobs=8)(\n",
    "            delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    "        )\n",
    "        \n",
    "        # Initialize array with the appropriate shape\n",
    "        ArrShape = mask.shape\n",
    "        NoiseEst = np.zeros(list(ArrShape) + [7])\n",
    "        \n",
    "        # Assign the optimization results to NoiseEst\n",
    "        for chunk in results:\n",
    "            for i, j, x in chunk:\n",
    "                NoiseEst[i, j] = x\n",
    "        \n",
    "        NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]):    \n",
    "                NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1:]])\n",
    "        MD_SBIMin = np.zeros(ArrShape)\n",
    "        FA_SBIMin = np.zeros(ArrShape)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]): \n",
    "                Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "                MD_SBIMin[i,j] = np.mean(Eigs)\n",
    "                FA_SBIMin[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "        FA_SBIMin[np.isnan(FA_SBIMin)] = 0\n",
    "        MDMinArr.append(MD_SBIMin)\n",
    "        FAMinArr.append(FA_SBIMin)\n",
    "if os.path.exists(f\"{DatFolder}/Mid_MD_HCP.npy\"):\n",
    "    MDMidArr = np.load(f\"{DatFolder}/Mid_MD_HCP.npy\",allow_pickle=True)\n",
    "    FAMidArr = np.load(f\"{DatFolder}/Mid_FA_HCP.npy\",allow_pickle=True)\n",
    "else:    \n",
    "    MDMidArr = []\n",
    "    FAMidArr = []\n",
    "    for kk in tqdm(range(32)):\n",
    "        fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "        bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "        bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "        \n",
    "        bvalsHCP = np.loadtxt(bvalloc)\n",
    "        bvecsHCP = np.loadtxt(bvecloc)\n",
    "        gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "        \n",
    "        data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "        data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "        axial_middle = data.shape[2] // 2\n",
    "        maskdata, _ = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                     numpass=1, autocrop=True, dilate=2)\n",
    "        # Compute the mask where the sum is not zero\n",
    "        mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "        \n",
    "        # Get the indices where mask is True\n",
    "        Arr = maskdata[:,:,axial_middle, selected_indices20]\n",
    "        indices = np.argwhere(mask)\n",
    "        def optimize_chunk(pixels):\n",
    "            results = []\n",
    "            for i, j in pixels:\n",
    "                posterior_samples_1 = posteriorMid.sample((500,), x=np.hstack([Arr[i,j],kk]),show_progress_bars=False)\n",
    "                results.append((i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])))\n",
    "            return results\n",
    "        \n",
    "        chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "        results = Parallel(n_jobs=8)(\n",
    "            delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    "        )\n",
    "        \n",
    "        # Initialize array with the appropriate shape\n",
    "        ArrShape = mask.shape\n",
    "        NoiseEst = np.zeros(list(ArrShape) + [7])\n",
    "        \n",
    "        # Assign the optimization results to NoiseEst\n",
    "        for chunk in results:\n",
    "            for i, j, x in chunk:\n",
    "                NoiseEst[i, j] = x\n",
    "        \n",
    "        NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]):    \n",
    "                NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1:]])\n",
    "        MD_SBIMid = np.zeros(ArrShape)\n",
    "        FA_SBIMid = np.zeros(ArrShape)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]): \n",
    "                Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "                MD_SBIMid[i,j] = np.mean(Eigs)\n",
    "                FA_SBIMid[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "        FA_SBIMid[np.isnan(FA_SBIMid)] = 0\n",
    "        MDMidArr.append(MD_SBIMid)\n",
    "        FAMidArr.append(FA_SBIMid)\n",
    "MDFullNLArr = []\n",
    "FAFullNLArr = []\n",
    "\n",
    "MDMidNLArr = []\n",
    "FAMidNLArr = []\n",
    "\n",
    "MDMinNLArr = []\n",
    "FAMinNLArr = []\n",
    "for kk in range(32):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    ArrShape = maskdata[:,:,axial_middle,0].shape\n",
    "    tenmodel = dti.TensorModel(gtabHCP,return_S0_hat = True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(maskdata[:,:,axial_middle])\n",
    "    FAFull_t = dti.fractional_anisotropy(tenfit.evals)\n",
    "    MDFull_t = dti.mean_diffusivity(tenfit.evals)\n",
    "    MDFullNLArr.append(MDFull_t)\n",
    "    FAFullNLArr.append(FAFull_t)\n",
    "\n",
    "    tenmodel = dti.TensorModel(gTabs20[kk],return_S0_hat = True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(maskdata[:,:,axial_middle,selected_indices20])\n",
    "    FAFull_t = dti.fractional_anisotropy(tenfit.evals)\n",
    "    MDFull_t = dti.mean_diffusivity(tenfit.evals)\n",
    "    MDMidNLArr.append(MDFull_t)\n",
    "    FAMidNLArr.append(FAFull_t)\n",
    "    \n",
    "    tenmodel = dti.TensorModel(gTabs7[kk],return_S0_hat = True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(maskdata[:,:,axial_middle,selected_indices])\n",
    "    FAFull_t = dti.fractional_anisotropy(tenfit.evals)\n",
    "    MDFull_t = dti.mean_diffusivity(tenfit.evals)\n",
    "    MDMinNLArr.append(MDFull_t)\n",
    "    FAMinNLArr.append(FAFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1902c5-f490-4bcd-8d3c-28720dca05ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccM7_MD = []\n",
    "AccM20_MD = []\n",
    "AccMFulls_MD = []\n",
    "\n",
    "AccM7NL_MD = []\n",
    "AccM20NL_MD = []\n",
    "\n",
    "SSIM7_MD = []\n",
    "SSIM20_MD = []\n",
    "SSIMFulls_MD = []\n",
    "\n",
    "SSIM7NL_MD = []\n",
    "SSIM20NL_MD = []\n",
    "for i in tqdm(range(32)):\n",
    "    M7 = MDMinArr[i]\n",
    "    MF = MDFullArr[i]\n",
    "    Ma = Masks[i]\n",
    "    AccM7_MD.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = MDMidArr[i]\n",
    "    MF = MDFullArr[i]\n",
    "    AccM20_MD.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = MDFullArr[i]\n",
    "    MF = MDFullNLArr[i]\n",
    "    AccMFulls_MD.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = MDMinNLArr[i]\n",
    "    MF = MDFullNLArr[i]\n",
    "    AccM7NL_MD.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = MDMidNLArr[i]\n",
    "    MF = MDFullNLArr[i]\n",
    "    AccM20NL_MD.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "\n",
    "    NS1 = MDMinArr[i]\n",
    "    NS2 = MDFullArr[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM7_MD.append(result)\n",
    "\n",
    "    NS1 = MDMidArr[i]\n",
    "    NS2 = MDFullArr[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM20_MD.append(result)\n",
    "    \n",
    "    NS1 = MDFullArr[i]\n",
    "    NS2 = MDFullNLArr[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIMFulls_MD.append(result)\n",
    "\n",
    "    NS1 = MDMinNLArr[i]\n",
    "    NS2 = MDFullNLArr[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM7NL_MD.append(result)\n",
    "\n",
    "    NS1 = MDMidNLArr[i]\n",
    "    NS2 = MDFullNLArr[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM20NL_MD.append(result)\n",
    "\n",
    "Prec7_SBI_MD = []\n",
    "Prec20_SBI_MD = []\n",
    "PrecFull_SBI_MD = []\n",
    "\n",
    "Prec7_NLLS_MD = []\n",
    "Prec20_NLLS_MD = []\n",
    "PrecFull_NLLS_MD = []\n",
    "for i in range(32):\n",
    "    Prec7_SBI_MD.append(np.std(MDMinArr[i][WMs[i]]))\n",
    "    Prec20_SBI_MD.append(np.std(MDMidArr[i][WMs[i]]))\n",
    "    PrecFull_SBI_MD.append(np.std(MDFullArr[i][WMs[i]]))\n",
    "\n",
    "    Prec7_NLLS_MD.append(np.std(MDMinNLArr[i][WMs[i]]))\n",
    "    Prec20_NLLS_MD.append(np.std(MDMidNLArr[i][WMs[i]]))\n",
    "    PrecFull_NLLS_MD.append(np.std(MDFullNLArr[i][WMs[i]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb38c16-8efd-41fa-bdb3-8a78b7a1c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccM7_FA = []\n",
    "AccM20_FA = []\n",
    "AccMFulls_FA = []\n",
    "\n",
    "AccM7NL_FA = []\n",
    "AccM20NL_FA = []\n",
    "\n",
    "SSIM7_FA = []\n",
    "SSIM20_FA = []\n",
    "SSIMFulls_FA = []\n",
    "\n",
    "SSIM7NL_FA = []\n",
    "SSIM20NL_FA = []\n",
    "for i in range(32):\n",
    "    M7 = FAMinArr[i]\n",
    "    MF = FAFullArr[i]\n",
    "    Ma = Masks[i]\n",
    "    AccM7_FA.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = FAMidArr[i]\n",
    "    MF = FAFullArr[i]\n",
    "    AccM20_FA.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = FAFullArr[i]\n",
    "    MF = FAFullNLArr[i]\n",
    "    AccMFulls_FA.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = FAMinNLArr[i]\n",
    "    MF = FAFullNLArr[i]\n",
    "    AccM7NL_FA.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = FAMidNLArr[i]\n",
    "    MF = FAFullNLArr[i]\n",
    "    AccM20NL_FA.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "\n",
    "    NS1 = FAMinArr[i]\n",
    "    NS2 = FAFullArr[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM7_FA.append(result)\n",
    "\n",
    "    NS1 = FAMidArr[i]\n",
    "    NS2 = FAFullArr[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM20_FA.append(result)\n",
    "    \n",
    "    NS1 = FAFullArr[i]\n",
    "    NS2 = FAFullNLArr[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIMFulls_FA.append(result)\n",
    "\n",
    "    NS1 = FAMinNLArr[i]\n",
    "    NS2 = FAFullNLArr[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM7NL_FA.append(result)\n",
    "\n",
    "    NS1 = FAMidNLArr[i]\n",
    "    NS2 = FAFullNLArr[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM20NL_FA.append(result)\n",
    "\n",
    "\n",
    "Prec7_SBI_FA = []\n",
    "Prec20_SBI_FA = []\n",
    "PrecFull_SBI_FA = []\n",
    "\n",
    "Prec7_NLLS_FA = []\n",
    "Prec20_NLLS_FA = []\n",
    "PrecFull_NLLS_FA = []\n",
    "for i in range(32):\n",
    "    Prec7_SBI_FA.append(np.std(FAMinArr[i][WMs[i]]))\n",
    "    Prec20_SBI_FA.append(np.std(FAMidArr[i][WMs[i]]))\n",
    "    PrecFull_SBI_FA.append(np.std(FAFullArr[i][WMs[i]]))\n",
    "\n",
    "    Prec7_NLLS_FA.append(np.std(FAMinNLArr[i][WMs[i]]))\n",
    "    Prec20_NLLS_FA.append(np.std(FAMidNLArr[i][WMs[i]]))\n",
    "    PrecFull_NLLS_FA.append(np.std(FAFullNLArr[i][WMs[i]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27b958a-9980-44a4-81b0-74d854fe97d2",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cddaed4-4b15-43fe-b73a-9c23da16b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(MD_SBIFull)\n",
    "\n",
    "temp[~mask_cutout] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot')\n",
    "plt.axis('off')\n",
    "vmin, vmax = img.get_clim()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_MD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501224f-f98c-4250-bc8d-f53c154a9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(MDFull)\n",
    "\n",
    "temp[~mask_cutout] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot')\n",
    "plt.axis('off')\n",
    "vmin, vmax = img.get_clim()\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_MD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1948c0-fd61-42a2-a04f-d491a4053441",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MD_SBIFull.T-MDFull.T\n",
    "data[~mask_cutout.T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=np.nanmin(data), vcenter=0, vmax=np.nanmax(data))\n",
    "plt.imshow(data,cmap='seismic',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "ticks = [np.nanmin(data), 0, np.nanmax(data)]  # Adjust the number of ticks as needed\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_MD_Diff.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9f7fe-445b-4dd3-b115-81e6d0b6def1",
   "metadata": {},
   "source": [
    "### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b21d86-1d2e-41e3-b10e-39d39479a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(MD_SBI7)\n",
    "\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot',vmin=0,vmax=3.5e-3)\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_MD_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56362702-1efb-4dd5-bb24-c06ab279258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(MD7)\n",
    "\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot',vmin=0,vmax=3.5e-3)\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_MD_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f250c7-2c64-4811-b2bf-3f840256dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs(MD_SBIFull.T-MD_SBI7.T)\n",
    "data[~mask.T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.nanmax(data)/2, vmax=np.nanmax(data))\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'DTI_MDSBIErr.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.nanmax(data)/2, vmax=np.nanmax(data))\n",
    "ticks = [0, np.round(np.nanmax(data),3)]  # Adjust the number of ticks as needed\n",
    "data = np.abs(MDFull.T-MD7.T)\n",
    "data[~mask.T] = np.nan\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'DTI_MDWLSErr.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc380bcd-1156-4d47-a3a5-acbbb155ad5d",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c1788-0502-4c9c-93ea-85ceaacc11d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(FA_SBIFull)\n",
    "\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot')\n",
    "plt.axis('off')\n",
    "vmin, vmax = img.get_clim()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_FA.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07005aba-0874-4d7d-902a-7396bc872019",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(FAFull)\n",
    "\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot')\n",
    "plt.axis('off')\n",
    "vmin, vmax = img.get_clim()\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_FA.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feeeb83-d652-4f95-9512-516ef7f5a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FA_SBIFull.T-FAFull.T\n",
    "data[~mask.T] = np.nan\n",
    "plt.imshow(data,cmap='seismic',vmin=-1, vmax=1)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "ticks = [-1, 0, 1]  # Adjust the number of ticks as needed\n",
    "cbar.set_ticks(ticks)\n",
    "if Save: plt.savefig(FigLoc+'HCP_FA_Diff.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd894538-9fee-4333-8768-6a82d26a5422",
   "metadata": {},
   "source": [
    "### d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9eea4-4209-455a-a9bc-f626bec02a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(FA_SBI7)\n",
    "\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot')\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_FA_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b492936-6624-4d17-9610-ab377ced55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(FA7)\n",
    "\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot')\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_FA_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1c6ee2-5bfa-4336-92c1-1d6395190692",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs(FA_SBIFull.T-FA_SBI7.T)\n",
    "data[~mask.T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=0.5, vmax=1)\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'DTI_FASBIErr.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=0.5, vmax=1)\n",
    "ticks = [0, 1]  # Adjust the number of ticks as needed\n",
    "data = np.abs(FAFull.T-FA7.T)\n",
    "data[~mask.T] = np.nan\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "\n",
    "cbar.set_ticks(ticks)\n",
    "if Save: plt.savefig(FigLoc+'DTI_FAWLSErr.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c8c21c-5c47-4435-a73c-9adfb35052ea",
   "metadata": {},
   "source": [
    "### e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3dcdea-7a6a-422c-a2f6-cbd3db10ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "\n",
    "\n",
    "y_data = np.array(AccM7NL_MD)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax1.set_ylim(0.0001, 2.5e-3)\n",
    "ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax1.yaxis.set_ticks(np.arange(0.0005, 0.006, 0.002))\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.sca(ax2)\n",
    "\n",
    "y_data = np.array(AccMFulls_MD)\n",
    "g_pos = np.array([1])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20_MD)\n",
    "g_pos = np.array([1.7])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM7_MD)\n",
    "g_pos = np.array([2])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20NL_MD)\n",
    "g_pos = np.array([2.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax2.set_ylim(0, 0.00016)\n",
    "ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "ax2.yaxis.set_ticks(np.arange(0, 0.00018, 0.0001))\n",
    "\n",
    "# Common x-ticks\n",
    "ax2.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "ax2.set_xticklabels(['Full', 'Mid', 'Min', 'Mid', 'Min'], fontsize=32, rotation=90)\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "ax1.set_xlim(ax2.get_xlim())\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "\n",
    "leg_patch1 = mpatches.Patch(color='gray', label='Full Comp.')\n",
    "leg_patch2 = mpatches.Patch(color='lightseagreen', label='Mid. (SBI)')\n",
    "leg_patch3 = mpatches.Patch(color='mediumturquoise', label='Min. (SBI)')\n",
    "leg_patch4 = mpatches.Patch(color='sandybrown', label='Mid. (NLLS)')\n",
    "leg_patch5 = mpatches.Patch(color='burlywood', label='Min. (NLLS)')\n",
    "\n",
    "\n",
    "# Show plot\n",
    "if Save:\n",
    "    plt.savefig(FigLoc + 'DTIHCP_Acc_MD.pdf', format='PDF', transparent=True, bbox_inches='tight')\n",
    "\n",
    "ax1.legend(\n",
    "    handles=[leg_patch1,leg_patch2,leg_patch3],\n",
    "    loc='upper left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=1,\n",
    "fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,bbox_to_anchor=(0,1.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb6dfd-a7ff-4952-981e-4e063219a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1)#, sharex=True)\n",
    "fig.subplots_adjust(hspace=0.05)  # adjust space between Axes\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax1)\n",
    "y_data = np.array(SSIMFulls_MD)\n",
    "g_pos = np.array([1])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20_MD)\n",
    "g_pos = np.array([1.7])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM7_MD)\n",
    "g_pos = np.array([2])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20NL_MD)\n",
    "g_pos = np.array([2.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_MD)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_MD)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "\n",
    "ax1.set_ylim(.7, 1.)  # outliers only\n",
    "ax2.set_ylim(0, .7)  # most of the data\n",
    "\n",
    "ax1.set_xticks([]) \n",
    "ax2.set_xticks([]) \n",
    "plt.yticks(fontsize=32)\n",
    "plt.sca(ax2)\n",
    "plt.yticks(fontsize=32)\n",
    "\n",
    "d = .5  # proportion of vertical to horizontal extent of the slanted line\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12,\n",
    "              linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "ax2.set_xlim(ax1.get_xlim())\n",
    "ax2.axhline(0.66, lw=3, ls='--', c='k')\n",
    "ax2.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "ax2.set_xticklabels(['Full', 'Mid', 'Min', 'Mid', 'Min'], fontsize=32, rotation=90)\n",
    "\n",
    "ax2.legend(\n",
    "    handles=[leg_patch4,leg_patch5],\n",
    "    loc='upper left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=1,\n",
    "fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,bbox_to_anchor= (-0.0,0.9))\n",
    "if Save: plt.savefig(FigLoc+'DTI_MD_SSIMErr.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86f05d-07a7-48a8-a9e7-a6faa306808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = np.array(PrecFull_SBI_MD)\n",
    "g_pos = np.array([1])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "fig,ax = plt.subplots()\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_SBI_MD)\n",
    "g_pos = np.array([1.2])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_MD)\n",
    "g_pos = np.array([1.4])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "y_data = np.array(PrecFull_NLLS_MD)\n",
    "g_pos = np.array([2])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_NLLS_MD)\n",
    "g_pos = np.array([2.2])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_MD)\n",
    "g_pos = np.array([2.4])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "x = np.arange(1.85,2.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_MD)[~np.isnan(PrecFull_NLLS_MD)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_MD)[~np.isnan(PrecFull_NLLS_MD)], 77)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.85,1.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_MD)[~np.isnan(PrecFull_SBI_MD)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_MD)[~np.isnan(PrecFull_SBI_MD)], 77)\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "plt.xticks([1,1.2,1.4,2,2.2,2.4],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "if Save: plt.savefig(FigLoc+'DTI_MD_Prec.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c49bf-a56c-4b9e-9ca6-9722c5bf6126",
   "metadata": {},
   "source": [
    "### f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c102e9-c250-4be1-b354-2b6f21ef4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "\n",
    "\n",
    "y_data = np.array(AccM7NL_FA)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "#ax1.yaxis.set_ticks(np.arange(0.0005, 0.006, 0.002))\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.sca(ax2)\n",
    "\n",
    "y_data = np.array(AccMFulls_FA)\n",
    "g_pos = np.array([1])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20_FA)\n",
    "g_pos = np.array([1.7])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM7_FA)\n",
    "g_pos = np.array([2])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20NL_FA)\n",
    "g_pos = np.array([2.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "ax1.set_ylim(0.4, 1)\n",
    "#ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "ax1.yaxis.set_ticks([0.4,0.7,1])\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax2.set_ylim(0.0,0.3)\n",
    "#ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "#ax2.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "ax2.set_xticklabels(['Full', 'Mid', 'Min', 'Mid', 'Min'], fontsize=32, rotation=90)\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "ax1.set_xlim(ax2.get_xlim())\n",
    "# Show plot\n",
    "if Save:\n",
    "    plt.savefig(FigLoc + 'DTIHCP_Acc_FA.pdf', format='PDF', transparent=True, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b9e54-6630-482a-8872-8b7330205c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "y_data = np.array(SSIMFulls_FA)\n",
    "g_pos = np.array([1])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20_FA)\n",
    "g_pos = np.array([1.7])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM7_FA)\n",
    "g_pos = np.array([2])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20NL_FA)\n",
    "g_pos = np.array([2.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_FA)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_FA)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66,lw=3,ls='--',c='k')\n",
    "plt.yticks(fontsize=32)\n",
    "plt.ylim([0,1])\n",
    "ax2.legend(\n",
    "    handles=[leg_patch3],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=2,\n",
    "fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "ax.set_xticks([1,1.7,2,2.8,3.1])\n",
    "ax.set_xticklabels(['Full', 'Mid', 'Min', 'Mid', 'Min'], fontsize=32, rotation=90)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DTI_FA_SSIMErr.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6385d27d-50c8-447a-b5c3-fd441758c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = np.array(PrecFull_SBI_FA)\n",
    "g_pos = np.array([1])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "fig,ax = plt.subplots()\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_SBI_FA)\n",
    "g_pos = np.array([1.2])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_FA)\n",
    "g_pos = np.array([1.4])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "y_data = np.array(PrecFull_NLLS_FA)\n",
    "g_pos = np.array([2])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_NLLS_FA)\n",
    "g_pos = np.array([2.2])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_FA)\n",
    "g_pos = np.array([2.4])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "x = np.arange(1.85,2.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_FA)[~np.isnan(PrecFull_NLLS_FA)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_FA)[~np.isnan(PrecFull_NLLS_FA)], 77)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.85,1.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_FA)[~np.isnan(PrecFull_SBI_FA)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_FA)[~np.isnan(PrecFull_SBI_FA)], 75)\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "plt.xticks([1,1.2,1.4,2,2.2,2.4],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "if Save: plt.savefig(FigLoc+'DTI_FA_Prec.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb2c18-8ad2-44f0-8764-9c398955ec77",
   "metadata": {},
   "source": [
    "## MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d541978-7674-4f8a-b4d3-3604c4b49130",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dats_MS   = []\n",
    "gTabs7_MS = []\n",
    "gTabs20_MS = []\n",
    "gTabsF_MS = []\n",
    "Masks_MS   = []\n",
    "TrueIndxs = []\n",
    "axial_middles_MS = []\n",
    "for i,Name in tqdm(enumerate(['NMSS_11_1year','NMSS_15','NMSS_16','NMSS_18','NMSS_19','Ctrl055_R01_28','Ctrl056_R01_29','Ctrl057_R01_30'])):\n",
    "    MatDir = MSDir+Name\n",
    "\n",
    "    F = pmt.read_mat(MatDir+'/data_loaded.mat')\n",
    "    affine = np.ones((4,4))\n",
    "    \n",
    "    data, affine = reslice(F['data'], affine, (2,2,2), (2.5,2.5,2.5))\n",
    "    _, maskCut = median_otsu(data, vol_idx=range(10, 80), autocrop=False)\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 80), autocrop=True)\n",
    "    Masks_MS.append(mask)\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    axial_middles_MS.append(axial_middle)\n",
    "    bvecs = (F['direction'].T/np.linalg.norm(F['direction'],axis=1)).T\n",
    "    bvecs[np.isnan(bvecs)] = 0\n",
    "    bvals = F['bval']\n",
    "    bvecs2000 = bvecs[bvals==2000]\n",
    "    bvecs4000 = bvecs[bvals==4000]\n",
    "\n",
    "    bvals2000 = np.array([0] + list(bvals[bvals==2000]))\n",
    "    bvecs2000 = np.vstack([[0,0,0],bvecs[bvals==2000]])\n",
    "\n",
    "    Dats_MS.append(maskdata[:,:,:,np.hstack([0,np.where(bvals==2000)[0]])])\n",
    "    \n",
    "    gTabsF_MS.append(gradient_table(bvals2000,bvecs2000))\n",
    "\n",
    "    if(i == 0):\n",
    "        # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "        selected_indices = [0]\n",
    "        distance_matrix = squareform(pdist(bvecs2000))\n",
    "        # Iteratively select the point furthest from the current selection\n",
    "        for _ in range(6):  # We need 7 points in total, and one is already selected\n",
    "            remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "            \n",
    "            # Calculate the minimum distance to the selected points for each remaining point\n",
    "            min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "            \n",
    "            # Select the point with the maximum minimum distance\n",
    "            next_index = remaining_indices[np.argmax(min_distances)]\n",
    "            selected_indices.append(next_index)\n",
    "        \n",
    "        selected_indices_MS = selected_indices\n",
    "\n",
    "        selected_indices20_MS = [0]\n",
    "        distance_matrix = squareform(pdist(bvecs2000))\n",
    "        # Iteratively select the point furthest from the current selection\n",
    "        for _ in range(19):  # We need 7 points in total, and one is already selected\n",
    "            remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices20_MS))\n",
    "            \n",
    "            # Calculate the minimum distance to the selected points for each remaining point\n",
    "            min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices20_MS], axis=1)\n",
    "            \n",
    "            # Select the point with the maximum minimum distance\n",
    "            next_index = remaining_indices[np.argmax(min_distances)]\n",
    "            selected_indices20_MS.append(next_index)\n",
    "\n",
    "    bvalsHCP7 = bvals2000[selected_indices_MS]\n",
    "    bvecsHCP7 = bvecs2000[selected_indices_MS]\n",
    "    \n",
    "    gTabs7_MS.append(gradient_table(bvalsHCP7, bvecsHCP7))\n",
    "    bvalsHCP7 = bvals2000[selected_indices20_MS]\n",
    "    bvecsHCP7 = bvecs2000[selected_indices20_MS]\n",
    "    \n",
    "    gTabs20_MS.append(gradient_table(bvalsHCP7, bvecsHCP7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46c73f-5cdc-467c-857d-a0aa3f9d1620",
   "metadata": {},
   "outputs": [],
   "source": [
    "WMDir = MSDir+'WM_masks/'\n",
    "WMs_MS = []\n",
    "for i,Name in tqdm(enumerate(['NMSS_11_1year','NMSS_15','NMSS_16','NMSS_18','NMSS_19','Ctrl055_R01_28','Ctrl056_R01_29','Ctrl057_R01_30'])):\n",
    "    MatDir = MSDir+Name\n",
    "    F = pmt.read_mat(MatDir+'/data_loaded.mat')\n",
    "    affine = np.ones((4,4))\n",
    "    \n",
    "    data, affine = reslice(F['data'], affine, (2,2,2), (2.5,2.5,2.5))\n",
    "    _, maskCut = median_otsu(data, vol_idx=range(10, 80), autocrop=False)\n",
    "    \n",
    "    true_indices = np.argwhere(maskCut)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    for k,x in enumerate(os.listdir(WMDir)):\n",
    "        if Name in x:\n",
    "            print(Name)\n",
    "            WM, affine, img = load_nifti(WMDir+x, return_img=True)\n",
    "            WM, affine = reslice(WM, affine, (2,2,2), (2.5,2.5,2.5))\n",
    "            if(i<5):\n",
    "                WM_t = np.fliplr(np.swapaxes(WM,0,1))\n",
    "            else:\n",
    "                WM_t = np.fliplr(np.flipud(np.swapaxes(WM,0,1)))\n",
    "            WM_t  = WM_t[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "            WMs_MS.append(WM_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8890a129-6e1e-42ff-a599-ae2f38fe08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIMultiMSFull_300.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMultiMSFull_300.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm(range(TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-2])\n",
    "        dt = ForceLowFA(dt)\n",
    "        cG = gTabsF_MS[int(params[-1])]\n",
    "        Obs.append(np.hstack([CustomSimulator(dt,cG,params[-2],50),params[-1]]))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-2]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMultiMSFull_300.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMultiMSFull_300.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n",
    "            \n",
    "if os.path.exists(f\"{network_path}/DTIMultiMSMin_300.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMultiMSMin_300.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm(range(TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-2])\n",
    "        dt = ForceLowFA(dt)\n",
    "        cG = gTabs7_MS[int(params[-1])]\n",
    "        Obs.append(np.hstack([CustomSimulator(dt,cG,params[-2],50),params[-1]]))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-2]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorMin = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMultiMSMin_300.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMultiMSMin_300.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorMin, handle)\n",
    "            \n",
    "if os.path.exists(f\"{network_path}/DTIMultiMSMid_300.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMultiMSMid_300.pickle\", \"rb\") as handle:\n",
    "        posteriorMid = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm(range(TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-2])\n",
    "        dt = ForceLowFA(dt)\n",
    "        cG = gTabs20_MS[int(params[-1])]\n",
    "        Obs.append(np.hstack([CustomSimulator(dt,cG,params[-2],50),params[-1]]))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-2]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorMid = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMultiMSMid_300.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMultiMSMid_300.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorMid, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99cd1d3-f96f-461e-8ae7-587688041ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDFullArr_MS = []\n",
    "FAFullArr_MS = []\n",
    "for kk in tqdm(range(8)):\n",
    "\n",
    "    \n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(Dats_MS[kk][:, :, axial_middles_MS[kk], :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = posteriorFull.sample((500,), x=np.hstack([Dats_MS[kk][i,j,axial_middles_MS[kk], :],kk]),show_progress_bars=False)\n",
    "        return i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [7])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "    \n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1:]])\n",
    "    MD_SBIMin = np.zeros(ArrShape)\n",
    "    FA_SBIMin = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "            MD_SBIMin[i,j] = np.mean(Eigs)\n",
    "            FA_SBIMin[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "    FA_SBIMin[np.isnan(FA_SBIMin)] = 0\n",
    "    MDFullArr_MS.append(MD_SBIMin)\n",
    "    FAFullArr_MS.append(FA_SBIMin)\n",
    "\n",
    "MDMinArr_MS = []\n",
    "FAMinArr_MS = []\n",
    "for kk in tqdm(range(8)):\n",
    "\n",
    "    \n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(Dats_MS[kk][:, :, axial_middles_MS[kk], :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    Arr = Dats_MS[kk][:,:,axial_middles_MS[kk], selected_indices_MS]\n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = posteriorMin.sample((500,), x=np.hstack([Arr[i,j],kk]),show_progress_bars=False)\n",
    "        return i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [7])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "    \n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1:]])\n",
    "    MD_SBIMin = np.zeros(ArrShape)\n",
    "    FA_SBIMin = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "            MD_SBIMin[i,j] = np.mean(Eigs)\n",
    "            FA_SBIMin[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "    FA_SBIMin[np.isnan(FA_SBIMin)] = 0\n",
    "    MDMinArr_MS.append(MD_SBIMin)\n",
    "    FAMinArr_MS.append(FA_SBIMin)\n",
    "\n",
    "MDMidArr_MS = []\n",
    "FAMidArr_MS = []\n",
    "\n",
    "for kk in tqdm(range(8)):\n",
    "\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(Dats_MS[kk][:, :, axial_middles_MS[kk], :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "\n",
    "    Arr = Dats_MS[kk][:,:,axial_middles_MS[kk], selected_indices20_MS]\n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = posteriorMid.sample((500,), x=np.hstack([Arr[i,j],kk]),show_progress_bars=False)\n",
    "        return i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [7])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "    \n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1:]])\n",
    "    MD_SBIMin = np.zeros(ArrShape)\n",
    "    FA_SBIMin = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "            MD_SBIMin[i,j] = np.mean(Eigs)\n",
    "            FA_SBIMin[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "    FA_SBIMin[np.isnan(FA_SBIMin)] = 0\n",
    "    MDMidArr_MS.append(MD_SBIMin)\n",
    "    FAMidArr_MS.append(FA_SBIMin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7594b12e-845d-4f29-b191-620c90d032ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDFullNLArr_MS = []\n",
    "FAFullNLArr_MS = []\n",
    "for kk in range(8):\n",
    "    \n",
    "    tenmodel = dti.TensorModel(gTabsF_MS[kk],return_S0_hat = True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(Dats_MS[kk][:,:,axial_middles_MS[kk],:])\n",
    "    FAFull = dti.fractional_anisotropy(tenfit.evals)\n",
    "    MDFull = dti.mean_diffusivity(tenfit.evals)\n",
    "    MDFullNLArr_MS.append(MDFull)\n",
    "    FAFullNLArr_MS.append(FAFull)\n",
    "MDMinNLArr_MS = []\n",
    "FAMinNLArr_MS = []\n",
    "for kk in range(8):\n",
    "    \n",
    "    tenmodel = dti.TensorModel(gTabs7_MS[kk],return_S0_hat = True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(Dats_MS[kk][:,:,axial_middles_MS[kk],selected_indices_MS])\n",
    "    FAFull = dti.fractional_anisotropy(tenfit.evals)\n",
    "    MDFull = dti.mean_diffusivity(tenfit.evals)\n",
    "    MDMinNLArr_MS.append(MDFull)\n",
    "    FAMinNLArr_MS.append(FAFull)\n",
    "MDMidNLArr_MS = []\n",
    "FAMidNLArr_MS = []\n",
    "for kk in range(8):\n",
    "    \n",
    "    tenmodel = dti.TensorModel(gTabs20_MS[kk],return_S0_hat = True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(Dats_MS[kk][:,:,axial_middles_MS[kk],selected_indices20_MS])\n",
    "    FAFull = dti.fractional_anisotropy(tenfit.evals)\n",
    "    MDFull = dti.mean_diffusivity(tenfit.evals)\n",
    "    MDMidNLArr_MS.append(MDFull)\n",
    "    FAMidNLArr_MS.append(FAFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6260ec4b-fe45-4449-a9dd-ade44d7de370",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccM7_MD_MS = []\n",
    "AccM20_MD_MS = []\n",
    "AccMFulls_MD_MS = []\n",
    "\n",
    "AccM7NL_MD_MS = []\n",
    "AccM20NL_MD_MS = []\n",
    "\n",
    "SSIM7_MD_MS = []\n",
    "SSIM20_MD_MS = []\n",
    "SSIMFulls_MD_MS = []\n",
    "\n",
    "SSIM7NL_MD_MS = []\n",
    "SSIM20NL_MD_MS = []\n",
    "for i in range(8):\n",
    "    M7 = MDMinArr_MS[i]\n",
    "    MF = MDFullArr_MS[i]\n",
    "    Ma = Masks_MS[i][:,:,axial_middles_MS[i]]\n",
    "    AccM7_MD_MS.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = MDMidArr_MS[i]\n",
    "    MF = MDFullArr_MS[i]\n",
    "    AccM20_MD_MS.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = MDFullArr_MS[i]\n",
    "    MF = MDFullNLArr_MS[i]\n",
    "    AccMFulls_MD_MS.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = MDMinNLArr_MS[i]\n",
    "    MF = MDFullNLArr_MS[i]\n",
    "    AccM7NL_MD_MS.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = MDMidNLArr_MS[i]\n",
    "    MF = MDFullNLArr_MS[i]\n",
    "    AccM20NL_MD_MS.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "\n",
    "    NS1 = MDMinArr_MS[i]\n",
    "    NS2 = MDFullArr_MS[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM7_MD_MS.append(result)\n",
    "\n",
    "    NS1 = MDMidArr_MS[i]\n",
    "    NS2 = MDFullArr_MS[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM20_MD_MS.append(result)\n",
    "    \n",
    "    NS1 = MDFullArr_MS[i]\n",
    "    NS2 = MDFullNLArr_MS[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIMFulls_MD_MS.append(result)\n",
    "\n",
    "    NS1 = MDMinNLArr_MS[i]\n",
    "    NS2 = MDFullNLArr_MS[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM7NL_MD_MS.append(result)\n",
    "\n",
    "    NS1 = MDMidNLArr_MS[i]\n",
    "    NS2 = MDFullNLArr_MS[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM20NL_MD_MS.append(result)\n",
    "\n",
    "\n",
    "Prec7_SBI_MD_MS = []\n",
    "Prec20_SBI_MD_MS = []\n",
    "PrecFull_SBI_MD_MS = []\n",
    "\n",
    "Prec7_NLLS_MD_MS = []\n",
    "Prec20_NLLS_MD_MS = []\n",
    "PrecFull_NLLS_MD_MS = []\n",
    "for i in range(8):\n",
    "    Prec7_SBI_MD_MS.append(np.std(MDMinArr_MS[i][WMs_MS[i][:,:,axial_middles_MS[i]]>0.8]))\n",
    "    Prec20_SBI_MD_MS.append(np.std(MDMidArr_MS[i][WMs_MS[i][:,:,axial_middles_MS[i]]>0.8]))\n",
    "    PrecFull_SBI_MD_MS.append(np.std(MDFullArr_MS[i][WMs_MS[i][:,:,axial_middles_MS[i]]>0.8]))\n",
    "\n",
    "    Prec7_NLLS_MD_MS.append(np.std(MDMinNLArr_MS[i][WMs_MS[i][:,:,axial_middles_MS[i]]>0.8]))\n",
    "    Prec20_NLLS_MD_MS.append(np.std(MDMidNLArr_MS[i][WMs_MS[i][:,:,axial_middles_MS[i]]>0.8]))\n",
    "    PrecFull_NLLS_MD_MS.append(np.std(MDFullNLArr_MS[i][WMs_MS[i][:,:,axial_middles_MS[i]]>0.8]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a48df1-09f2-4cda-98a8-bb9f1eeb27f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccM7_FA_MS = []\n",
    "AccM20_FA_MS = []\n",
    "AccMFulls_FA_MS = []\n",
    "\n",
    "AccM7NL_FA_MS = []\n",
    "AccM20NL_FA_MS = []\n",
    "\n",
    "SSIM7_FA_MS = []\n",
    "SSIM20_FA_MS = []\n",
    "SSIMFulls_FA_MS = []\n",
    "\n",
    "SSIM7NL_FA_MS = []\n",
    "SSIM20NL_FA_MS = []\n",
    "for i in range(8):\n",
    "    M7 = FAMinArr_MS[i]\n",
    "    MF = FAFullArr_MS[i]\n",
    "    Ma = Masks_MS[i][:,:,axial_middles_MS[i]]\n",
    "    AccM7_FA_MS.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = FAMidArr_MS[i]\n",
    "    MF = FAFullArr_MS[i]\n",
    "    AccM20_FA_MS.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = FAFullArr_MS[i]\n",
    "    MF = FAFullNLArr_MS[i]\n",
    "    AccMFulls_FA_MS.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = FAMinNLArr_MS[i]\n",
    "    MF = FAFullNLArr_MS[i]\n",
    "    AccM7NL_FA_MS.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = FAMidNLArr_MS[i]\n",
    "    MF = FAFullNLArr_MS[i]\n",
    "    AccM20NL_FA_MS.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "\n",
    "    NS1 = FAMinArr_MS[i]\n",
    "    NS2 = FAFullArr_MS[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM7_FA_MS.append(result)\n",
    "\n",
    "    NS1 = FAMidArr_MS[i]\n",
    "    NS2 = FAFullArr_MS[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM20_FA_MS.append(result)\n",
    "    \n",
    "    NS1 = FAFullArr_MS[i]\n",
    "    NS2 = FAFullNLArr_MS[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIMFulls_FA_MS.append(result)\n",
    "\n",
    "    NS1 = FAMinNLArr_MS[i]\n",
    "    NS2 = FAFullNLArr_MS[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM7NL_FA_MS.append(result)\n",
    "\n",
    "    NS1 = FAMidNLArr_MS[i]\n",
    "    NS2 = FAFullNLArr_MS[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM20NL_FA_MS.append(result)\n",
    "\n",
    "\n",
    "Prec7_SBI_FA_MS = []\n",
    "Prec20_SBI_FA_MS = []\n",
    "PrecFull_SBI_FA_MS = []\n",
    "\n",
    "Prec7_NLLS_FA_MS = []\n",
    "Prec20_NLLS_FA_MS = []\n",
    "PrecFull_NLLS_FA_MS = []\n",
    "for i in range(8):\n",
    "    Prec7_SBI_FA_MS.append(np.std(FAMinArr_MS[i][WMs_MS[i][:,:,axial_middles_MS[i]]>0.8]))\n",
    "    Prec20_SBI_FA_MS.append(np.std(FAMidArr_MS[i][WMs_MS[i][:,:,axial_middles_MS[i]]>0.8]))\n",
    "    PrecFull_SBI_FA_MS.append(np.std(FAFullArr_MS[i][WMs_MS[i][:,:,axial_middles_MS[i]]>0.8]))\n",
    "\n",
    "    Prec7_NLLS_FA_MS.append(np.std(FAMinNLArr_MS[i][WMs_MS[i][:,:,axial_middles_MS[i]]>0.8]))\n",
    "    Prec20_NLLS_FA_MS.append(np.std(FAMidNLArr_MS[i][WMs_MS[i][:,:,axial_middles_MS[i]]>0.8]))\n",
    "    PrecFull_NLLS_FA_MS.append(np.std(FAFullNLArr_MS[i][WMs_MS[i][:,:,axial_middles_MS[i]]>0.8]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5e05d-fb56-4753-a768-da6fa3610105",
   "metadata": {},
   "source": [
    "### g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90bee7d-ef30-42f4-b850-6d86b1ba6886",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)#, sharex=True)\n",
    "fig.subplots_adjust(hspace=0.05)  # adjust space between Axes\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax1)\n",
    "y_data = np.array(SSIMFulls_MD_MS)\n",
    "g_pos = np.array([1])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM20_MD_MS)\n",
    "g_pos = np.array([1.7])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM7_MD_MS)\n",
    "g_pos = np.array([2])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM20NL_MD_MS)\n",
    "g_pos = np.array([2.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM7NL_MD_MS)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "ax1.axhline(0.66, lw=3, ls='--', c='k')\n",
    "ax1.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "ax1.set_xticklabels(['Full', 'Mid', 'Min', 'Mid', 'Min'], fontsize=32, rotation=90)\n",
    "ax1.set_ylim([0,1])\n",
    "\n",
    "leg_patch1 = mpatches.Patch(color='lightseagreen', label='Mid. (SBI)')\n",
    "leg_patch2 = mpatches.Patch(color='mediumturquoise', label='Min. (SBI)')\n",
    "leg_patch3 = mpatches.Patch(color='sandybrown', label='Mid. (NLLS)')\n",
    "leg_patch4 = mpatches.Patch(color='burlywood', label='Min. (NLLS)')\n",
    "leg_patch5 = mpatches.Patch(color='gray', label='Full Comp.')\n",
    "\n",
    "ax1.legend(\n",
    "    handles=[leg_patch3,leg_patch4],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=1,\n",
    "fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,bbox_to_anchor= (-0.0,-0.05))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DTI_MD_SSIM_MS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd91ac9-f9c2-4c27-9127-649b4d6a7857",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)#, sharex=True)\n",
    "fig.subplots_adjust(hspace=0.05)  # adjust space between Axes\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax1)\n",
    "y_data = np.array(AccMFulls_MD_MS)\n",
    "g_pos = np.array([1])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(AccM20_MD_MS)\n",
    "g_pos = np.array([1.7])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(AccM7_MD_MS)\n",
    "g_pos = np.array([2])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(AccM20NL_MD_MS)\n",
    "g_pos = np.array([2.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(AccM7NL_MD_MS)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "ax1.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "ax1.set_xticklabels(['Full', 'Mid', 'Min', 'Mid', 'Min'], fontsize=32, rotation=90)\n",
    "\n",
    "leg_patch1 = mpatches.Patch(color='lightseagreen', label='Mid. (SBI)')\n",
    "leg_patch2 = mpatches.Patch(color='mediumturquoise', label='Min. (SBI)')\n",
    "leg_patch3 = mpatches.Patch(color='sandybrown', label='Mid. (NLLS)')\n",
    "leg_patch4 = mpatches.Patch(color='burlywood', label='Min. (NLLS)')\n",
    "leg_patch5 = mpatches.Patch(color='gray', label='Full Comp.')\n",
    "\n",
    "ax1.legend(\n",
    "    handles=[leg_patch5,leg_patch1,leg_patch2],\n",
    "    loc='upper left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=1,\n",
    "fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,bbox_to_anchor= (0,1.1))\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DTI_MD_Acc_MS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e98f8-54ec-4d85-a628-da324b7c7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = np.array(PrecFull_SBI_MD_MS)\n",
    "g_pos = np.array([1])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "fig,ax = plt.subplots()\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "y_data = np.array(Prec20_SBI_MD_MS)\n",
    "g_pos = np.array([1.2])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_MD_MS)\n",
    "g_pos = np.array([1.4])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "y_data = np.array(PrecFull_NLLS_MD_MS)\n",
    "g_pos = np.array([2])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(Prec20_NLLS_MD_MS)\n",
    "g_pos = np.array([2.2])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_MD_MS)\n",
    "g_pos = np.array([2.4])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "x = np.arange(1.85,2.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_MD_MS)[~np.isnan(PrecFull_NLLS_MD_MS)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_MD_MS)[~np.isnan(PrecFull_NLLS_MD_MS)], 77)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.85,1.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_MD_MS)[~np.isnan(PrecFull_SBI_MD_MS)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_MD_MS)[~np.isnan(PrecFull_SBI_MD_MS)], 77)\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "plt.xticks([1,1.2,1.4,2,2.2,2.4],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "leg_o = Line2D([0], [0],\n",
    "               marker='o',\n",
    "               color='w',                # no line\n",
    "               markerfacecolor='lightseagreen',\n",
    "               markersize=10,\n",
    "               alpha=0.5,\n",
    "               linestyle='None',\n",
    "               label='Healthy indiv.')\n",
    "\n",
    "leg_tri = Line2D([0], [0],\n",
    "                 marker='^',\n",
    "                 color='w',\n",
    "                 markerfacecolor='lightseagreen',\n",
    "                 markersize=10,\n",
    "                 alpha=0.5,\n",
    "                 linestyle='None',\n",
    "                 label='MS indiv.')\n",
    "\n",
    "ax.legend(handles=[leg_o, leg_tri],\n",
    "          loc='upper left',fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,bbox_to_anchor= (-0.05,0.8))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DTI_MD_Prec_MS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1ca5f-dded-412e-8070-5ab77d075496",
   "metadata": {},
   "source": [
    "### h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340390c-5b75-46c0-b1e3-606998f4b354",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)#, sharex=True)\n",
    "fig.subplots_adjust(hspace=0.05)  # adjust space between Axes\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax1)\n",
    "y_data = np.array(AccMFulls_FA_MS)\n",
    "g_pos = np.array([1])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(AccM20_FA_MS)\n",
    "g_pos = np.array([1.7])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(AccM7_FA_MS)\n",
    "g_pos = np.array([2])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(AccM20NL_FA_MS)\n",
    "g_pos = np.array([2.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(AccM7NL_FA_MS)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "ax1.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "ax1.set_xticklabels(['Full', 'Mid', 'Min', 'Mid', 'Min'], fontsize=32, rotation=90)\n",
    "\n",
    "leg_patch1 = mpatches.Patch(color='lightseagreen', label='Mid. (SBI)')\n",
    "leg_patch2 = mpatches.Patch(color='mediumturquoise', label='Min. (SBI)')\n",
    "leg_patch3 = mpatches.Patch(color='sandybrown', label='Mid. (NLLS)')\n",
    "leg_patch4 = mpatches.Patch(color='burlywood', label='Min. (NLLS)')\n",
    "leg_patch5 = mpatches.Patch(color='gray', label='Full Comp.')\n",
    "\n",
    "ax1.ticklabel_format(axis='y', style='sci', scilimits=(-1, -1))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DTI_FA_Acc_MS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22071127-cf12-4b93-9de7-979f59e6d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)#, sharex=True)\n",
    "fig.subplots_adjust(hspace=0.05)  # adjust space between Axes\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax1)\n",
    "y_data = np.array(SSIMFulls_FA_MS)\n",
    "g_pos = np.array([1])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM20_FA_MS)\n",
    "g_pos = np.array([1.7])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM7_FA_MS)\n",
    "g_pos = np.array([2])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM20NL_FA_MS)\n",
    "g_pos = np.array([2.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM7NL_FA_MS)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "ax1.axhline(0.66, lw=3, ls='--', c='k')\n",
    "ax1.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "ax1.set_xticklabels(['Full', 'Mid', 'Min', 'Mid', 'Min'], fontsize=32, rotation=90)\n",
    "ax1.set_ylim([0,1])\n",
    "\n",
    "leg_patch1 = mpatches.Patch(color='lightseagreen', label='Mid. (SBI)')\n",
    "leg_patch2 = mpatches.Patch(color='mediumturquoise', label='Min. (SBI)')\n",
    "leg_patch3 = mpatches.Patch(color='sandybrown', label='Mid. (NLLS)')\n",
    "leg_patch4 = mpatches.Patch(color='burlywood', label='Min. (NLLS)')\n",
    "leg_patch5 = mpatches.Patch(color='gray', label='Full Comp.')\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DTI_FA_SSIM_MS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a1c9a-e5f4-45a2-8f72-76e1760966cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = np.array(PrecFull_SBI_FA_MS)\n",
    "g_pos = np.array([1])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "fig,ax = plt.subplots()\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "y_data = np.array(Prec20_SBI_FA_MS)\n",
    "g_pos = np.array([1.2])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_FA_MS)\n",
    "g_pos = np.array([1.4])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "y_data = np.array(PrecFull_NLLS_FA_MS)\n",
    "g_pos = np.array([2])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(Prec20_NLLS_FA_MS)\n",
    "g_pos = np.array([2.2])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_FA_MS)\n",
    "g_pos = np.array([2.4])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "plt.scatter(x_data[:5],y_data[:5],marker='o',color=colors2,s=100,alpha=0.5)\n",
    "plt.scatter(x_data[5:],y_data[5:],marker='^',color=colors2,s=100,alpha=0.5)\n",
    "\n",
    "x = np.arange(1.85,2.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_FA_MS)[~np.isnan(PrecFull_NLLS_FA_MS)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_FA_MS)[~np.isnan(PrecFull_NLLS_FA_MS)], 77)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.85,1.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_FA_MS)[~np.isnan(PrecFull_SBI_FA_MS)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_FA_MS)[~np.isnan(PrecFull_SBI_FA_MS)], 77)\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "plt.xticks([1,1.2,1.4,2,2.2,2.4],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "leg_o = Line2D([0], [0],\n",
    "               marker='o',\n",
    "               color='w',                # no line\n",
    "               markerfacecolor='lightseagreen',\n",
    "               markersize=10,\n",
    "               alpha=0.5,\n",
    "               linestyle='None',\n",
    "               label='Healthy indiv.')\n",
    "\n",
    "leg_tri = Line2D([0], [0],\n",
    "                 marker='^',\n",
    "                 color='w',\n",
    "                 markerfacecolor='lightseagreen',\n",
    "                 markersize=10,\n",
    "                 alpha=0.5,\n",
    "                 linestyle='None',\n",
    "                 label='MS indiv.')\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DTI_FA_Prec_MS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb10af9-f149-4232-8b7f-6e01e2f96d78",
   "metadata": {},
   "source": [
    "# Figure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d6b48-b860-434e-8e35-fcb7e3158786",
   "metadata": {},
   "source": [
    "## HCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc068bfc-4996-47e4-b762-56b3befe6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "_, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "\n",
    "\n",
    "data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "# Get the indices of True values\n",
    "true_indices = np.argwhere(mask)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = true_indices.min(axis=0)\n",
    "max_coords = true_indices.max(axis=0)\n",
    "\n",
    "maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "axial_middle = maskdata.shape[2] // 2\n",
    "maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices7 = [0]+selected_indices\n",
    "\n",
    "bvalsHCP7_1 = bvalsHCP[selected_indices7]\n",
    "bvecsHCP7_1 = bvecsHCP[selected_indices7]\n",
    "\n",
    "i=3\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(14):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx = []\n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "true_indx = selected_indices7+[t+69 for t in true_indx]\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "cutout = np.sum(TestData4D[:,:,axial_middle,:69], axis=-1) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd3f78-22ef-4649-b261-32de8ba1da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIHCPFull.pickle\"):\n",
    "    with open(f\"{network_path}/DKIHCPFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(13000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(13000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(26000))   \n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT2,DT3,DT5])\n",
    "    KT = np.vstack([KT2,KT3,KT5])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([4*13000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabExt.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabExt,S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    if not os.path.exists(f\"{network_path}/DKIHCPFull.pickle\"):\n",
    "        with open(f\"{network_path}/DKIHCPFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIHCPMin.pickle\"):\n",
    "    with open(f\"{network_path}/DKIHCPMin.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(3*13000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(3*13000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(3*26000))   \n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT2,DT3,DT5])\n",
    "    KT = np.vstack([KT2,KT3,KT5])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([3*52000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabHCP7.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabHCP7,S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    if not os.path.exists(f\"{network_path}/DKIHCPMin.pickle\"):\n",
    "        with open(f\"{network_path}/DKIHCPMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorMin, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c399f-5bd3-4cd6-858a-63edd2b63e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(TestData4D[:,:,axial_middle,:], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "def optimize_chunk(pixels):\n",
    "    results = []\n",
    "    for i, j in pixels:\n",
    "        samples = posteriorFull.sample((500,), x=TestData4D[i,j,axial_middle, :],show_progress_bars=False)\n",
    "        results.append((i, j, samples.mean(axis=0)))\n",
    "    return results\n",
    "\n",
    "chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "NoiseEst = np.zeros([62, 68 ,22])\n",
    "for chunk in results:\n",
    "    for i, j, x in chunk:\n",
    "        NoiseEst[i, j] = x\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])\n",
    "\n",
    "MK_SBIFull  = np.zeros([62, 68])\n",
    "AK_SBIFull  = np.zeros([62, 68])\n",
    "RK_SBIFull  = np.zeros([62, 68])\n",
    "MKT_SBIFull = np.zeros([62, 68])\n",
    "KFA_SBIFull = np.zeros([62, 68])\n",
    "for i in tqdm(range(62)):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "        MK_SBIFull[i,j] = Metrics[0]\n",
    "        AK_SBIFull[i,j] = Metrics[1]\n",
    "        RK_SBIFull[i,j] = Metrics[2]\n",
    "        MKT_SBIFull[i,j] = Metrics[3]\n",
    "        KFA_SBIFull[i,j] = Metrics[4]\n",
    "KFA_SBIFull[np.isnan(KFA_SBIFull)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ec6e0-3736-4590-924c-40af3e8d23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(TestData4D[:,:,axial_middle,:], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "Arr = TestData4D[:,:,axial_middle, true_indx]\n",
    "# Define the function for optimization\n",
    "def optimize_chunk(pixels):\n",
    "    results = []\n",
    "    for i, j in pixels:\n",
    "        samples = posteriorMin.sample((500,), x=Arr[i,j],show_progress_bars=False)\n",
    "        results.append((i, j, samples.mean(axis=0)))\n",
    "    return results\n",
    "\n",
    "chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    ")\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "\n",
    "NoiseEst7 = np.zeros([62, 68 ,22])\n",
    "for chunk in results:\n",
    "    for i, j, x in chunk:\n",
    "        NoiseEst7[i, j] = x\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst7[i,j]))),NoiseEst7[i,j,6:]])\n",
    "\n",
    "MK_SBI7  = np.zeros([62, 68])\n",
    "AK_SBI7  = np.zeros([62, 68])\n",
    "RK_SBI7  = np.zeros([62, 68])\n",
    "MKT_SBI7 = np.zeros([62, 68])\n",
    "KFA_SBI7 = np.zeros([62, 68])\n",
    "for i in tqdm(range(62)):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "        MK_SBI7[i,j] = Metrics[0]\n",
    "        AK_SBI7[i,j] = Metrics[1]\n",
    "        RK_SBI7[i,j] = Metrics[2]\n",
    "        MKT_SBI7[i,j] = Metrics[3]\n",
    "        KFA_SBI7[i,j] = Metrics[4]\n",
    "KFA_SBI7[np.isnan(KFA_SBI7)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38437d1a-8fb6-4570-acf6-cf104f6a662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodelNL = dki.DiffusionKurtosisModel(gtabExt,fit_method='NLLS')\n",
    "dkifitNL = dkimodelNL.fit(TestData[:,:,:])\n",
    "MK_NLFull  = np.zeros([62, 68])\n",
    "AK_NLFull  = np.zeros([62, 68])\n",
    "RK_NLFull  = np.zeros([62, 68])\n",
    "MKT_NLFull = np.zeros([62, 68])\n",
    "KFA_NLFull = np.zeros([62, 68])\n",
    "for i in range(62):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt)\n",
    "        MK_NLFull[i,j] = Metrics[0]\n",
    "        AK_NLFull[i,j] = Metrics[1]\n",
    "        RK_NLFull[i,j] = Metrics[2]\n",
    "        MKT_NLFull[i,j] = Metrics[3]\n",
    "        KFA_NLFull[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4585e-f1ad-4a03-8099-2f37b78f74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodelNL = dki.DiffusionKurtosisModel(gtabHCP7,fit_method='NLLS')\n",
    "dkifitNL = dkimodelNL.fit(TestData[:,:,true_indx])\n",
    "MK_NL7  = np.zeros([62, 68])\n",
    "AK_NL7  = np.zeros([62, 68])\n",
    "RK_NL7 = np.zeros([62, 68])\n",
    "MKT_NL7 = np.zeros([62, 68])\n",
    "KFA_NL7 = np.zeros([62, 68])\n",
    "for i in range(62):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt)\n",
    "        MK_NL7[i,j] = Metrics[0]\n",
    "        AK_NL7[i,j] = Metrics[1]\n",
    "        RK_NL7[i,j] = Metrics[2]\n",
    "        MKT_NL7[i,j] = Metrics[3]\n",
    "        KFA_NL7[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48d6b4-ee39-4794-8b53-e65db683525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices7 = [0]+selected_indices\n",
    "\n",
    "bvalsHCP7_1 = bvalsHCP[selected_indices7]\n",
    "bvecsHCP7_1 = bvecsHCP[selected_indices7]\n",
    "\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(14):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx = []\n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "selected_indices7 = selected_indices7+[t+69 for t in true_indx]\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "\n",
    "temp_bvecs = bvecsHCP[bvalsHCP>0]\n",
    "temp_bvals = bvalsHCP[bvalsHCP>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(18):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "temp = selected_indices\n",
    "\n",
    "bvalsHCP7_1 = np.insert(temp_bvals[temp],0,0)\n",
    "bvecsHCP7_1 = np.insert(temp_bvecs[temp],0,[0,0,0],axis=0)\n",
    "\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(27):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP20 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx_one = []\n",
    "for b in bvecsHCP7_1:\n",
    "    true_indx_one.append(np.linalg.norm(b-bvecsHCP,axis=1).argmin())\n",
    "true_indx = []        \n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "selected_indices20 = true_indx_one+[t+69 for t in true_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91919fe-45ed-48c8-b4bc-4889e9b15054",
   "metadata": {},
   "outputs": [],
   "source": [
    "gTabsF = []\n",
    "gTabs7 = []\n",
    "gTabs20 = []\n",
    "\n",
    "FullDat   = []\n",
    "\n",
    "for i in tqdm(range(1,33)):\n",
    "    fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    gTabsF.append(gtabExt)\n",
    "    \n",
    "    bvalsHCP7 = gtabExt.bvals[selected_indices7]\n",
    "    bvecsHCP7 = gtabExt.bvecs[selected_indices7]\n",
    "    gtabHCP7 = gradient_table(bvalsHCP7, bvecsHCP7)\n",
    "    gTabs7.append(gtabHCP7)\n",
    "\n",
    "    bvalsHCP20 = gtabExt.bvals[selected_indices20]\n",
    "    bvecsHCP20 = gtabExt.bvecs[selected_indices20]\n",
    "    gtabHCP20 = gradient_table(bvalsHCP20, bvecsHCP20)\n",
    "    gTabs20.append(gtabHCP20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1315a61-defc-4313-9e58-41577d392b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DKIMultiHCPFull_300k.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiHCPFull_300k.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(75000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(75000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(150000))   \n",
    "    \n",
    "    DT = np.vstack([DT5,DT2,DT3])\n",
    "    KT = np.vstack([KT5,KT2,KT3])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([DT.shape[0]])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    A  = np.random.choice(32,DT.shape[0])\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabsF[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabsF[A[i]],S0[i],50)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = np.hstack([Obs,np.expand_dims(A, axis=-1)])\n",
    "        \n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 50)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiHCPFull_300k.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiHCPFull_300k.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n",
    "    import os\n",
    "    os.system(\"say 'DKI network done'\") # or '\\7'\n",
    "\n",
    "if os.path.exists(f\"{network_path}/DKIMultiHCPMin_300k.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiHCPMin_300k.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(75000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(75000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(150000))   \n",
    "    \n",
    "    DT = np.vstack([DT5,DT2,DT3])\n",
    "    KT = np.vstack([KT5,KT2,KT3])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([DT.shape[0]])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    A  = np.random.choice(32,DT.shape[0])\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabsF[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabsF[A[i]],S0[i],50)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = np.hstack([Obs[:,selected_indices7],np.expand_dims(A, axis=-1)])\n",
    "        \n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 50)\n",
    "    posteriorMin = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiHCPMin_300k.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiHCPMin_300k.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorMin, handle)\n",
    "    import os\n",
    "    os.system(\"say 'DKI network done'\") # or '\\7'\n",
    "\n",
    "if os.path.exists(f\"{network_path}/DKIMultiHCPMid_300k.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiHCPMid_300k.pickle\", \"rb\") as handle:\n",
    "        posteriorMid = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(75000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(75000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(150000))   \n",
    "    \n",
    "    DT = np.vstack([DT5,DT2,DT3])\n",
    "    KT = np.vstack([KT5,KT2,KT3])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([DT.shape[0]])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    A  = np.random.choice(32,DT.shape[0])\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabsF[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabsF[A[i]],S0[i],50)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = np.hstack([Obs[:,selected_indices20],np.expand_dims(A, axis=-1)])\n",
    "        \n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 50)\n",
    "    posteriorMid = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiHCPMid_300k.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiHCPMid_300k.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorMid, handle)\n",
    "    import os\n",
    "    os.system(\"say 'DKI network done'\") # or '\\7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ccc75a-cfde-4f52-b33a-06c851d43347",
   "metadata": {},
   "outputs": [],
   "source": [
    "TD = []\n",
    "axial_middles = []\n",
    "masks = []\n",
    "WMs = []\n",
    "for kk in tqdm(range(32)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk+1)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk+1)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk+1)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middles.append(axial_middle)\n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    TD.append(TestData4D)\n",
    "    masks.append(mask[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,axial_middle])\n",
    "    WM, affine, img = load_nifti('./HCP_data/WM_Masks/c2Pat'+str(kk+1)+'_FP.nii', return_img=True)\n",
    "    WMs.append(np.fliplr(WM[:,:,axial_middle]>0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff01ac1-1d58-42bd-8981-de8434f5cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{DatFolder}/Full_MK_HCP.npy\"):\n",
    "    MKFullArr = np.load(f\"{DatFolder}/Full_MK_HCP.npy\",allow_pickle=True)\n",
    "    RKFullArr = np.load(f\"{DatFolder}/Full_RK_HCP.npy\",allow_pickle=True)\n",
    "    AKFullArr = np.load(f\"{DatFolder}/Full_AK_HCP.npy\",allow_pickle=True)\n",
    "else:\n",
    "    MKFullArr = []\n",
    "    RKFullArr = []\n",
    "    AKFullArr = []\n",
    "    for kk in tqdm(range(32)):\n",
    "        \n",
    "        # Compute the mask where the sum is not zero\n",
    "        mask = np.sum(TD[kk][:, :, axial_middles[kk], :69], axis=-1) != 0\n",
    "        \n",
    "        # Get the indices where mask is True\n",
    "        indices = np.argwhere(mask)\n",
    "        def optimize_chunk(pixels):\n",
    "            results = []\n",
    "            for i, j in pixels:\n",
    "                posterior_samples_1 = posteriorFull.sample((500,), x=np.hstack([TD[kk][i, j, axial_middles[kk], :],kk]),show_progress_bars=False)\n",
    "                results.append((i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])))\n",
    "            return results\n",
    "        \n",
    "        chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "        results = Parallel(n_jobs=8)(\n",
    "            delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    "        )\n",
    "        \n",
    "        # Initialize NoiseEst with the appropriate shape\n",
    "        ArrShape = mask.shape\n",
    "    \n",
    "        \n",
    "        NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "        \n",
    "        # Assign the optimization results to NoiseEst\n",
    "        for chunk in results:\n",
    "            for i, j, x in chunk:\n",
    "                NoiseEst[i, j] = x\n",
    "                \n",
    "        MK_SBIFull  = np.zeros([NoiseEst.shape[0], NoiseEst.shape[1]])\n",
    "        AK_SBIFull  = np.zeros([NoiseEst.shape[0], NoiseEst.shape[1]])\n",
    "        RK_SBIFull  = np.zeros([NoiseEst.shape[0], NoiseEst.shape[1]])\n",
    "        NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "        for i in range(NoiseEst.shape[0]):\n",
    "            for j in range(NoiseEst.shape[1]):    \n",
    "                NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])\n",
    "                Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "                MK_SBIFull[i,j] = Metrics[0]\n",
    "                AK_SBIFull[i,j] = Metrics[1]\n",
    "                RK_SBIFull[i,j] = Metrics[2]\n",
    "            \n",
    "    \n",
    "        MKFullArr.append(MK_SBIFull)\n",
    "        RKFullArr.append(RK_SBIFull)\n",
    "        AKFullArr.append(AK_SBIFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b55a37-850a-4a5d-b9cb-1c56e12f5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{DatFolder}/Min_MK_HCP.npy\"):\n",
    "    MKMinArr = np.load(f\"{DatFolder}/Min_MK_HCP.npy\",allow_pickle=True)\n",
    "    RKMinArr = np.load(f\"{DatFolder}/Min_RK_HCP.npy\",allow_pickle=True)\n",
    "    AKMinArr = np.load(f\"{DatFolder}/Min_AK_HCP.npy\",allow_pickle=True)\n",
    "else:\n",
    "    MKMinArr = []\n",
    "    RKMinArr = []\n",
    "    AKMinArr = []\n",
    "    for kk in tqdm(range(32)):\n",
    "        \n",
    "        # Compute the mask where the sum is not zero\n",
    "        mask = np.sum(TD[kk][:, :, axial_middles[kk], :69], axis=-1) != 0\n",
    "        \n",
    "        # Get the indices where mask is True\n",
    "        indices = np.argwhere(mask)\n",
    "        Arr = TD[kk][:,:, axial_middles[kk], selected_indices7]\n",
    "        def optimize_chunk(pixels):\n",
    "            results = []\n",
    "            for i, j in pixels:\n",
    "                posterior_samples_1 = posteriorMin.sample((500,), x=np.hstack([Arr[i,j],kk]),show_progress_bars=False)\n",
    "                results.append((i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])))\n",
    "            return results\n",
    "        \n",
    "        chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "        results = Parallel(n_jobs=8)(\n",
    "            delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    "        )\n",
    "        \n",
    "        # Initialize NoiseEst with the appropriate shape\n",
    "        ArrShape = mask.shape\n",
    "    \n",
    "        \n",
    "        NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "        \n",
    "        # Assign the optimization results to NoiseEst\n",
    "        for chunk in results:\n",
    "            for i, j, x in chunk:\n",
    "                NoiseEst[i, j] = x\n",
    "        \n",
    "        NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "        for i in range(NoiseEst.shape[0]):\n",
    "            for j in range(NoiseEst.shape[1]):    \n",
    "                NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])\n",
    "    \n",
    "        MK_SBIFull  = np.zeros([NoiseEst.shape[0], NoiseEst.shape[1]])\n",
    "        AK_SBIFull  = np.zeros([NoiseEst.shape[0], NoiseEst.shape[1]])\n",
    "        RK_SBIFull  = np.zeros([NoiseEst.shape[0], NoiseEst.shape[1]])\n",
    "\n",
    "        for i in range(NoiseEst.shape[0]):\n",
    "            for j in range(NoiseEst.shape[1]): \n",
    "                Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "                MK_SBIFull[i,j] = Metrics[0]\n",
    "                AK_SBIFull[i,j] = Metrics[1]\n",
    "                RK_SBIFull[i,j] = Metrics[2]\n",
    "\n",
    "    \n",
    "        MKMinArr.append(MK_SBIFull)\n",
    "        RKMinArr.append(RK_SBIFull)\n",
    "        AKMinArr.append(AK_SBIFull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca0f35-c076-4b4b-ad11-9f995aa54357",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{DatFolder}/Mid_MK_HCP.npy\"):\n",
    "    MKMidArr = np.load(f\"{DatFolder}/Mid_MK_HCP.npy\",allow_pickle=True)\n",
    "    RKMidArr = np.load(f\"{DatFolder}/Mid_RK_HCP.npy\",allow_pickle=True)\n",
    "    AKMidArr = np.load(f\"{DatFolder}/Mid_AK_HCP.npy\",allow_pickle=True)\n",
    "else:\n",
    "    MKMidArr = []\n",
    "    RKMidArr = []\n",
    "    AKMidArr = []\n",
    "    for kk in tqdm(range(32)):\n",
    "        \n",
    "        # Compute the mask where the sum is not zero\n",
    "        mask = np.sum(TD[kk][:, :, axial_middles[kk], :69], axis=-1) != 0\n",
    "        \n",
    "        # Get the indices where mask is True\n",
    "        indices = np.argwhere(mask)\n",
    "        Arr = TD[kk][:,:, axial_middles[kk], selected_indices20]\n",
    "        def optimize_chunk(pixels):\n",
    "            results = []\n",
    "            for i, j in pixels:\n",
    "                posterior_samples_1 = posteriorMid.sample((500,), x=np.hstack([Arr[i,j],kk]),show_progress_bars=False)\n",
    "                results.append((i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])))\n",
    "            return results\n",
    "        \n",
    "        chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "        results = Parallel(n_jobs=8)(\n",
    "            delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    "        )\n",
    "        \n",
    "        # Initialize NoiseEst with the appropriate shape\n",
    "        ArrShape = mask.shape\n",
    "    \n",
    "        \n",
    "        NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "        \n",
    "        # Assign the optimization results to NoiseEst\n",
    "        for chunk in results:\n",
    "            for i, j, x in chunk:\n",
    "                NoiseEst[i, j] = x\n",
    "        \n",
    "        NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "        for i in range(NoiseEst.shape[0]):\n",
    "            for j in range(NoiseEst.shape[1]):    \n",
    "                NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])\n",
    "    \n",
    "        MK_SBIFull  = np.zeros([NoiseEst.shape[0], NoiseEst.shape[1]])\n",
    "        AK_SBIFull  = np.zeros([NoiseEst.shape[0], NoiseEst.shape[1]])\n",
    "        RK_SBIFull  = np.zeros([NoiseEst.shape[0], NoiseEst.shape[1]])\n",
    "    \n",
    "        for i in range(NoiseEst.shape[0]):\n",
    "            for j in range(NoiseEst.shape[1]): \n",
    "                Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "                MK_SBIFull[i,j] = Metrics[0]\n",
    "                AK_SBIFull[i,j] = Metrics[1]\n",
    "                RK_SBIFull[i,j] = Metrics[2]\n",
    "    \n",
    "        MKMidArr.append(MK_SBIFull)\n",
    "        RKMidArr.append(RK_SBIFull)\n",
    "        AKMidArr.append(AK_SBIFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f3f6d-13da-4e63-ba49-e75d2a8cda31",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullNLArr = []\n",
    "RKFullNLArr = []\n",
    "AKFullNLArr = []\n",
    "MKTFullNLArr = []\n",
    "KFAFullNLArr = []\n",
    "for kk in tqdm(range(32)):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabsF[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(TD[kk][:,:,axial_middles[kk]])\n",
    "    ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    MK_NL7_t  = np.zeros(ArrShape)\n",
    "    AK_NL7_t  = np.zeros(ArrShape)\n",
    "    RK_NL7_t = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7_t[i,j] = Metrics[0]\n",
    "            AK_NL7_t[i,j] = Metrics[1]\n",
    "            RK_NL7_t[i,j] = Metrics[2]\n",
    "    MKFullNLArr.append(MK_NL7_t)\n",
    "    RKFullNLArr.append(RK_NL7_t)\n",
    "    AKFullNLArr.append(AK_NL7_t)\n",
    "\n",
    "MKMidNLArr = []\n",
    "RKMidNLArr = []\n",
    "AKMidNLArr = []\n",
    "for kk in tqdm(range(32)):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabs20[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(TD[kk][:,:,axial_middles[kk],selected_indices20])\n",
    "    ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    MK_NL7_t  = np.zeros(ArrShape)\n",
    "    AK_NL7_t  = np.zeros(ArrShape)\n",
    "    RK_NL7_t = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7_t[i,j] = Metrics[0]\n",
    "            AK_NL7_t[i,j] = Metrics[1]\n",
    "            RK_NL7_t[i,j] = Metrics[2]\n",
    "    MKMidNLArr.append(MK_NL7_t)\n",
    "    RKMidNLArr.append(RK_NL7_t)\n",
    "    AKMidNLArr.append(AK_NL7_t)\n",
    "\n",
    "MKMinNLArr = []\n",
    "RKMinNLArr = []\n",
    "AKMinNLArr = []\n",
    "for kk in tqdm(range(32)):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabs7[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(TD[kk][:,:,axial_middles[kk],selected_indices7])\n",
    "    ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    MK_NL7_t  = np.zeros(ArrShape)\n",
    "    AK_NL7_t  = np.zeros(ArrShape)\n",
    "    RK_NL7_t = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7_t[i,j] = Metrics[0]\n",
    "            AK_NL7_t[i,j] = Metrics[1]\n",
    "            RK_NL7_t[i,j] = Metrics[2]\n",
    "    MKMinNLArr.append(MK_NL7_t)\n",
    "    RKMinNLArr.append(RK_NL7_t)\n",
    "    AKMinNLArr.append(AK_NL7_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb64a243-ebb8-4506-8c6c-18c5b49afa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccM7_MK = []\n",
    "AccM20_MK = []\n",
    "AccMFulls_MK = []\n",
    "\n",
    "AccM7NL_MK = []\n",
    "AccM20NL_MK = []\n",
    "\n",
    "SSIM7_MK = []\n",
    "SSIM20_MK = []\n",
    "SSIMFulls_MK = []\n",
    "\n",
    "SSIM7NL_MK = []\n",
    "SSIM20NL_MK = []\n",
    "for i in range(32):\n",
    "    M7 =MKMinArr[i]\n",
    "    MF =MKFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    AccM7_MK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "\n",
    "    M7 =MKMidArr[i]\n",
    "    MF =MKFullArr[i]\n",
    "    AccM20_MK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "\n",
    "    M7 =MKFullArr[i]\n",
    "    MF =MKFullNLArr[i]\n",
    "    AccMFulls_MK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 =MKMinNLArr[i]\n",
    "    M7[np.isnan(M7)] = 0\n",
    "    MF =MKFullNLArr[i]\n",
    "    AccM7NL_MK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "\n",
    "    M7 =MKMidNLArr[i]\n",
    "    M7[np.isnan(M7)] = 0\n",
    "    MF =MKFullNLArr[i]\n",
    "    AccM20NL_MK.append(np.nanmean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    NS1 =MKMinArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =MKFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM7_MK.append(result)\n",
    "\n",
    "    NS1 =MKMidArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =MKFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM20_MK.append(result)\n",
    "    \n",
    "    NS1 =MKFullArr[i]\n",
    "    NS2 =MKFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIMFulls_MK.append(result)\n",
    "\n",
    "    NS1 =MKMinNLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =MKFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM7NL_MK.append(result)\n",
    "\n",
    "    NS1 =MKMidNLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =MKFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM20NL_MK.append(result)\n",
    "\n",
    "\n",
    "Prec7_SBI_MK = []\n",
    "Prec20_SBI_MK = []\n",
    "PrecFull_SBI_MK = []\n",
    "\n",
    "Prec7_NLLS_MK = []\n",
    "Prec20_NLLS_MK = []\n",
    "PrecFull_NLLS_MK = []\n",
    "for i in range(32):\n",
    "    Prec7_SBI_MK.append(np.std(MKMinArr[i][WMs[i]]))\n",
    "    Prec20_SBI_MK.append(np.std(MKMidArr[i][WMs[i]]))\n",
    "    PrecFull_SBI_MK.append(np.std(MKFullArr[i][WMs[i]]))\n",
    "\n",
    "    Prec7_NLLS_MK.append(np.std(MKMinNLArr[i][WMs[i]]))\n",
    "    Prec20_NLLS_MK.append(np.std(MKMidNLArr[i][WMs[i]]))\n",
    "    PrecFull_NLLS_MK.append(np.std(MKFullNLArr[i][WMs[i]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b326754-3a94-4eb2-989a-1b7617cded71",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccM7_AK = []\n",
    "AccM20_AK = []\n",
    "AccMFulls_AK = []\n",
    "\n",
    "AccM7NL_AK = []\n",
    "AccM20NL_AK = []\n",
    "\n",
    "SSIM7_AK = []\n",
    "SSIM20_AK = []\n",
    "SSIMFulls_AK = []\n",
    "\n",
    "SSIM7NL_AK = []\n",
    "SSIM20NL_AK = []\n",
    "for i in range(32):\n",
    "    M7 =AKMinArr[i]\n",
    "    MF =AKFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    AccM7_AK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "\n",
    "    M7 =AKMidArr[i]\n",
    "    MF =AKFullArr[i]\n",
    "    AccM20_AK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "\n",
    "    M7 =AKFullArr[i]\n",
    "    MF =AKFullNLArr[i]\n",
    "    AccMFulls_AK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 =AKMinNLArr[i]\n",
    "    M7[np.isnan(M7)] = 0\n",
    "    MF =AKFullNLArr[i]\n",
    "    AccM7NL_AK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "\n",
    "    M7 =AKMidNLArr[i]\n",
    "    M7[np.isnan(M7)] = 0\n",
    "    MF =AKFullNLArr[i]\n",
    "    AccM20NL_AK.append(np.nanmean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    \n",
    "    NS1 =AKMinArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =AKFullArr[i]\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM7_AK.append(result)\n",
    "\n",
    "    NS1 =AKMidArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =AKFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM20_AK.append(result)\n",
    "    \n",
    "    NS1 =AKFullArr[i]\n",
    "    NS2 =AKFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIMFulls_AK.append(result)\n",
    "\n",
    "    NS1 =AKMinNLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =AKFullNLArr[i]\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM7NL_AK.append(result)\n",
    "\n",
    "    NS1 =AKMidNLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =AKFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM20NL_AK.append(result)\n",
    "\n",
    "Prec7_SBI_AK = []\n",
    "Prec20_SBI_AK = []\n",
    "PrecFull_SBI_AK = []\n",
    "\n",
    "Prec7_NLLS_AK = []\n",
    "Prec20_NLLS_AK = []\n",
    "PrecFull_NLLS_AK = []\n",
    "for i in range(32):\n",
    "    Prec7_SBI_AK.append(np.std(AKMinArr[i][WMs[i]]))\n",
    "    Prec20_SBI_AK.append(np.std(AKMidArr[i][WMs[i]]))\n",
    "    PrecFull_SBI_AK.append(np.std(AKFullArr[i][WMs[i]]))\n",
    "\n",
    "    Prec7_NLLS_AK.append(np.std(AKMinNLArr[i][WMs[i]]))\n",
    "    Prec20_NLLS_AK.append(np.std(AKMidNLArr[i][WMs[i]]))\n",
    "    PrecFull_NLLS_AK.append(np.std(AKFullNLArr[i][WMs[i]]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93188757-b602-47cf-b21c-426156827dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccM7_RK = []\n",
    "AccM20_RK = []\n",
    "AccMFulls_RK = []\n",
    "\n",
    "AccM7NL_RK = []\n",
    "AccM20NL_RK = []\n",
    "\n",
    "SSIM7_RK = []\n",
    "SSIM20_RK = []\n",
    "SSIMFulls_RK = []\n",
    "\n",
    "SSIM7NL_RK = []\n",
    "SSIM20NL_RK = []\n",
    "for i in range(32):\n",
    "    M7 =RKMinArr[i]\n",
    "    MF =RKFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    AccM7_RK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "\n",
    "    M7 =RKMidArr[i]\n",
    "    MF =RKFullArr[i]\n",
    "    AccM20_RK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "\n",
    "    M7 =RKFullArr[i]\n",
    "    MF =RKFullNLArr[i]\n",
    "    AccMFulls_RK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 =RKMinNLArr[i]\n",
    "    M7[np.isnan(M7)] = 0\n",
    "    MF =RKFullNLArr[i]\n",
    "    AccM7NL_RK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "\n",
    "    M7 =RKMidNLArr[i]\n",
    "    M7[np.isnan(M7)] = 0\n",
    "    MF =RKFullNLArr[i]\n",
    "    AccM20NL_RK.append(np.nanmean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    \n",
    "    NS1 =RKMinArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =RKFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM7_RK.append(result)\n",
    "\n",
    "    NS1 =RKMidArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =RKFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM20_RK.append(result)\n",
    "    \n",
    "    NS1 =RKFullArr[i]\n",
    "    NS2 =RKFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIMFulls_RK.append(result)\n",
    "\n",
    "    NS1 =RKMinNLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =RKFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM7NL_RK.append(result)\n",
    "\n",
    "    NS1 =RKMidNLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =RKFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM20NL_RK.append(result)\n",
    "\n",
    "Prec7_SBI_RK = []\n",
    "Prec20_SBI_RK = []\n",
    "PrecFull_SBI_RK = []\n",
    "\n",
    "Prec7_NLLS_RK = []\n",
    "Prec20_NLLS_RK = []\n",
    "PrecFull_NLLS_RK = []\n",
    "for i in range(32):\n",
    "    Prec7_SBI_RK.append(np.std(RKMinArr[i][WMs[i]]))\n",
    "    Prec20_SBI_RK.append(np.std(RKMidArr[i][WMs[i]]))\n",
    "    PrecFull_SBI_RK.append(np.std(RKFullArr[i][WMs[i]]))\n",
    "\n",
    "    Prec7_NLLS_RK.append(np.std(RKMinNLArr[i][WMs[i]]))\n",
    "    Prec20_NLLS_RK.append(np.std(RKMidNLArr[i][WMs[i]]))\n",
    "    PrecFull_NLLS_RK.append(np.std(RKFullNLArr[i][WMs[i]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94cd656-4896-45a9-9e37-0f33efc9762e",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e0f3a-5251-4f16-96a7-3da9f40c9f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.6,vmax=1.2)\n",
    "temp = np.copy(MK_SBIFull)\n",
    "temp[~cutout] = math.nan\n",
    "plt.imshow(temp.T,norm=tnorm,cmap='hot')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'MKSBIFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "temp = np.copy(AK_SBIFull)\n",
    "temp[~cutout] = math.nan\n",
    "plt.imshow(temp.T,norm=tnorm,cmap='hot')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'AKSBIFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "temp = np.copy(RK_SBIFull)\n",
    "temp[~cutout] = math.nan\n",
    "plt.imshow(temp.T,norm=tnorm,cmap='hot')\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(fraction=0.032, pad=0.04)\n",
    "cbar.ax.set_ylim(0,1)\n",
    "if Save: plt.savefig(FigLoc+'RKSBIFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665239ec-d66c-40f2-9043-9f12a0d2191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.6,vmax=1.2)\n",
    "temp = np.copy(MK_SBI7)\n",
    "temp = gaussian_filter(temp, sigma=0.5)\n",
    "temp[~cutout] = math.nan\n",
    "plt.imshow(temp.T,norm=tnorm,cmap='hot')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'MKSBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "temp = np.copy(AK_SBI7)\n",
    "temp = gaussian_filter(temp, sigma=0.5)\n",
    "temp[~cutout] = math.nan\n",
    "plt.imshow(temp.T,norm=tnorm,cmap='hot')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'AKSBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "temp = np.copy(RK_SBI7)\n",
    "temp = gaussian_filter(temp, sigma=0.5)\n",
    "temp[~cutout] = math.nan\n",
    "plt.imshow(temp.T,norm=tnorm,cmap='hot')\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(fraction=0.032, pad=0.04)\n",
    "cbar.ax.set_ylim(0,1)\n",
    "if Save: plt.savefig(FigLoc+'RKSBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9d5d0-fb22-4f89-9c28-b574041e3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK_SBIFull.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1c6a8-5f06-4f4a-9069-be5a14c177f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK_SBI7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66267c7-52cd-4b2a-9a89-14161f85cda1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ec6dc-cadf-4484-95a4-69d0908ad558",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = [0,1,2]\n",
    "data = np.abs((MK_SBIFull-MK_SBI7)*cutout).T\n",
    "data[~cutout.T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=1, vmax=2)\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(ticks=ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'MKDiffSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = np.abs((AK_SBIFull-AK_SBI7)*cutout).T\n",
    "data[~cutout.T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.nanmax(data)/2, vmax=np.nanmax(data))\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(ticks=ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'AKDiffSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = np.abs((RK_SBIFull-RK_SBI7)*cutout).T\n",
    "data[~cutout.T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=1,vmax=2)\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'RKDiffSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f131cf-7ee1-4df2-be1c-87fe70c580ed",
   "metadata": {},
   "source": [
    "### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c49ec2-a2f9-4da8-89b6-6a78923e129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.6,vmax=1.2)\n",
    "temp = np.copy(MK_NLFull)\n",
    "temp[~cutout] = math.nan\n",
    "plt.imshow(temp.T,norm=tnorm,cmap='hot')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'MKNLFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "temp = np.copy(AK_NLFull)\n",
    "temp[~cutout] = math.nan\n",
    "plt.imshow(temp.T,norm=tnorm,cmap='hot')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'AKNLFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "temp = np.copy(RK_NLFull)\n",
    "temp[~cutout] = math.nan\n",
    "plt.imshow(temp.T,norm=tnorm,cmap='hot')\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(fraction=0.032, pad=0.04)\n",
    "cbar.ax.set_ylim(0,1)\n",
    "if Save: plt.savefig(FigLoc+'RKNLFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d88426-e455-4b3e-be10-462c3767a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.6,vmax=1.2)\n",
    "temp = np.copy(MK_NL7)\n",
    "temp[~cutout] = math.nan\n",
    "plt.imshow(temp.T,norm=tnorm,cmap='hot')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'MKNL7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "temp = np.copy(AK_NL7)\n",
    "temp[~cutout] = math.nan\n",
    "plt.imshow(temp.T,norm=tnorm,cmap='hot')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'AKNL7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "temp = np.copy(RK_NL7)\n",
    "temp[~cutout] = math.nan\n",
    "plt.imshow(temp.T,norm=tnorm,cmap='hot')\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(fraction=0.032, pad=0.04)\n",
    "cbar.ax.set_ylim(0,1)\n",
    "if Save: plt.savefig(FigLoc+'RKNL7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedcd9b-2749-4877-919b-97823d0d4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs((MK_NLFull-MK_NL7)*cutout).T\n",
    "data[~cutout.T] = np.nan\n",
    "plt.imshow(data,cmap='Reds',vmin=0,vmax=2)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(ticks=[0,1,2])\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'MKDiffWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "dat = np.abs((AK_SBIFull-AK_SBI7)*cutout).T\n",
    "data[~cutout.T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.nanmax(dat)/2, vmax=np.nanmax(dat))\n",
    "data = np.abs((AK_NLFull-AK_NL7)*cutout).T\n",
    "data[~cutout.T] = np.nan\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(ticks=ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'AKDiffWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=1,vmax=2)\n",
    "data = np.abs((RK_NLFull-RK_NL7)*cutout).T\n",
    "data[~cutout.T] = np.nan\n",
    "#ticks = [0, np.round(np.max(data),10)]  #Adjust the number of ticks as needed\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'RKDiffWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c0328-40da-44eb-b117-d076cde4d567",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe18d5f5-9e96-4f6d-a307-853b6d840234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(3.2,4.8))\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "\n",
    "\n",
    "y_data = np.array(AccM7NL_MK)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "#ax1.yaxis.set_ticks(np.arange(0.0005, 0.006, 0.002))\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.sca(ax2)\n",
    "\n",
    "y_data = np.array(AccMFulls_MK)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20_MK)\n",
    "g_pos = np.array([1.55])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM7_MK)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20NL_MK)\n",
    "g_pos = np.array([2.65])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "plt.xticks([0.8,1.55,1.95,2.65,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "#ax1.set_ylim(0.4, 1)\n",
    "#ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax1.yaxis.set_ticks([0.4,0.7,1])\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "#ax2.set_ylim(0.0,0.3)\n",
    "#ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "#ax2.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "ax1.set_xlim(np.array(ax2.get_xlim()))\n",
    "\n",
    "# Show plot\n",
    "ax2.set_ylim(0,0.5)\n",
    "ax2.set_yticks([0,0.2,0.4])\n",
    "ax1.set_ylim(0.8,1.8)\n",
    "\n",
    "leg_patch2 = mpatches.Patch(color='mediumturquoise', label='SBI')\n",
    "leg_patch3 = mpatches.Patch(color='sandybrown', label='NLLS')\n",
    "leg_patch5 = mpatches.Patch(color='gray', label='Full \\nComp.')\n",
    "\n",
    "ax1.legend(\n",
    "    handles=[leg_patch5,leg_patch2],\n",
    "    loc='upper left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=1,\n",
    "fontsize=28,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,bbox_to_anchor= (-0.0,0.9))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Acc_MK.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633fa38-bbbc-4884-9b9e-74a0a0ac22ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SSIMFulls_MK)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20_MK)\n",
    "g_pos = np.array([1.5])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM7_MK)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20NL_MK)\n",
    "g_pos = np.array([2.6])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_MK)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_MK)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.xticks([0.8,1.5,1.95,2.6,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "ax.legend(\n",
    "    handles=[leg_patch3],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=1,\n",
    "fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,bbox_to_anchor= (-0.1,-0.05))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_MK.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ebc966-0f41-4283-81c0-1c29d85584b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(3.2,4.8))\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_MK)\n",
    "g_pos = np.array([2.5])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "#ax1.yaxis.set_ticks(np.arange(0.0005, 0.006, 0.002))\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.sca(ax2)\n",
    "\n",
    "y_data = np.array(PrecFull_SBI_MK)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_SBI_MK)\n",
    "g_pos = np.array([1.0])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_MK)\n",
    "g_pos = np.array([1.35])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(PrecFull_NLLS_MK)\n",
    "g_pos = np.array([1.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_NLLS_MK)\n",
    "g_pos = np.array([2.15])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_MK)\n",
    "g_pos = np.array([2.5])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "#ax1.set_ylim(0.4, 1)\n",
    "#ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax1.yaxis.set_ticks([0.4,0.7,1])\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "#ax2.set_ylim(0.0,0.3)\n",
    "#ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "#ax2.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "ax2.set_xlim([0.3,2.7])\n",
    "ax1.set_xlim(ax2.get_xlim())\n",
    "# Show plot\n",
    "ax2.set_ylim(0,0.9)\n",
    "ax2.set_yticks([0,0.4,0.8])\n",
    "ax1.set_ylim(1,13)\n",
    "\n",
    "x = np.arange(1.7,2.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_MK)[~np.isnan(PrecFull_NLLS_MK)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_MK)[~np.isnan(PrecFull_NLLS_MK)], 77)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.5,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_MK)[~np.isnan(PrecFull_SBI_MK)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_MK)[~np.isnan(PrecFull_SBI_MK)], 77)\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "plt.xticks([0.65,1,1.35,1.8,2.15,2.5],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "if Save: plt.savefig(FigLoc+'DKI_MK_Prec.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509b9c3-7336-4ddb-8797-4ccd93bef578",
   "metadata": {},
   "source": [
    "### d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc151ac-8af1-43d5-9bc3-1c2b8c5562bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1,figsize=(3.2,4.8))\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_data = np.array(AccMFulls_AK)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20_AK)\n",
    "g_pos = np.array([1.5])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM7_AK)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20NL_AK)\n",
    "g_pos = np.array([2.6])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM7NL_AK)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([0.8,1.5,1.95,2.6,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "ax1.set_ylim(0., 0.5)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Acc_AK.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc46c9a-5b51-4646-a279-80ee1b883b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SSIMFulls_AK)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20_AK)\n",
    "g_pos = np.array([1.5])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM7_AK)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20NL_AK)\n",
    "g_pos = np.array([2.6])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_AK)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_AK)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.xticks([0.8,1.5,1.95,2.6,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_AK.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2abec-83ea-49e2-8053-a90c02361ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(3.2,4.8))\n",
    "y_data = np.array(PrecFull_SBI_AK)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_SBI_AK)\n",
    "g_pos = np.array([1])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_AK)\n",
    "g_pos = np.array([1.35])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "y_data = np.array(PrecFull_NLLS_AK)\n",
    "g_pos = np.array([1.8])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_NLLS_AK)\n",
    "g_pos = np.array([2.15])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_AK)\n",
    "g_pos = np.array([2.5])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "x = np.arange(1.7,2.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_AK)[~np.isnan(PrecFull_NLLS_AK)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_AK)[~np.isnan(PrecFull_NLLS_AK)], 77)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.5,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_AK)[~np.isnan(PrecFull_SBI_AK)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_AK)[~np.isnan(PrecFull_SBI_AK)], 77)\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "plt.xticks([0.65,1,1.35,1.8,2.15,2.5],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "ax.set_xlim([0.4,2.7])\n",
    "if Save: plt.savefig(FigLoc+'DKI_AK_Prec.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e6bbe-4664-443c-b8bc-91815858e28c",
   "metadata": {},
   "source": [
    "### e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961f77e-69db-4e97-acc2-515746f2bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(3.2,4.8))\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "\n",
    "\n",
    "y_data = np.array(AccM7NL_RK)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "#ax1.yaxis.set_ticks(np.arange(0.0005, 0.006, 0.002))\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.sca(ax2)\n",
    "\n",
    "y_data = np.array(AccMFulls_RK)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20_RK)\n",
    "g_pos = np.array([1.55])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM7_RK)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20NL_RK)\n",
    "g_pos = np.array([2.65])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "plt.xticks([0.8,1.55,1.95,2.65,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "#ax1.set_ylim(0.4, 1)\n",
    "#ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax1.yaxis.set_ticks([0.4,0.7,1])\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "#ax2.set_ylim(0.0,0.3)\n",
    "#ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "#ax2.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "ax1.set_xlim(np.array(ax2.get_xlim()))\n",
    "\n",
    "# Show plot\n",
    "ax2.set_ylim(0,0.8)\n",
    "ax2.set_yticks([0,0.3,0.6])\n",
    "ax1.set_ylim(1,15)\n",
    "\n",
    "leg_patch2 = mpatches.Patch(color='mediumturquoise', label='SBI')\n",
    "leg_patch3 = mpatches.Patch(color='sandybrown', label='NLLS')\n",
    "leg_patch5 = mpatches.Patch(color='gray', label='Full \\nComp.')\n",
    "\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Acc_RK.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c623f-8980-4866-a982-74ae729ef7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SSIMFulls_RK)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20_RK)\n",
    "g_pos = np.array([1.5])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM7_RK)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20NL_RK)\n",
    "g_pos = np.array([2.6])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_RK)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_RK)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.xticks([0.8,1.5,1.95,2.6,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_RK.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba4524-bd0a-4ac5-82a9-d00e587a8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(3.2,4.8))\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_RK)\n",
    "g_pos = np.array([2.5])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "#ax1.yaxis.set_ticks(np.arange(0.0005, 0.006, 0.002))\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.sca(ax2)\n",
    "\n",
    "y_data = np.array(PrecFull_SBI_RK)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_SBI_RK)\n",
    "g_pos = np.array([1.0])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_RK)\n",
    "g_pos = np.array([1.35])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(PrecFull_NLLS_RK)\n",
    "g_pos = np.array([1.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_NLLS_RK)\n",
    "g_pos = np.array([2.15])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_RK)\n",
    "g_pos = np.array([2.5])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "#ax1.set_ylim(0.4, 1)\n",
    "#ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax1.yaxis.set_ticks([0.4,0.7,1])\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "#ax2.set_ylim(0.0,0.3)\n",
    "#ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "#ax2.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "ax2.set_xlim([0.3,2.7])\n",
    "ax1.set_xlim(ax2.get_xlim())\n",
    "# Show plot\n",
    "ax2.set_ylim(0,2)\n",
    "ax2.set_yticks([0,0.5,1,1.5])\n",
    "ax1.set_ylim(1,250)\n",
    "\n",
    "x = np.arange(1.7,2.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_RK)[~np.isnan(PrecFull_NLLS_RK)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_RK)[~np.isnan(PrecFull_NLLS_RK)], 77)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.5,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_RK)[~np.isnan(PrecFull_SBI_RK)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_RK)[~np.isnan(PrecFull_SBI_RK)], 77)\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "plt.xticks([0.65,1,1.35,1.8,2.15,2.5],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "if Save: plt.savefig(FigLoc+'DKI_RK_Prec.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a80024-1c85-40b4-a11c-4f48693a8c28",
   "metadata": {},
   "source": [
    "## MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28684a-332f-4d30-8386-dc72a4767112",
   "metadata": {},
   "outputs": [],
   "source": [
    "gTabsF = []\n",
    "Dats   = []\n",
    "\n",
    "gTabs7 = []\n",
    "gTabs20 = []\n",
    "Masks = []\n",
    "WMDir = MSDir+'WM_masks/'\n",
    "WMs = []\n",
    "for i,Name in tqdm(enumerate(['NMSS_11_1year','NMSS_15','NMSS_16','NMSS_18','NMSS_19','Ctrl055_R01_28','Ctrl056_R01_29','Ctrl057_R01_30'])):\n",
    "    MatDir = MSDir+Name\n",
    "\n",
    "    F = pmt.read_mat(MatDir+'/data_loaded.mat')\n",
    "    affine = np.ones((4,4))\n",
    "    \n",
    "    data, affine = reslice(F['data'], affine, (2,2,2), (2.5,2.5,2.5))\n",
    "    _, maskCut = median_otsu(data, vol_idx=range(10, 50),autocrop=False)\n",
    "\n",
    "    true_indices = np.argwhere(maskCut)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    for k,x in enumerate(os.listdir(WMDir)):\n",
    "        if Name in x:\n",
    "            print(Name)\n",
    "            WM, affine, img = load_nifti(WMDir+x, return_img=True)\n",
    "            WM, affine = reslice(WM, affine, (2,2,2), (2.5,2.5,2.5))\n",
    "            if(i<5):\n",
    "                WM_t = np.fliplr(np.swapaxes(WM,0,1))\n",
    "            else:\n",
    "                WM_t = np.fliplr(np.flipud(np.swapaxes(WM,0,1)))\n",
    "            WM_t  = WM_t[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "            WMs.append(WM_t)\n",
    "            \n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50),autocrop=True)\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    Masks.append(mask)\n",
    "    bvecs = (F['direction'].T/np.linalg.norm(F['direction'],axis=1)).T\n",
    "    bvecs[np.isnan(bvecs)] = 0\n",
    "    bvals = F['bval']\n",
    "    bvecs2000 = bvecs[bvals==2000]\n",
    "    bvecs4000 = bvecs[bvals==4000]\n",
    "\n",
    "    bvals2000 = np.array([0] + list(bvals[bvals==2000]))\n",
    "    bvecs2000 = np.vstack([[0,0,0],bvecs[bvals==2000]])\n",
    "\n",
    "    Dats.append(maskdata[:,:,:,:])\n",
    "    \n",
    "    gTabsF.append(gradient_table(bvals,bvecs))\n",
    "    if i == 0:\n",
    "        # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "        selected_indices = [0]\n",
    "        distance_matrix = squareform(pdist(bvecs2000))\n",
    "        # Iteratively select the point furthest from the current selection\n",
    "        for _ in range(6):  # We need 7 points in total, and one is already selected\n",
    "            remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "            \n",
    "            # Calculate the minimum distance to the selected points for each remaining point\n",
    "            min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "            \n",
    "            # Select the point with the maximum minimum distance\n",
    "            next_index = remaining_indices[np.argmax(min_distances)]\n",
    "            selected_indices.append(next_index)\n",
    "        \n",
    "        selected_indices7 = selected_indices\n",
    "        \n",
    "        # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "        selected_indices = [0]\n",
    "        distance_matrix = squareform(pdist(bvecs4000))\n",
    "        # Iteratively select the point furthest from the current selection\n",
    "        for _ in range(14):  # We need 7 points in total, and one is already selected\n",
    "            remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "            \n",
    "            # Calculate the minimum distance to the selected points for each remaining point\n",
    "            min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "            \n",
    "            # Select the point with the maximum minimum distance\n",
    "            next_index = remaining_indices[np.argmax(min_distances)]\n",
    "            selected_indices.append(next_index)\n",
    "        selected_indices7_2 = selected_indices\n",
    "        \n",
    "        selected_indices = [0]\n",
    "        distance_matrix = squareform(pdist(bvecs2000))\n",
    "        # Iteratively select the point furthest from the current selection\n",
    "        for _ in range(19):  # We need 7 points in total, and one is already selected\n",
    "            remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "            \n",
    "            # Calculate the minimum distance to the selected points for each remaining point\n",
    "            min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "            \n",
    "            # Select the point with the maximum minimum distance\n",
    "            next_index = remaining_indices[np.argmax(min_distances)]\n",
    "            selected_indices.append(next_index)\n",
    "        selected_indices20 = selected_indices\n",
    "        \n",
    "        # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "        selected_indices = [0]\n",
    "        distance_matrix = squareform(pdist(bvecs4000))\n",
    "        # Iteratively select the point furthest from the current selection\n",
    "        for _ in range(27):  # We need 7 points in total, and one is already selected\n",
    "            remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "            \n",
    "            # Calculate the minimum distance to the selected points for each remaining point\n",
    "            min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "            \n",
    "            # Select the point with the maximum minimum distance\n",
    "            next_index = remaining_indices[np.argmax(min_distances)]\n",
    "            selected_indices.append(next_index)\n",
    "        selected_indices20_2 = selected_indices\n",
    "    \n",
    "        Indxs7 = np.hstack([0,np.where(bvals==2000)[0][np.array(selected_indices7)[1:]-1],np.where(bvals==4000)[0][selected_indices7_2]])\n",
    "        Indxs20 = np.hstack([0,np.where(bvals==2000)[0][np.array(selected_indices20)[1:]-1],np.where(bvals==4000)[0][selected_indices20_2]])\n",
    "    gTabs7.append(gradient_table([0]+[2000]*6 + [4000]*15,np.vstack([bvecs2000[selected_indices7],bvecs4000[selected_indices7_2]])))\n",
    "    gTabs20.append(gradient_table([0]+[2000]*19 + [4000]*28,np.vstack([bvecs2000[selected_indices20],bvecs4000[selected_indices20_2]])))\n",
    "axial_middles = [32]*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc1841d-8c1b-4753-bbcb-c78183cd2ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DKIMultiMSFull_300k.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiMSFull_300k.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT,KT = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,30000) \n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([DT.shape[0]])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    A  = np.random.choice(8,DT.shape[0])\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabsF[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabsF[A[i]],S0[i],50)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = np.hstack([Obs,np.expand_dims(A, axis=-1)])\n",
    "        \n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs=100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiMSFull_300k.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiMSFull_300k.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n",
    "    import os\n",
    "    os.system(\"say 'DKI network done'\") # or '\\7'\n",
    "if os.path.exists(f\"{network_path}/DKIMultiMSMin_300k.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiMSMin_300k.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT,KT = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,30000)\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([DT.shape[0]])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    A  = np.random.choice(8,DT.shape[0])\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabsF[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabsF[A[i]],S0[i],50)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = np.hstack([Obs[:,Indxs7],np.expand_dims(A, axis=-1)])\n",
    "        \n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posteriorMin = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiMSMin_300k.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiMSMin_300k.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorMin, handle)\n",
    "    import os\n",
    "    os.system(\"say 'DKI network done'\") # or '\\7'\n",
    "if os.path.exists(f\"{network_path}/DKIMultiMSMid_300k.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiMSMid_300k.pickle\", \"rb\") as handle:\n",
    "        posteriorMid = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT,KT = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,30000)\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([DT.shape[0]])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    A  = np.random.choice(8,DT.shape[0])\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabsF[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabsF[A[i]],S0[i],50)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = np.hstack([Obs[:,Indxs20],np.expand_dims(A, axis=-1)])\n",
    "        \n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posteriorMid = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiMSMid_300k.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiMSMid_300k.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorMid, handle)\n",
    "    import os\n",
    "    os.system(\"say 'DKI network done'\") # or '\\7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b55df-db37-41b2-a244-10f12ee5fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullArr = []\n",
    "RKFullArr = []\n",
    "AKFullArr = []\n",
    "MKTFullArr = []\n",
    "KFAFullArr = []\n",
    "for kk in range(8):\n",
    "    Dat = Dats[kk]\n",
    "    ArrShape = Dat.shape[:2]\n",
    "    \n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(Dats[kk][:, :, axial_middles[kk], :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    def optimize_chunk(pixels):\n",
    "        results = []\n",
    "        for i, j in pixels:\n",
    "            posterior_samples_1 = posteriorFull.sample((500,), x=np.hstack([Dats[kk][i,j,axial_middles[kk], :],kk]),show_progress_bars=False)\n",
    "            results.append((i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])))\n",
    "        return results\n",
    "    \n",
    "    chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    "    )\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for chunk in results:\n",
    "        for i, j, x in chunk:\n",
    "            NoiseEst[i, j] = x\n",
    "        \n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):  \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])\n",
    "    \n",
    "    MK7  = np.zeros(ArrShape)\n",
    "    RK7  = np.zeros(ArrShape)\n",
    "    AK7  = np.zeros(ArrShape)\n",
    "    MKT7 = np.zeros(ArrShape)\n",
    "    KFA7 = np.zeros(ArrShape)\n",
    "    for i in tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK7[i,j] = Metrics[0]\n",
    "            AK7[i,j] = Metrics[1]\n",
    "            RK7[i,j] = Metrics[2]\n",
    "            MKT7[i,j] = Metrics[3]\n",
    "            KFA7[i,j] = Metrics[4]\n",
    "    MKFullArr.append(MK7)\n",
    "    RKFullArr.append(RK7)\n",
    "    AKFullArr.append(AK7)\n",
    "    MKTFullArr.append(MKT7)\n",
    "    KFAFullArr.append(KFA7)\n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(RK7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(AK7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA7.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e395d5-4353-4c12-9e3f-5d7a5d59d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKMidArr = []\n",
    "RKMidArr = []\n",
    "AKMidArr = []\n",
    "MKTMidArr = []\n",
    "KFAMidArr = []\n",
    "for kk in range(8):\n",
    "    Dat = Dats[kk]\n",
    "    ArrShape = Dat.shape[:2]\n",
    "    \n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(Dats[kk][:, :, axial_middles[kk], :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    Arr = Dats[kk][:,:,axial_middles[kk], Indxs20]\n",
    "    def optimize_chunk(pixels):\n",
    "        results = []\n",
    "        for i, j in pixels:\n",
    "            posterior_samples_1 = posteriorMid.sample((500,), x=np.hstack([Arr[i,j],kk]),show_progress_bars=False)\n",
    "            results.append((i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])))\n",
    "        return results\n",
    "    \n",
    "    chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    "    )\n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for chunk in results:\n",
    "        for i, j, x in chunk:\n",
    "            NoiseEst[i, j] = x\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):  \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])\n",
    "    \n",
    "    MK7  = np.zeros(ArrShape)\n",
    "    RK7  = np.zeros(ArrShape)\n",
    "    AK7  = np.zeros(ArrShape)\n",
    "    MKT7 = np.zeros(ArrShape)\n",
    "    KFA7 = np.zeros(ArrShape)\n",
    "    for i in tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK7[i,j] = Metrics[0]\n",
    "            AK7[i,j] = Metrics[1]\n",
    "            RK7[i,j] = Metrics[2]\n",
    "            MKT7[i,j] = Metrics[3]\n",
    "            KFA7[i,j] = Metrics[4]\n",
    "    MKMidArr.append(MK7)\n",
    "    RKMidArr.append(RK7)\n",
    "    AKMidArr.append(AK7)\n",
    "    MKTMidArr.append(MKT7)\n",
    "    KFAMidArr.append(KFA7)\n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(RK7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(AK7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA7.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066cf4a7-cad6-4418-b754-cdfb10d97b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKMinArr = []\n",
    "RKMinArr = []\n",
    "AKMinArr = []\n",
    "MKTMinArr = []\n",
    "KFAMinArr = []\n",
    "for kk in range(8):\n",
    "    Dat = Dats[kk]\n",
    "    ArrShape = Dat.shape[:2]\n",
    "    \n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(Dats[kk][:, :, axial_middles[kk], :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    Arr = Dats[kk][:,:,axial_middles[kk], Indxs7]\n",
    "    def optimize_chunk(pixels):\n",
    "        results = []\n",
    "        for i, j in pixels:\n",
    "            posterior_samples_1 = posteriorMin.sample((500,), x=np.hstack([Arr[i,j],kk]),show_progress_bars=False)\n",
    "            results.append((i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])))\n",
    "        return results\n",
    "    \n",
    "    chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    "    )\n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for chunk in results:\n",
    "        for i, j, x in chunk:\n",
    "            NoiseEst[i, j] = x  \n",
    "        \n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):  \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])\n",
    "    \n",
    "    MK7  = np.zeros(ArrShape)\n",
    "    RK7  = np.zeros(ArrShape)\n",
    "    AK7  = np.zeros(ArrShape)\n",
    "    MKT7 = np.zeros(ArrShape)\n",
    "    KFA7 = np.zeros(ArrShape)\n",
    "    for i in tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK7[i,j] = Metrics[0]\n",
    "            AK7[i,j] = Metrics[1]\n",
    "            RK7[i,j] = Metrics[2]\n",
    "            MKT7[i,j] = Metrics[3]\n",
    "            KFA7[i,j] = Metrics[4]\n",
    "    MKMinArr.append(MK7)\n",
    "    RKMinArr.append(RK7)\n",
    "    AKMinArr.append(AK7)\n",
    "    MKTMinArr.append(MKT7)\n",
    "    KFAMinArr.append(KFA7)\n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(RK7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(AK7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA7.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15387696-4e8d-4be3-b2fd-1c4c3f82b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullNLArr = []\n",
    "RKFullNLArr = []\n",
    "AKFullNLArr = []\n",
    "MKTFullNLArr = []\n",
    "KFAFullNLArr = []\n",
    "for kk in range(8):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabsF[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(Dats[kk][:,:,axial_middles[kk],:])\n",
    "    ArrShape = Dats[kk][:,:,axial_middles[kk],0].shape\n",
    "    MK_NLFull  = np.zeros(ArrShape)\n",
    "    AK_NLFull  = np.zeros(ArrShape)\n",
    "    RK_NLFull = np.zeros(ArrShape)\n",
    "    MKT_NLFull = np.zeros(ArrShape)\n",
    "    KFA_NLFull = np.zeros(ArrShape)\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    for i in tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            if(np.sum(Dats[kk][i,j,axial_middles[kk]]) == 0):\n",
    "                pass\n",
    "            else: \n",
    "                Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "                MK_NLFull[i,j] = Metrics[0]\n",
    "                AK_NLFull[i,j] = Metrics[1]\n",
    "                RK_NLFull[i,j] = Metrics[2]\n",
    "                MKT_NLFull[i,j] = Metrics[3]\n",
    "                KFA_NLFull[i,j] = Metrics[4]\n",
    "    MKFullNLArr.append(MK_NLFull)\n",
    "    RKFullNLArr.append(RK_NLFull)\n",
    "    AKFullNLArr.append(AK_NLFull)\n",
    "    MKTFullNLArr.append(MKT_NLFull)\n",
    "    KFAFullNLArr.append(KFA_NLFull)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(RK_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(AK_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0eba93-0dbe-44b2-9cba-f98711af6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK20NLArr = []\n",
    "RK20NLArr = []\n",
    "AK20NLArr = []\n",
    "MKT20NLArr = []\n",
    "KFA20NLArr = []\n",
    "for kk in range(8):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabs20[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(Dats[kk][:,:,axial_middles[kk],Indxs20])\n",
    "    ArrShape = Dats[kk][:,:,axial_middles[kk],0].shape\n",
    "    MK_NLFull  = np.zeros(ArrShape)\n",
    "    AK_NLFull  = np.zeros(ArrShape)\n",
    "    RK_NLFull = np.zeros(ArrShape)\n",
    "    MKT_NLFull = np.zeros(ArrShape)\n",
    "    KFA_NLFull = np.zeros(ArrShape)\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    for i in tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            if(np.sum(Dats[kk][i,j,axial_middles[kk]]) == 0):\n",
    "                pass\n",
    "            else: \n",
    "                Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "                MK_NLFull[i,j] = Metrics[0]\n",
    "                AK_NLFull[i,j] = Metrics[1]\n",
    "                RK_NLFull[i,j] = Metrics[2]\n",
    "                MKT_NLFull[i,j] = Metrics[3]\n",
    "                KFA_NLFull[i,j] = Metrics[4]\n",
    "    MK20NLArr.append(MK_NLFull)\n",
    "    RK20NLArr.append(RK_NLFull)\n",
    "    AK20NLArr.append(AK_NLFull)\n",
    "    MKT20NLArr.append(MKT_NLFull)\n",
    "    KFA20NLArr.append(KFA_NLFull)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(RK_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(AK_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b6b2d-2a6e-4dbc-9064-51564a435dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK7NLArr = []\n",
    "RK7NLArr = []\n",
    "AK7NLArr = []\n",
    "MKT7NLArr = []\n",
    "KFA7NLArr = []\n",
    "for kk in range(8):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabs7[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(Dats[kk][:,:,axial_middles[kk],Indxs7])\n",
    "    ArrShape = Dats[kk][:,:,axial_middles[kk],0].shape\n",
    "    MK_NLFull  = np.zeros(ArrShape)\n",
    "    AK_NLFull  = np.zeros(ArrShape)\n",
    "    RK_NLFull = np.zeros(ArrShape)\n",
    "    MKT_NLFull = np.zeros(ArrShape)\n",
    "    KFA_NLFull = np.zeros(ArrShape)\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    for i in tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            if(np.sum(Dats[kk][i,j,axial_middles[kk]]) == 0):\n",
    "                pass\n",
    "            else: \n",
    "                Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "                MK_NLFull[i,j] = Metrics[0]\n",
    "                AK_NLFull[i,j] = Metrics[1]\n",
    "                RK_NLFull[i,j] = Metrics[2]\n",
    "                MKT_NLFull[i,j] = Metrics[3]\n",
    "                KFA_NLFull[i,j] = Metrics[4]\n",
    "    MK7NLArr.append(MK_NLFull)\n",
    "    RK7NLArr.append(RK_NLFull)\n",
    "    AK7NLArr.append(AK_NLFull)\n",
    "    MKT7NLArr.append(MKT_NLFull)\n",
    "    KFA7NLArr.append(KFA_NLFull)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(AK_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(RK_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a795244-4321-4c37-8772-fe00075c314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccM7_MK = []\n",
    "AccM20_MK = []\n",
    "AccMFulls_MK = []\n",
    "\n",
    "AccM7NL_MK = []\n",
    "AccM20NL_MK = []\n",
    "\n",
    "SSIM7_MK = []\n",
    "SSIM20_MK = []\n",
    "SSIMFulls_MK = []\n",
    "\n",
    "SSIM7NL_MK = []\n",
    "SSIM20NL_MK = []\n",
    "for i in range(8):\n",
    "    M7 = MKMinArr[i]\n",
    "    MF = MKFullArr[i]\n",
    "    Ma = Masks[i][:,:,axial_middles[i]]\n",
    "    AccM7_MK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = MKMidArr[i]\n",
    "    MF = MKFullArr[i]\n",
    "    Ma = Masks[i][:,:,axial_middles[i]]\n",
    "    AccM20_MK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = MKFullArr[i]\n",
    "    MF = MKFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,axial_middles[i]]\n",
    "    AccMFulls_MK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = MK7NLArr[i]\n",
    "    MF = MKFullNLArr[i]\n",
    "\n",
    "    AccM7NL_MK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = MK20NLArr[i]\n",
    "    MF = MKFullNLArr[i]\n",
    "\n",
    "    AccM20NL_MK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    NS1 = MKMinArr[i]\n",
    "    NS2 = MKFullArr[i]\n",
    "\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM7_MK.append(result)\n",
    "\n",
    "    NS1 = MKMidArr[i]\n",
    "    NS2 = MKFullArr[i]\n",
    "\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM20_MK.append(result)\n",
    "    \n",
    "    NS1 = MKFullArr[i]\n",
    "    NS2 = MKFullNLArr[i]\n",
    "\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIMFulls_MK.append(result)\n",
    "\n",
    "    NS1 = MK7NLArr[i]\n",
    "    NS2 = MKFullNLArr[i]\n",
    "\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM7NL_MK.append(result)\n",
    "\n",
    "    NS1 = MK20NLArr[i]\n",
    "    NS2 = MKFullNLArr[i]\n",
    "\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM20NL_MK.append(result)\n",
    "\n",
    "\n",
    "Prec7_SBI_MK = []\n",
    "Prec20_SBI_MK = []\n",
    "PrecFull_SBI_MK = []\n",
    "\n",
    "Prec7_NLLS_MK = []\n",
    "Prec20_NLLS_MK = []\n",
    "PrecFull_NLLS_MK = []\n",
    "for i in range(8):\n",
    "    Prec7_SBI_MK.append(np.std(MKMinArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "    Prec20_SBI_MK.append(np.std(MKMidArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "    PrecFull_SBI_MK.append(np.std(MKFullArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "\n",
    "    Prec7_NLLS_MK.append(np.std(MK7NLArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "    Prec20_NLLS_MK.append(np.std(MK20NLArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "    PrecFull_NLLS_MK.append(np.std(MKFullNLArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810ce81-62bc-4c81-9df3-5f9faa75ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccM7_AK = []\n",
    "AccM20_AK = []\n",
    "AccMFulls_AK = []\n",
    "\n",
    "AccM7NL_AK = []\n",
    "AccM20NL_AK = []\n",
    "\n",
    "SSIM7_AK = []\n",
    "SSIM20_AK = []\n",
    "SSIMFulls_AK = []\n",
    "\n",
    "SSIM7NL_AK = []\n",
    "SSIM20NL_AK = []\n",
    "for i in range(8):\n",
    "    M7 = AKMinArr[i]\n",
    "    MF = AKFullArr[i]\n",
    "    Ma = Masks[i][:,:,axial_middles[i]]\n",
    "    AccM7_AK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = AKMidArr[i]\n",
    "    MF = AKFullArr[i]\n",
    "    AccM20_AK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "\n",
    "    M7 = AKFullArr[i]\n",
    "    MF = AKFullNLArr[i]\n",
    "    AccMFulls_AK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = AK7NLArr[i]\n",
    "    MF = AKFullNLArr[i]\n",
    "    AccM7NL_AK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = AK20NLArr[i]\n",
    "    MF = AKFullNLArr[i]\n",
    "\n",
    "    AccM20NL_AK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    NS1 = np.clip(AKMinArr[i],0,1)\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.clip(AKFullArr[i],0,1)\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM7_AK.append(result)\n",
    "\n",
    "    NS1 = np.clip(AKMidArr[i],0,1)\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.clip(AKFullArr[i],0,1)\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM20_AK.append(result)\n",
    "    \n",
    "    NS1 = AKFullArr[i]\n",
    "    NS2 = AKFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIMFulls_AK.append(result)\n",
    "\n",
    "    NS1 = np.clip(AK7NLArr[i],0,1)\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.clip(AKFullNLArr[i],0,1)\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = Masks[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM7NL_AK.append(result)\n",
    "\n",
    "    NS1 = np.clip(AK20NLArr[i],0,1)\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.clip(AKFullNLArr[i],0,1)\n",
    "    Ma = Masks[i][:,:,axial_middles[i]]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM20NL_AK.append(result)\n",
    "\n",
    "Prec7_SBI_AK = []\n",
    "Prec20_SBI_AK = []\n",
    "PrecFull_SBI_AK = []\n",
    "\n",
    "Prec7_NLLS_AK = []\n",
    "Prec20_NLLS_AK = []\n",
    "PrecFull_NLLS_AK = []\n",
    "for i in range(8):\n",
    "    Prec7_SBI_AK.append(np.std(AKMinArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "    Prec20_SBI_AK.append(np.std(AKMidArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "    PrecFull_SBI_AK.append(np.std(AKFullArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "\n",
    "    Prec7_NLLS_AK.append(np.std(AK7NLArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "    Prec20_NLLS_AK.append(np.std(AK20NLArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "    PrecFull_NLLS_AK.append(np.std(AKFullNLArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae8795-8465-41cf-b41e-d5ba947b35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccM7_RK = []\n",
    "AccM20_RK = []\n",
    "AccMFulls_RK = []\n",
    "\n",
    "AccM7NL_RK = []\n",
    "AccM20NL_RK = []\n",
    "\n",
    "SSIM7_RK = []\n",
    "SSIM20_RK = []\n",
    "SSIMFulls_RK = []\n",
    "\n",
    "SSIM7NL_RK = []\n",
    "SSIM20NL_RK = []\n",
    "for i in range(8):\n",
    "    M7 = RKMinArr[i]\n",
    "    MF = RKFullArr[i]\n",
    "    Ma = Masks[i][:,:,axial_middles[i]]\n",
    "    AccM7_RK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = RKMidArr[i]\n",
    "    MF = RKFullArr[i]\n",
    "    Ma = Masks[i][:,:,axial_middles[i]]\n",
    "    AccM20_RK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = RKFullArr[i]\n",
    "    MF = RKFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,axial_middles[i]]\n",
    "    AccMFulls_RK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = RK7NLArr[i]\n",
    "    MF = RKFullNLArr[i]\n",
    "\n",
    "    AccM7NL_RK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    M7 = RK20NLArr[i]\n",
    "    MF = RKFullNLArr[i]\n",
    "\n",
    "    AccM20NL_RK.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "    NS1 = RKMinArr[i]\n",
    "    NS2 = RKFullArr[i]\n",
    "\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM7_RK.append(result)\n",
    "\n",
    "    NS1 = RKMidArr[i]\n",
    "    NS2 = RKFullArr[i]\n",
    "\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM20_RK.append(result)\n",
    "    \n",
    "    NS1 = RKFullArr[i]\n",
    "    NS2 = RKFullNLArr[i]\n",
    "\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIMFulls_RK.append(result)\n",
    "\n",
    "    NS1 = RK7NLArr[i]\n",
    "    NS2 = RKFullNLArr[i]\n",
    "\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM7NL_RK.append(result)\n",
    "\n",
    "    NS1 = RK20NLArr[i]\n",
    "    NS2 = RKFullNLArr[i]\n",
    "\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7)\n",
    "    SSIM20NL_RK.append(result)\n",
    "\n",
    "\n",
    "Prec7_SBI_RK = []\n",
    "Prec20_SBI_RK = []\n",
    "PrecFull_SBI_RK = []\n",
    "\n",
    "Prec7_NLLS_RK = []\n",
    "Prec20_NLLS_RK = []\n",
    "PrecFull_NLLS_RK = []\n",
    "for i in range(8):\n",
    "    Prec7_SBI_RK.append(np.std(RKMinArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "    Prec20_SBI_RK.append(np.std(RKMidArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "    PrecFull_SBI_RK.append(np.std(RKFullArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "\n",
    "    Prec7_NLLS_RK.append(np.std(RK7NLArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "    Prec20_NLLS_RK.append(np.std(RK20NLArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n",
    "    PrecFull_NLLS_RK.append(np.std(RKFullNLArr[i][WMs[i][:,:,axial_middles[i]]>0.8]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8603ce5-41f7-4971-aa56-bda8f7360e6a",
   "metadata": {},
   "source": [
    "### f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222432e4-dc95-4096-92a9-b58c04169bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, ax2 = plt.subplots(1,1,figsize=(3.2,4.8))\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "\n",
    "\n",
    "y_data = np.array(AccM7NL_MK)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "\n",
    "plt.sca(ax2)\n",
    "\n",
    "y_data = np.array(AccMFulls_MK)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20_MK)\n",
    "g_pos = np.array([1.55])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM7_MK)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20NL_MK)\n",
    "g_pos = np.array([2.65])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "plt.xticks([0.8,1.55,1.95,2.65,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "#ax1.set_ylim(0.4, 1)\n",
    "#ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax1.yaxis.set_ticks([0.4,0.7,1])\n",
    "\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "#ax2.set_ylim(0.0,0.3)\n",
    "#ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "#ax2.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "\n",
    "# Show plot\n",
    "\n",
    "leg_patch2 = mpatches.Patch(color='mediumturquoise', label='SBI')\n",
    "leg_patch3 = mpatches.Patch(color='sandybrown', label='NLLS')\n",
    "leg_patch5 = mpatches.Patch(color='gray', label='Full \\nComp.')\n",
    "\n",
    "ax2.set_xlim(0.5,3.5)\n",
    "if Save: plt.savefig(FigLoc+'DKI_Acc_MK_MS.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2ccab-6522-4820-a550-79c829d34669",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SSIMFulls_MK)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20_MK)\n",
    "g_pos = np.array([1.5])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM7_MK)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20NL_MK)\n",
    "g_pos = np.array([2.6])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_MK)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_MK)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.xticks([0.8,1.5,1.95,2.6,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "ax.legend(\n",
    "    handles=[leg_patch3],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=1,\n",
    "fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,bbox_to_anchor= (-0.1,-0.05))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_MK_MS.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8f3e6-9485-4281-978a-185b08ba6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(3.2,4.8))\n",
    "y_data = np.array(PrecFull_SBI_MK)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_SBI_MK)\n",
    "g_pos = np.array([1])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_MK)\n",
    "g_pos = np.array([1.35])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "y_data = np.array(PrecFull_NLLS_MK)\n",
    "g_pos = np.array([1.8])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_NLLS_MK)\n",
    "g_pos = np.array([2.15])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_MK)\n",
    "g_pos = np.array([2.5])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "x = np.arange(1.7,2.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_MK)[~np.isnan(PrecFull_NLLS_MK)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_MK)[~np.isnan(PrecFull_NLLS_MK)], 77)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.5,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_MK)[~np.isnan(PrecFull_SBI_MK)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_MK)[~np.isnan(PrecFull_SBI_MK)], 77)\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "plt.xticks([0.65,1,1.35,1.8,2.15,2.5],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "ax.set_xlim([0.4,2.7])\n",
    "if Save: plt.savefig(FigLoc+'DKI_RK_Prec_MS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c316beb-8216-4e9f-9a39-3421e02be7e4",
   "metadata": {},
   "source": [
    "### g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777be3b9-bda0-4f50-b053-373fa0738ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, ax2 = plt.subplots(1,1,figsize=(3.2,4.8))\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "\n",
    "\n",
    "y_data = np.array(AccM7NL_AK)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "\n",
    "plt.sca(ax2)\n",
    "\n",
    "y_data = np.array(AccMFulls_AK)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20_AK)\n",
    "g_pos = np.array([1.55])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM7_AK)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20NL_AK)\n",
    "g_pos = np.array([2.65])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "plt.xticks([0.8,1.55,1.95,2.65,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "#ax1.set_ylim(0.4, 1)\n",
    "#ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax1.yaxis.set_ticks([0.4,0.7,1])\n",
    "\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "#ax2.set_ylim(0.0,0.3)\n",
    "#ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "#ax2.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "\n",
    "# Show plot\n",
    "\n",
    "leg_patch2 = mpatches.Patch(color='mediumturquoise', label='SBI')\n",
    "leg_patch3 = mpatches.Patch(color='sandybrown', label='NLLS')\n",
    "leg_patch5 = mpatches.Patch(color='gray', label='Full \\nComp.')\n",
    "\n",
    "ax2.set_xlim(0.5,3.5)\n",
    "if Save: plt.savefig(FigLoc+'DKI_Acc_AK_MS.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5080aa1-001d-4dda-8811-67de0cd3a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SSIMFulls_AK)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20_AK)\n",
    "g_pos = np.array([1.5])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM7_AK)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20NL_AK)\n",
    "g_pos = np.array([2.6])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_AK)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_AK)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.xticks([0.8,1.5,1.95,2.6,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "ax.legend(\n",
    "    handles=[leg_patch3],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=1,\n",
    "fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,bbox_to_anchor= (-0.1,-0.05))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKI_SSIM_AK_MS.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971d038-a56a-43ef-930e-1402fe1210f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(3.2,4.8))\n",
    "y_data = np.array(PrecFull_SBI_AK)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_SBI_AK)\n",
    "g_pos = np.array([1])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_AK)\n",
    "g_pos = np.array([1.35])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "y_data = np.array(PrecFull_NLLS_AK)\n",
    "g_pos = np.array([1.8])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_NLLS_AK)\n",
    "g_pos = np.array([2.15])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_AK)\n",
    "g_pos = np.array([2.5])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "x = np.arange(1.7,2.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_AK)[~np.isnan(PrecFull_NLLS_AK)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_AK)[~np.isnan(PrecFull_NLLS_AK)], 77)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.5,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_AK)[~np.isnan(PrecFull_SBI_AK)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_AK)[~np.isnan(PrecFull_SBI_AK)], 77)\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "plt.xticks([0.65,1,1.35,1.8,2.15,2.5],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "ax.set_xlim([0.4,2.7])\n",
    "if Save: plt.savefig(FigLoc+'DKI_AK_Prec_MS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8af3b0-c4c6-42cc-b163-1f9ff3247fa2",
   "metadata": {},
   "source": [
    "### h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e73108-1598-4c75-a71f-4bbdf01d2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, ax2 = plt.subplots(1,1,figsize=(3.2,4.8))\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "\n",
    "\n",
    "y_data = np.array(AccM7NL_RK)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "\n",
    "plt.sca(ax2)\n",
    "\n",
    "y_data = np.array(AccMFulls_RK)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20_RK)\n",
    "g_pos = np.array([1.55])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM7_RK)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20NL_RK)\n",
    "g_pos = np.array([2.65])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax2,widths=0.2,scatter=True)\n",
    "\n",
    "plt.xticks([0.8,1.55,1.95,2.65,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "#ax1.set_ylim(0.4, 1)\n",
    "#ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax1.yaxis.set_ticks([0.4,0.7,1])\n",
    "\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "#ax2.set_ylim(0.0,0.3)\n",
    "#ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "#ax2.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "\n",
    "# Show plot\n",
    "\n",
    "leg_patch2 = mpatches.Patch(color='mediumturquoise', label='SBI')\n",
    "leg_patch3 = mpatches.Patch(color='sandybrown', label='NLLS')\n",
    "leg_patch5 = mpatches.Patch(color='gray', label='Full \\nComp.')\n",
    "\n",
    "ax2.set_xlim(0.5,3.5)\n",
    "if Save: plt.savefig(FigLoc+'DKI_Acc_RK_MS.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee94b8-43af-45d4-9c2b-7816885c2d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SSIMFulls_RK)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20_RK)\n",
    "g_pos = np.array([1.5])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM7_RK)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20NL_RK)\n",
    "g_pos = np.array([2.6])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_RK)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL_RK)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.xticks([0.8,1.5,1.95,2.6,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "ax.legend(\n",
    "    handles=[leg_patch3],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=1,\n",
    "fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,bbox_to_anchor= (-0.1,-0.05))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_RK_MS.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ced5a0-d947-4167-926d-e13ea6ffaed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(3.2,4.8))\n",
    "y_data = np.array(PrecFull_SBI_RK)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_SBI_RK)\n",
    "g_pos = np.array([1])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_RK)\n",
    "g_pos = np.array([1.35])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "y_data = np.array(PrecFull_NLLS_RK)\n",
    "g_pos = np.array([1.8])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_NLLS_RK)\n",
    "g_pos = np.array([2.15])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_RK)\n",
    "g_pos = np.array([2.5])\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "x = np.arange(1.7,2.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_RK)[~np.isnan(PrecFull_NLLS_RK)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_RK)[~np.isnan(PrecFull_NLLS_RK)], 77)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.5,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_RK)[~np.isnan(PrecFull_SBI_RK)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_RK)[~np.isnan(PrecFull_SBI_RK)], 77)\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "plt.xticks([0.65,1,1.35,1.8,2.15,2.5],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "ax.set_xlim([0.4,2.7])\n",
    "if Save: plt.savefig(FigLoc+'DKI_RK_Prec_MS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c1757-5c17-4707-9513-12e18450d72b",
   "metadata": {},
   "source": [
    "# Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843af4bd-2fb6-4c31-9a6e-93c71744c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Par_frac(i,j,Mat):\n",
    "    MD = np.linalg.eigh(vals_to_mat(Mat[i,j]))[0].mean()\n",
    "\n",
    "    FA = FracAni(np.linalg.eigh(vals_to_mat(Mat[i,j]))[0],MD)\n",
    "    return i, j, [FA,MD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e186e1a-fe7d-49a3-be4c-c4d38b09c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta = [0.017, 0.035, 0.061]             # ms\n",
    "delta = 0.007           # ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd87094-736e-4157-aa8e-ae02b6c51955",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "n_pts = 90\n",
    "theta = np.pi * np.random.random(n_pts)\n",
    "phi = 2 * np.pi * np.random.random(n_pts)\n",
    "hsph_initial = HemiSphere(theta=theta, phi=phi)\n",
    "hsph_updated, potential = disperse_charges(hsph_initial, 5000)\n",
    "vertices = hsph_updated.vertices\n",
    "values = np.ones(31)\n",
    "bvecs = np.vstack((vertices))\n",
    "bvecs = np.insert(bvecs, 0, np.array([0, 0, 0]), axis=0)\n",
    "bvals = np.hstack((0,[2000] * 30,[4000]*60))\n",
    "bvecs = np.vstack([bvecs,bvecs,bvecs])\n",
    "bvals = np.hstack([bvals,bvals,bvals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30919ad2-df3f-455e-b519-099744893b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs2000 = bvecs[:91][bvals[:91]==2000]\n",
    "distance_matrix = squareform(pdist(bvecs2000))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaini'ng point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs2000_selected = bvecs[:91][bvals[:91]==2000][selected_indices]\n",
    "true_indices = []\n",
    "for b in bvecs2000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs4000 = bvecs[:91][bvals[:91]==4000]\n",
    "distance_matrix = squareform(pdist(bvecs4000))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs4000_selected = bvecs[:91][bvals[:91]==4000][selected_indices]\n",
    "for b in bvecs4000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "MinIdices = np.array(true_indices)\n",
    "DevilIndices = np.hstack([MinIdices,MinIdices+91,MinIdices+182])\n",
    "DevilIndices = np.hstack([0,DevilIndices])\n",
    "bvecs_Dev = bvecs[DevilIndices]\n",
    "bvals_Dev = bvals[DevilIndices]\n",
    "\n",
    "bve_split = [bvecs[:(n_pts+1)],bvecs[(n_pts+1):2*(n_pts+1)],bvecs[2*(n_pts+1):]]\n",
    "bva_split = [bvals[:(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],bvals[2*(n_pts+1):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d70f4-6e40-46a0-98b6-05f6af8904f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "TestSamps = 20\n",
    "\n",
    "# Directions\n",
    "x1  = np.random.randn(TestSamps)\n",
    "y1  = np.random.randn(TestSamps)\n",
    "z1  =  np.random.randn(TestSamps)\n",
    "V = np.vstack([x1,y1,z1])\n",
    "V = (V/np.linalg.norm(V,axis=0)).T\n",
    "Angs = np.array([SpherAng(v) for v in V])\n",
    "\n",
    "#Diffusion of restricted\n",
    "Dpar  = np.random.rand(TestSamps)*5e-3\n",
    "Dperp = np.random.rand(TestSamps)*5e-3\n",
    "\n",
    "#Diffusion of hindered\n",
    "Params_abc =  np.random.rand(TestSamps,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(TestSamps,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "\n",
    "#Fraction of hindered\n",
    "frac  = np.random.rand(TestSamps)\n",
    "\n",
    "mean = np.random.rand(TestSamps)*0.005+1e-4\n",
    "sig2 = np.random.rand(TestSamps) * (4e-7 - 9e-8) + 9e-8\n",
    "\n",
    "S0Rand =np.ones(TestSamps)\n",
    "\n",
    "TestParams = np.column_stack([Angs,Dpar,Dperp,DHind,frac,mean])\n",
    "\n",
    "TestSig = []\n",
    "NoisyTestSig = []\n",
    "for i in tqdm(range(TestSamps)):\n",
    "    v = np.array([Angs[i]])\n",
    "    dpar = Dpar[i]\n",
    "    dperp = Dperp[i]\n",
    "    \n",
    "    dh   = DHind[i]\n",
    "    f    = [frac[i],1-frac[i]]\n",
    "\n",
    "    a = mean[i]\n",
    "    s = sig2[i]\n",
    "    alpha     = a * a / s\n",
    "    scale = s / a\n",
    "    rv = stats.gamma(a=alpha,scale=scale)\n",
    "    \n",
    "    R = np.linspace(0.0001,0.005, 30)\n",
    "    weights = rv.pdf(R)\n",
    "    weights = weights/np.sum(weights)\n",
    "    s0 = 1\n",
    "\n",
    "    TestSig1 = CombSignal_poisson(bvecs[:(n_pts+1)],bvals[:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "    TestSig2 = CombSignal_poisson(bvecs[(n_pts+1):2*(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "    TestSig3 = CombSignal_poisson(bvecs[2*(n_pts+1):],bvals[2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "    TestSig.append(np.hstack([TestSig1,TestSig2,TestSig3]))\n",
    "    Noisy = []\n",
    "    for Noise in [2,10,20,30]:\n",
    "        Noisy.append(AddNoise(TestSig[-1],s0,Noise))\n",
    "    NoisyTestSig.append(Noisy)\n",
    "NoisyTestSig = np.array(NoisyTestSig)\n",
    "NoisyTestSig = np.swapaxes(NoisyTestSig,0,1)\n",
    "TestSig = np.array(TestSig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8f3eb-77b9-4e3f-a084-6ee782e93f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/Full_Sim_50_100k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/Full_Sim_50_100k_poisson.pickle\", \"rb\") as handle:\n",
    "        posterior = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(10)\n",
    "    NumSamps = 100000\n",
    "    \n",
    "    # Directions\n",
    "    x1  = np.random.randn(NumSamps)\n",
    "    y1  = np.random.randn(NumSamps)\n",
    "    z1  =  np.random.randn(NumSamps)\n",
    "    VS = np.vstack([x1,y1,z1])\n",
    "    VS = (VS/np.linalg.norm(VS,axis=0)).T\n",
    "    AngsS = np.array([SpherAng(v) for v in VS])\n",
    "    \n",
    "    #Diffusion of restricted\n",
    "    DparS  = np.random.rand(NumSamps)*5e-3\n",
    "    DperpS = np.random.rand(NumSamps)*5e-3\n",
    "    \n",
    "    #Diffusion of hindered\n",
    "    Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "    Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "    Params = np.hstack([Params_abc,Params_rest])\n",
    "    DHindS = np.array([ComputeDTI(p) for p in Params])\n",
    "    DHindS = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHindS])\n",
    "    \n",
    "    meanS = np.random.rand(NumSamps)*0.005+1e-4\n",
    "    sig2S = np.random.rand(NumSamps) * (4e-7 - 9e-8) + 9e-8\n",
    "    \n",
    "    #Fraction of hindered\n",
    "    fracS  = np.random.rand(NumSamps)\n",
    "    TrainParams = np.column_stack([AngsS,DparS,DperpS,DHindS,fracS,meanS])\n",
    "\n",
    "\n",
    "    TrainSigS = []\n",
    "    NoisyTrainSigS = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([AngsS[i]])\n",
    "        dpar = DparS[i]\n",
    "        dperp = DperpS[i]\n",
    "        \n",
    "        dh   = DHindS[i]\n",
    "        f    = [fracS[i],1-fracS[i]]\n",
    "    \n",
    "        a = meanS[i]\n",
    "        s = sig2S[i]\n",
    "        s0 = 1\n",
    "        \n",
    "        Noise = 50\n",
    "        \n",
    "        TrainSig1 = CombSignal_poisson(bvecs[:(n_pts+1)],bvals[:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(bvecs[(n_pts+1):2*(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(bvecs[2*(n_pts+1):],bvals[2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSigS.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        \n",
    "        NoisyTrainSigS.append(AddNoise(TrainSigS[-1],s0,Noise))\n",
    "    NoisyTrainSigS = np.array(NoisyTrainSigS)\n",
    "\n",
    "    Obs = torch.tensor(NoisyTrainSigS).float()\n",
    "    Par = torch.tensor(TrainParams).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "\n",
    "    with open(f\"{network_path}/Full_Sim_50_100k_poisson\", \"wb\") as handle:\n",
    "        pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc256fb-28d3-41e7-bbc2-93bc398a3c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/Dev_Sim_50_100k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/Dev_Sim_50_100k_poisson.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "\n",
    "    np.random.seed(10)\n",
    "    torch.manual_seed(10)\n",
    "    TrainSigS = []\n",
    "    NoisyTrainSigS = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([AngsS[i]])\n",
    "        dpar = DparS[i]\n",
    "        dperp = DperpS[i]\n",
    "        \n",
    "        dh   = DHindS[i]\n",
    "        f    = [fracS[i],1-fracS[i]]\n",
    "    \n",
    "        a = meanS[i]\n",
    "        s0 = 1\n",
    "        \n",
    "        Noise = 50\n",
    "        \n",
    "        TrainSig1 = CombSignal_poisson(bvecs_Dev[:7],bvals_Dev[:7],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(bvecs_Dev[7:13],bvals_Dev[7:13],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(bvecs_Dev[13:],bvals_Dev[13:],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSigS.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        \n",
    "        NoisyTrainSigS.append(AddNoise(TrainSigS[-1],s0,Noise))\n",
    "    NoisyTrainSigS = np.array(NoisyTrainSigS)\n",
    "\n",
    "\n",
    "    Obs = torch.tensor(NoisyTrainSigS).float()\n",
    "    Par = torch.tensor(TrainParams).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posteriorMin = inference.build_posterior(density_estimator)\n",
    "\n",
    "    with open(f\"{network_path}/Dev_Sim_50_100k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posteriorMin, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a12664-f898-46a9-bf38-d1c76f0336e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Errors(TrueSig,TrueParams,GuessParams,Delta,bvecs,bvals):\n",
    "\n",
    "    Res = np.linalg.norm(residuals(GuessParams,TrueSig,bvecs,bvals,Delta))\n",
    "    alpha_err = np.abs(GuessParams[11]-TrueParams[11])\n",
    "\n",
    "    angle_err1 =  np.abs(GuessParams[0]-TrueParams[0])\n",
    "    angle_err2 =  np.abs(GuessParams[1]-TrueParams[1])\n",
    "\n",
    "    Dpar_err  = np.abs(TrueParams[2]-GuessParams[2])\n",
    "    Dperp_err  = np.abs(TrueParams[3]-GuessParams[3])\n",
    "\n",
    "    MD_guess = np.linalg.eigh(vals_to_mat(GuessParams[4:10]))[0].mean()\n",
    "    MD_true = np.linalg.eigh(vals_to_mat(TrueParams[4:10]))[0].mean()\n",
    "\n",
    "    FA_guess = FracAni(np.linalg.eigh(vals_to_mat(GuessParams[4:10]))[0],MD_guess)\n",
    "    FA_true  = FracAni(np.linalg.eigh(vals_to_mat(TrueParams[4:10]))[0],MD_true)\n",
    "\n",
    "    MD_err = np.abs(MD_guess-MD_true)\n",
    "    FA_err = np.abs(FA_guess-FA_true)\n",
    "\n",
    "    Frac_err  = np.abs(TrueParams[10]-GuessParams[10])\n",
    "\n",
    "    return Res, alpha_err,angle_err1,angle_err2,Dpar_err,Dperp_err,MD_err,FA_err,Frac_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95929853-d454-4c32-8152-78635911381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(params,TrueSig,bvecs,bvals,Delta):\n",
    "    Signal = Simulator_new(params,bvecs,bvals,Delta,S0=1)\n",
    "    return TrueSig - Signal\n",
    "\n",
    "def residuals_S0(params,TrueSig,bvecs,bvals,Delta):\n",
    "    Signal = Simulator_new(params,bvecs,bvals,Delta,S0=params[-1])\n",
    "    return TrueSig - Signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9224e43e-6c4c-4d96-923d-09f60f2e8f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simulator_new(params,bvecs,bvals,Delta,S0=1):\n",
    "    new_params = [np.array([params[:2]]),params[2],params[3],params[4:10],[params[10],1-params[10]],params[11],S0]\n",
    "    Sig = []\n",
    "    for bve,bva,d in zip(bvecs,bvals,Delta):\n",
    "        Sig.append(CombSignal_poisson(bve,bva,d,delta,new_params))\n",
    "    return np.hstack(Sig) \n",
    "\n",
    "def Simulator_Min(params,Delta,S0=1):\n",
    "    new_params = [np.array([params[:2]]),params[2],params[3],params[4:10],[params[10],1-params[10]],params[11],S0]\n",
    "    Sig1 = CombSignal_poisson(bvecs_Dev[:7],bvals_Dev[:7],Delta[0],delta,new_params)\n",
    "    Sig2 = CombSignal_poisson(bvecs_Dev[7:13],bvals_Dev[7:13],Delta[1],delta,new_params)\n",
    "    Sig3 = CombSignal_poisson(bvecs_Dev[13:],bvals_Dev[13:],Delta[2],delta,new_params)\n",
    "    return np.hstack([Sig1,Sig2,Sig3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595cd5b6-abef-44d1-a666-98c69724165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.random.rand(1)*0.005+1e-4\n",
    "Params_abc =  np.random.rand(1,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(1,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind_guess = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind_guess = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind_guess])\n",
    "\n",
    "Dpar_guess = np.random.rand()*1e-3            # mm^2/s\n",
    "Dperp_guess = np.random.rand()*1e-3             # mm^2/s\n",
    "phi = 0#np.random.rand()*pi\n",
    "cos_theta = 0#np.random.rand()  # uniform in [0,1]\n",
    "theta = np.arccos(cos_theta)         # in [0, pi/2]\n",
    "Angs_guess = np.vstack([theta,phi]).T\n",
    "\n",
    "mean_guess = np.random.rand()*0.005 + 1e-4\n",
    "\n",
    "frac_guess = np.random.rand()\n",
    "guess = np.column_stack([Angs_guess,Dpar_guess,Dperp_guess,DHind_guess,frac_guess,mean_guess]).squeeze()\n",
    "bounds = np.array([[-np.inf,np.inf]]*12).T\n",
    "bounds[:,0] = [0,np.pi/2]\n",
    "bounds[:,1] = [-np.pi,np.pi]\n",
    "bounds[:,2] = [0,5e-3]\n",
    "bounds[:,3] = [0,5e-3]\n",
    "bounds[:,4] = [-5e-3,5e-3]\n",
    "bounds[:,5] = [-5e-3,5e-3]\n",
    "bounds[:,6] = [-5e-3,5e-3]\n",
    "bounds[:,7] = [-5e-3,5e-3]\n",
    "bounds[:,8] = [-5e-3,5e-3]\n",
    "bounds[:,9] = [-5e-3,5e-3]\n",
    "bounds[:,10] = [0,1]\n",
    "bounds[:,11] = [1e-4,0.005+1e-4]\n",
    "LS_result = np.zeros([4,20,12])\n",
    "bve_split = [bvecs[:(n_pts+1)],bvecs[(n_pts+1):2*(n_pts+1)],bvecs[2*(n_pts+1):]]\n",
    "bva_split = [bvals[:(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],bvals[2*(n_pts+1):]]\n",
    "for i in tqdm(range(20)):\n",
    "    for j in range(4):\n",
    "        result = sp.optimize.least_squares(residuals, guess, args=[NoisyTestSig[j,i],bve_split,bva_split,Delta],\n",
    "                                      bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        LS_result[j,i] = result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e844ce61-44e8-44cf-97c0-f185be29f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_Errors = []\n",
    "for N in tqdm(LS_result):\n",
    "    temp = []\n",
    "    for n_guess,n_true,sig in zip(N,TestParams,TestSig):\n",
    "        temp.append(Errors(sig,n_true,n_guess,Delta,bve_split,bva_split))\n",
    "    LS_Errors.append(temp)\n",
    "LS_Errors = np.array(LS_Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07697675-1cd1-40dd-b70d-8c5a173745da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for optimization\n",
    "def fit_SBI(i,j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posterior.sample((1000,), x=NoisyTestSig[i,j],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "y_indx = np.repeat(np.arange(20),4)\n",
    "x_indx = np.tile(np.arange(4),20)\n",
    "indices = np.column_stack([x_indx,y_indx])\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(fit_SBI)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "SBI_Res = np.zeros([4,20,12])\n",
    "\n",
    "for i, j, x in results:\n",
    "    SBI_Res[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    SBI_Res[i, j,-2] = np.clip(SBI_Res[i, j,-2],0,100)\n",
    "    \n",
    "SBI_Errors = []\n",
    "for N in tqdm(SBI_Res):\n",
    "    temp = []\n",
    "    for n_guess,n_true,sig in zip(N,TestParams,TestSig):\n",
    "        temp.append(Errors(sig,n_true,n_guess,Delta,bve_split,bva_split))\n",
    "    SBI_Errors.append(temp)\n",
    "SBI_Errors = np.array(SBI_Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0abdc41-499c-4d54-8a5d-52c490ceb988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for optimization\n",
    "def fit_SBI(i,j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posteriorMin.sample((1000,), x=NoisyTestSig[i,j][DevilIndices],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "y_indx = np.repeat(np.arange(20),4)\n",
    "x_indx = np.tile(np.arange(4),20)\n",
    "indices = np.column_stack([x_indx,y_indx])\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(fit_SBI)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "SBI_Res = np.zeros([4,20,12])\n",
    "\n",
    "for i, j, x in results:\n",
    "    SBI_Res[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    SBI_Res[i, j,-2] = np.clip(SBI_Res[i, j,-2],0,100)\n",
    "    \n",
    "SBI_Errors_Min = []\n",
    "for N in tqdm(SBI_Res):\n",
    "    temp = []\n",
    "    for n_guess,n_true,sig in zip(N,TestParams,TestSig):\n",
    "        temp.append(Errors(sig,n_true,n_guess,Delta,bve_split,bva_split))\n",
    "    SBI_Errors_Min.append(temp)\n",
    "SBI_Errors_Min = np.array(SBI_Errors_Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7babbd4d-43b6-453a-879d-48315dcd1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bve_splitd = [bvecs_Dev[:7],bvecs_Dev[7:13],bvecs_Dev[13:]]\n",
    "bva_splitd = [bvals_Dev[:7],bvals_Dev[7:13],bvals_Dev[13:]]\n",
    "for i in tqdm(range(20)):\n",
    "    for j in range(4):\n",
    "        result = sp.optimize.least_squares(residuals, guess, args=[NoisyTestSig[j,i][DevilIndices],bve_splitd,bva_splitd,Delta],\n",
    "                                      bounds=bounds,verbose=1,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        LS_result[j,i] = result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45857a06-a988-4baa-862a-e45d3b27b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_Errors_Min = []\n",
    "for N in tqdm(LS_result):\n",
    "    temp = []\n",
    "    for n_guess,n_true,sig in zip(N,TestParams,TestSig):\n",
    "        temp.append(Errors(sig,n_true,n_guess,Delta,bve_split,bva_split))\n",
    "    LS_Errors_Min.append(temp)\n",
    "LS_Errors_Min = np.array(LS_Errors_Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb588823-2d5f-4805-a3ab-9dee291746a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dir = MSDir+'/Ctrl055_R01_28/'\n",
    "dat = pmt.read_mat(Dir+'data_loaded.mat')\n",
    "bvecs = dat['direction']\n",
    "bvals = dat['bval']\n",
    "FixedParams = {\n",
    "    'bvals':bvals,\n",
    "    'bvecs':bvecs,\n",
    "    'Delta':[0.017,0.035,0.061],\n",
    "    'delta':0.007,\n",
    "}\n",
    "Delta = FixedParams['Delta']\n",
    "delta = FixedParams['delta']\n",
    "n_pts = 90\n",
    "\n",
    "Delta = [0.017,0.035,0.061] # We know this \n",
    "delta = 0.007 # We know this \n",
    "\n",
    "\n",
    "data = dat['data']\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(0, 10), median_radius=5,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "\n",
    "S_mask, _, _ = load_nifti(Dir+'mask_055.nii.gz', return_img=True)\n",
    "\n",
    "\n",
    "mask1 = np.ones_like(S_mask[:,54,:])\n",
    "mask1[S_mask[:,54,:]==0] = 0\n",
    "structure = np.ones((3, 3), dtype=bool)\n",
    "\n",
    "# Apply dilation. Increase 'iterations' to make the mask even fatter.\n",
    "fat_mask = binary_dilation(mask1, structure=structure, iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1275730-483b-4ec4-adf6-f5c1673081d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/Full_Dat_50_100k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/Full_Dat_50_100k_poisson.pickle\", \"rb\") as handle:\n",
    "        posterior = pickle.load(handle)\n",
    "else:\n",
    "\n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        s0 = S0Rand[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "    \n",
    "        TrainSig1 = CombSignal_poisson(bvecs[:(n_pts+1)],bvals[:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(bvecs[(n_pts+1):2*(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(bvecs[2*(n_pts+1):],bvals[2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(AddNoise(TrainSig[-1],s0,Noise))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "\n",
    "\n",
    "\n",
    "    Obs = torch.tensor(NoisyTrainSig).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    with open(f\"{network_path}/Full_Dat_50_100k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5fc9a-9c0d-4406-bebb-2e2e874c529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "def optimize_chunk(pixels):\n",
    "    results = []\n",
    "    for i, j in pixels:\n",
    "        posterior_samples_1 = posterior.sample((1000,), x=maskdata[i, j,axial_middle, :],show_progress_bars=False)        \n",
    "        results.append((i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])))\n",
    "    return results\n",
    "\n",
    "chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    ")\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for chunk in results:\n",
    "    for i, j, x in chunk:\n",
    "        NoiseEst[i, j] = x\n",
    "        NoiseEst[i, j,-2] = np.clip(NoiseEst[i, j,-2],0,100)\n",
    "        NoiseEst[i, j,-3] = np.clip(NoiseEst[i, j,-3],0,1)\n",
    "NoiseEst2 = np.copy(NoiseEst)\n",
    "\n",
    "for i in range(13):\n",
    "    NoiseEst2[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2[(1-NoiseEst2[...,-3])<0.3,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7c5b5-e2c0-46ca-97ee-a3ab9b49e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(133)\n",
    "S0 = 2000\n",
    "mean_guess = np.random.rand()*0.005+1e-4\n",
    "Params_abc =  np.random.rand(1,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(1,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind_guess = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind_guess = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind_guess])\n",
    "\n",
    "Dpar_guess = np.random.rand()*1e-3            # mm^2/s\n",
    "Dperp_guess = np.random.rand()*1e-3             # mm^2/s\n",
    "phi = 0#np.random.rand()*pi\n",
    "cos_theta = 0#np.random.rand()  # uniform in [0,1]\n",
    "theta = np.arccos(cos_theta)         # in [0, pi/2]\n",
    "Angs_guess = np.vstack([theta,phi]).T\n",
    "S0_guess =np.random.rand()*2475+25\n",
    "\n",
    "frac_guess = np.random.rand()\n",
    "guess = np.column_stack([Angs_guess,Dpar_guess,Dperp_guess,DHind_guess,frac_guess,mean_guess,S0_guess]).squeeze()\n",
    "bounds = np.array([[-np.inf,np.inf]]*13).T\n",
    "bounds[:,0] = [0,np.pi/2]\n",
    "bounds[:,1] = [-np.pi,np.pi]\n",
    "bounds[:,2] = [0,5e-3]\n",
    "bounds[:,3] = [0,5e-3]\n",
    "bounds[:,4] = [-5e-3,5e-3]\n",
    "bounds[:,5] = [-5e-3,5e-3]\n",
    "bounds[:,6] = [-5e-3,5e-3]\n",
    "bounds[:,7] = [-5e-3,5e-3]\n",
    "bounds[:,8] = [-5e-3,5e-3]\n",
    "bounds[:,9] = [-5e-3,5e-3]\n",
    "bounds[:,10] = [0,1]\n",
    "bounds[:,11] = [1e-4,0.005+1e-4]\n",
    "bounds[:,12] = [25,2500]\n",
    "\n",
    "bve_split = [bvecs[:(n_pts+1)],bvecs[(n_pts+1):2*(n_pts+1)],bvecs[2*(n_pts+1):]]\n",
    "bva_split = [bvals[:(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],bvals[2*(n_pts+1):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1a583-1a99-4590-b0d9-fab0ad8c5bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c48114-e2c5-4d52-ae0a-f11fbd5d4371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "def optimize_pixel_LS(i, j):\n",
    "    result = sp.optimize.least_squares(residuals_S0, guess, args=[maskdata[i, j, axial_middle, :],bve_split,bva_split,Delta],\n",
    "                              bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "    return i, j, result.x\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_LS[i, j] = x\n",
    "\n",
    "NoiseEst2_LS = np.copy(NoiseEst_LS)\n",
    "for i in range(13):\n",
    "    NoiseEst2_LS[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2_LS[(1-NoiseEst2_LS[...,-3])<0.3,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc79384-7f2c-4d8b-b27b-e1dca5dbd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs2000 = bvecs[:91][bvals[:91]==2000]\n",
    "distance_matrix = squareform(pdist(bvecs2000))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs2000_selected = bvecs[:91][bvals[:91]==2000][selected_indices]\n",
    "true_indices = []\n",
    "for b in bvecs2000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs4000 = bvecs[:91][bvals[:91]==4000]\n",
    "distance_matrix = squareform(pdist(bvecs4000))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs4000_selected = bvecs[:91][bvals[:91]==4000][selected_indices]\n",
    "for b in bvecs4000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "true_indices1 = true_indices\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs2000 = bvecs[91:182][bvals[91:182]==2000]\n",
    "distance_matrix = squareform(pdist(bvecs2000))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs2000_selected = bvecs[91:182][bvals[91:182]==2000][selected_indices]\n",
    "true_indices = []\n",
    "for b in bvecs2000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs4000 = bvecs[91:182][bvals[91:182]==4000]\n",
    "distance_matrix = squareform(pdist(bvecs4000))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs4000_selected = bvecs[91:182][bvals[91:182]==4000][selected_indices]\n",
    "for b in bvecs4000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "true_indices2 = true_indices\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs2000 = bvecs[182:][bvals[182:]==2000]\n",
    "distance_matrix = squareform(pdist(bvecs2000))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs2000_selected = bvecs[182:][bvals[182:]==2000][selected_indices]\n",
    "true_indices = []\n",
    "for b in bvecs2000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs4000 = bvecs[182:][bvals[182:]==4000]\n",
    "distance_matrix = squareform(pdist(bvecs4000))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs4000_selected = bvecs[182:][bvals[182:]==4000][selected_indices]\n",
    "for b in bvecs4000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "true_indices3 = true_indices\n",
    "\n",
    "DevIndices = [0] + true_indices1 + true_indices2 + true_indices3\n",
    "bvecs_Dev = bvecs[DevIndices]\n",
    "bvals_Dev = bvals[DevIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9826ddd-a173-45b4-8ae6-21aa67aec252",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/Dev_Dat_50_200k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/Dev_Dat_50_200k_poisson.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "\n",
    "    np.random.seed(12)\n",
    "    NumSamps = 100000\n",
    "    \n",
    "    # Directions\n",
    "    x1  = np.random.randn(NumSamps)\n",
    "    y1  = np.random.randn(NumSamps)\n",
    "    z1  =  np.random.randn(NumSamps)\n",
    "    V = np.vstack([x1,y1,z1])\n",
    "    V = (V/np.linalg.norm(V,axis=0)).T\n",
    "    Angs = np.array([SpherAng(v) for v in V])\n",
    "    \n",
    "    #Diffusion of restricted\n",
    "    Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "    Dperp = np.random.rand(NumSamps)*5e-3\n",
    "    \n",
    "    #Diffusion of hindered\n",
    "    Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "    Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "    Params = np.hstack([Params_abc,Params_rest])\n",
    "    DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "    DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "    \n",
    "    #Fraction of hindered\n",
    "    frac  = np.random.rand(NumSamps)\n",
    "    \n",
    "    mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "    scale = np.random.rand(NumSamps)*0.0009+0.0001\n",
    "    \n",
    "    S0Rand =np.random.rand(NumSamps)*2475+25\n",
    "    TrainParams = np.column_stack([V,Angs,Dpar,Dperp,DHind,frac,mean,S0Rand])\n",
    "\n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        #s = sig2[i]\n",
    "        s0 = S0Rand[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "                \n",
    "        TrainSig1 = CombSignal_poisson(bvecs_Dev[:7],bvals_Dev[:7],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(bvecs_Dev[7:13],bvals_Dev[7:13],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(bvecs_Dev[13:],bvals_Dev[13:],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(AddNoise(TrainSig[-1],s0,Noise))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "\n",
    "\n",
    "    Obs = torch.tensor(NoisyTrainSig).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posteriorMin = inference.build_posterior(density_estimator)\n",
    "    with open(f\"{network_path}/Dev_Dat_50_200k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posteriorMin, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f3ce5c-3d6f-43ba-9671-31203fbd50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel(i, j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posteriorMin.sample((1000,), x=maskdata[i, j,axial_middle, DevIndices],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_Min = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_Min[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    NoiseEst_Min[i, j,-2] = np.clip(NoiseEst_Min[i, j,-2],0,100)\n",
    "    NoiseEst_Min[i, j,-3] = np.clip(NoiseEst_Min[i, j,-3],0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049212a-1f49-49e1-9707-090ecc362488",
   "metadata": {},
   "outputs": [],
   "source": [
    "bve_splitD = [bvecs_Dev[:7],bvecs_Dev[7:13],bvecs_Dev[13:]]\n",
    "bva_splitD = [bvals_Dev[:7],bvals_Dev[7:13],bvals_Dev[13:]]\n",
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel_LS(i, j):\n",
    "    result = sp.optimize.least_squares(residuals_S0, guess, args=[maskdata[i, j,axial_middle, DevIndices],bve_splitD,bva_splitD,Delta],\n",
    "                          bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "    return i, j, result.x\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_LS_Min = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_LS_Min[i, j] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff98071-ce2d-4ff5-bc3c-7fcae5416adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, 54, :, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "def optimize_chunk(pixels):\n",
    "    results = []\n",
    "    for i, j in pixels:\n",
    "        posterior_samples_1 = posterior.sample((1000,), x=maskdata[i, 54,j, :],show_progress_bars=False)        \n",
    "        results.append((i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])))\n",
    "    return results\n",
    "\n",
    "chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    ")\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "NoiseEst_CC = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for chunk in results:\n",
    "    for i, j, x in chunk:\n",
    "        NoiseEst_CC[i, j] = x\n",
    "        NoiseEst_CC[i, j,-2] = np.clip(NoiseEst_CC[i, j,-2],0,100)\n",
    "        NoiseEst_CC[i, j,-3] = np.clip(NoiseEst_CC[i, j,-3],0,1)\n",
    "NoiseEst2_CC = np.copy(NoiseEst_CC)\n",
    "\n",
    "comb_mask = fat_mask * ((1-NoiseEst2_CC[...,-3])>0.1)\n",
    "\n",
    "mask_CC = (1-NoiseEst2_CC[...,-3])<0.3\n",
    "for i in range(13):\n",
    "    NoiseEst2_CC[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2_CC[~comb_mask,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da67794-d138-4565-acf3-25791b746035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, 54, :, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel(i, j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posteriorMin.sample((500,), x=maskdata[i, 54, j, DevIndices],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_Min_CC = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_Min_CC[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    NoiseEst_Min_CC[i, j,-2] = np.clip(NoiseEst_Min_CC[i, j,-2],0,100)\n",
    "    NoiseEst_Min_CC[i, j,-3] = np.clip(NoiseEst_Min_CC[i, j,-3],0,1)\n",
    "NoiseEst2_Min_CC = np.copy(NoiseEst_Min_CC)\n",
    "\n",
    "for i in range(13):\n",
    "    NoiseEst2_Min_CC[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2_Min_CC[~comb_mask,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f691554-9ae9-44a8-81e7-96cd375dd495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, 54, :, :], axis=-1) != 0\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel_LS(i, j):\n",
    "    result = sp.optimize.least_squares(residuals_S0, guess, args=[maskdata[i, 54, j, :],bve_split,bva_split,Delta],\n",
    "                              bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "    return i, j, result.x\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_LS_CC = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_LS_CC[i, j] = x\n",
    "\n",
    "NoiseEst2_LS_CC = np.copy(NoiseEst_LS_CC)\n",
    "\n",
    "for i in range(13):\n",
    "    NoiseEst2_LS_CC[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2_LS_CC[~comb_mask,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff069e5-993f-4623-876b-7c27e78b5d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bve_splitD = [bvecs_Dev[:7],bvecs_Dev[7:13],bvecs_Dev[13:]]\n",
    "bva_splitD = [bvals_Dev[:7],bvals_Dev[7:13],bvals_Dev[13:]]\n",
    "\n",
    "if os.path.exists(f\"{DatFolder}/Temp_LS_Min_CC.npy\"):\n",
    "    with open(f\"{DatFolder}/Temp_LS_Min_CC.npy\", \"rb\") as handle:\n",
    "        NoiseEst_LS_Min_CC = np.load(f\"{DatFolder}/Temp_LS_Min_CC.npy\",allow_pickle=True)\n",
    "else:\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(maskdata[:, 54, :, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    def optimize_pixel_LS(i, j):\n",
    "        result = sp.optimize.least_squares(residuals_S0, guess, args=[maskdata[i,54,j, DevIndices],bve_splitD,bva_splitD,Delta],\n",
    "                              bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        return i, j, result.x\n",
    "    \n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    \n",
    "    \n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst_LS_Min_CC = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst_LS_Min_CC[i, j] = x\n",
    "NoiseEst2_LS_Min_CC = np.copy(NoiseEst_LS_Min_CC)\n",
    "\n",
    "for i in range(13):\n",
    "    NoiseEst2_LS_Min_CC[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2_LS_Min_CC[~comb_mask,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9348b-168d-4708-ba5a-c1f76c5be245",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dirs = ['Ctrl055_R01_28','Ctrl056_R01_29','Ctrl057_R01_30']\n",
    "Masks = ['mask_055.nii.gz','mask_056.nii.gz','mask_057.nii.gz']\n",
    "BVecs = []\n",
    "BVals = []\n",
    "Deltas = []\n",
    "deltas = []\n",
    "S_masks = []\n",
    "Datas = []\n",
    "Outlines = []\n",
    "for D,M in tqdm(zip(Dirs,Masks)):\n",
    "    dat = pmt.read_mat(MSDir+D+'/data_loaded.mat')\n",
    "    BVecs.append(dat['direction'])\n",
    "    BVals.append(dat['bval'])\n",
    "    Deltas.append(FixedParams['Delta'])\n",
    "    deltas.append(FixedParams['delta'])\n",
    "    \n",
    "    m, _, _ = load_nifti(MSDir+D+'/'+M, return_img=True)\n",
    "    S_masks.append(m)\n",
    "\n",
    "    data = dat['data']\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    md, mk = median_otsu(data, vol_idx=range(0, 10), median_radius=5,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    Datas.append(md)\n",
    "    Outlines.append(mk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d6626-f19f-421c-8ddd-3b6804a5e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"./Networks/3Indv_50_300k_poisson.pickle\"):\n",
    "    with open(f\"./Networks/3Indv_50_300k_poisson.pickle\", \"rb\") as handle:\n",
    "        posterior = pickle.load(handle)\n",
    "else:\n",
    "\n",
    "    np.random.seed(12)\n",
    "    NumSamps = 300000\n",
    "    \n",
    "    # Directions\n",
    "    x1  = np.random.randn(NumSamps)\n",
    "    y1  = np.random.randn(NumSamps)\n",
    "    z1  =  np.random.randn(NumSamps)\n",
    "    V = np.vstack([x1,y1,z1])\n",
    "    V = (V/np.linalg.norm(V,axis=0)).T\n",
    "    Angs = np.array([SpherAng(v) for v in V])\n",
    "    \n",
    "    #Diffusion of restricted\n",
    "    Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "    Dperp = np.random.rand(NumSamps)*5e-3\n",
    "    \n",
    "    #Diffusion of hindered\n",
    "    Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "    Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "    Params = np.hstack([Params_abc,Params_rest])\n",
    "    DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "    DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "    \n",
    "    #Fraction of hindered\n",
    "    frac  = np.random.rand(NumSamps)\n",
    "    \n",
    "    mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "    \n",
    "    S0Rand =np.random.rand(NumSamps)*2475+25\n",
    "    \n",
    "    Choice = np.random.choice([1,2,3],NumSamps)\n",
    "    TrainParams = np.column_stack([V,Angs,Dpar,Dperp,DHind,frac,mean,S0Rand,Choice*100])\n",
    "    \n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        s0 = S0Rand[i]\n",
    "        c = Choice[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "    \n",
    "        TrainSig1 = CombSignal_poisson(BVecs[c-1][:(n_pts+1)],bvals[:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(BVecs[c-1][(n_pts+1):2*(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(BVecs[c-1][2*(n_pts+1):],bvals[2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(np.append(AddNoise(TrainSig[-1],s0,Noise),c*100))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Obs = torch.tensor(NoisyTrainSig).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    with open(f\"{network_path}/3Indv_50_300k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df944b-fcdb-4421-b562-f2626b5955b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "IndxArr  = []\n",
    "BVecsDev = []\n",
    "BValsDev = []\n",
    "for bve,bva in zip(BVecs,BVals): \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[:91][bva[:91]==2000]\n",
    "    distance_matrix = squareform(pdist(bvecs2000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[:91][bva[:91]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[:91][bva[:91]==4000]\n",
    "    distance_matrix = squareform(pdist(bvecs4000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[:91][bva[:91]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices1 = true_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[91:182][bva[91:182]==2000]\n",
    "    distance_matrix = squareform(pdist(bvecs2000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[91:182][bva[91:182]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[91:182][bva[91:182]==4000]\n",
    "    distance_matrix = squareform(pdist(bvecs4000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[91:182][bva[91:182]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices2 = true_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[182:][bva[182:]==2000]\n",
    "    distance_matrix = squareform(pdist(bvecs2000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[182:][bva[182:]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[182:][bva[182:]==4000]\n",
    "    distance_matrix = squareform(pdist(bvecs4000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[182:][bva[182:]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices3 = true_indices\n",
    "    \n",
    "    DevIndices = [0] + true_indices1 + true_indices2 + true_indices3\n",
    "    bvecs_Dev = bve[DevIndices]\n",
    "    bvals_Dev = bva[DevIndices]\n",
    "\n",
    "    IndxArr.append(DevIndices)\n",
    "    BVecsDev.append(bvecs_Dev)\n",
    "    BValsDev.append(bvals_Dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d562179-b9ba-4a11-81f5-d9f0d6d0ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"./Networks/Dev_3Indv_50_300k_poisson.pickle\"):\n",
    "    with open(f\"./Networks/Dev_3Indv_50_300k_poisson.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "\n",
    "    np.random.seed(12)\n",
    "    NumSamps = 600000\n",
    "    \n",
    "    # Directions\n",
    "    x1  = np.random.randn(NumSamps)\n",
    "    y1  = np.random.randn(NumSamps)\n",
    "    z1  =  np.random.randn(NumSamps)\n",
    "    V = np.vstack([x1,y1,z1])\n",
    "    V = (V/np.linalg.norm(V,axis=0)).T\n",
    "    Angs = np.array([SpherAng(v) for v in V])\n",
    "    \n",
    "    #Diffusion of restricted\n",
    "    Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "    Dperp = np.random.rand(NumSamps)*5e-3\n",
    "    \n",
    "    #Diffusion of hindered\n",
    "    Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "    Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "    Params = np.hstack([Params_abc,Params_rest])\n",
    "    DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "    DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "    \n",
    "    #Fraction of hindered\n",
    "    frac  = np.random.rand(NumSamps)\n",
    "    \n",
    "    mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "    \n",
    "    S0Rand =np.random.rand(NumSamps)*2475+25\n",
    "    \n",
    "    Choice = np.random.choice([1,2,3],NumSamps)\n",
    "\n",
    "    TrainParams = np.column_stack([V,Angs,Dpar,Dperp,DHind,frac,mean,S0Rand,Choice*100])\n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        #s = sig2[i]\n",
    "        s0 = S0Rand[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "        c = Choice[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "\n",
    "        TrainSig1 = CombSignal_poisson(BVecsDev[c-1][:7],BValsDev[c-1][:7],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(BVecsDev[c-1][7:13],BValsDev[c-1][7:13],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(BVecsDev[c-1][13:],BValsDev[c-1][13:],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(np.append(AddNoise(TrainSig[-1],s0,Noise),c*100))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "    \n",
    "    \n",
    "    Obs = torch.tensor(NoisyTrainSig).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posteriorMin = inference.build_posterior(density_estimator)\n",
    "    with open(f\"Networks/Dev_3Indv_50_300k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posteriorMin, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a90ea-e96c-494d-b27a-e94a57f0e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_SBI = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[54,52,54],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = sma[:,sl,:]\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = posterior.sample((1000,), x=np.append(D[i, sl, j, :],100*(kk+1)),show_progress_bars=False)\n",
    "        return i, j, posterior_samples_1.mean(axis=0)\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [14])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "\n",
    "    Full_SBI.append(NoiseEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048500a-7dc2-48bc-8d72-3b6fac1a4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_SBI = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[54,52,54],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = sma[:,sl,:]\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = posteriorMin.sample((1000,), x=np.append(D[i, sl, j, IndxArr[kk]],100*(kk+1)),show_progress_bars=False)\n",
    "        return i, j, posterior_samples_1.mean(axis=0)\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [14])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "\n",
    "    Min_SBI.append(NoiseEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d849393-8204-4fbc-bb7d-137959eca91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMasks = []\n",
    "kk = 0\n",
    "d  = 54\n",
    "temp = np.copy(Full_SBI[kk])\n",
    "for i in range(14):\n",
    "    temp[~Outlines[kk][:,d,:],i] = math.nan\n",
    "    \n",
    "mask1 = np.ones_like(S_masks[kk][:,d,:])\n",
    "mask1[S_masks[kk][:,d,:]==0] = 0\n",
    "\n",
    "# Apply dilation. Increase 'iterations' to make the mask even fatter.\n",
    "fat_mask = binary_dilation(mask1, structure=structure, iterations=1)\n",
    "\n",
    "CMasks.append(fat_mask * ((1-temp[...,-4])>0.1) * (temp[...,-4]>0))\n",
    "\n",
    "kk = 1\n",
    "d  = 52\n",
    "temp = np.copy(Full_SBI[kk])\n",
    "for i in range(14):\n",
    "    temp[~Outlines[kk][:,d,:],i] = math.nan\n",
    "    \n",
    "mask1 = np.ones_like(S_masks[kk][:,d,:])\n",
    "mask1[S_masks[kk][:,d,:]==0] = 0\n",
    "\n",
    "# Apply dilation. Increase 'iterations' to make the mask even fatter.\n",
    "fat_mask = binary_dilation(mask1, structure=structure, iterations=1)\n",
    "\n",
    "CMasks.append(fat_mask * ((1-temp[...,-4])>0) * (temp[...,-4]>0))\n",
    "\n",
    "kk = 2\n",
    "d  = 54\n",
    "temp = np.copy(Full_SBI[kk])\n",
    "for i in range(14):\n",
    "    temp[~Outlines[kk][:,d,:],i] = math.nan\n",
    "    \n",
    "mask1 = np.ones_like(S_masks[kk][:,d,:])\n",
    "mask1[S_masks[kk][:,d,:]==0] = 0\n",
    "# Apply dilation. Increase 'iterations' to make the mask even fatter.\n",
    "fat_mask = binary_dilation(mask1, structure=structure, iterations=1)\n",
    "CMasks.append(fat_mask * ((1-temp[...,-4])>0.3) * (temp[...,-4]>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a596647-51ea-4ae3-82c5-39e991416c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_LS = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[54,52,54],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = sma[:,sl,:]\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    bve_split_kk = [BVecs[kk][:(n_pts+1)],BVecs[kk][(n_pts+1):2*(n_pts+1)],BVecs[kk][2*(n_pts+1):]]\n",
    "    bva_split_kk = [BVals[kk][:(n_pts+1)],BVals[kk][(n_pts+1):2*(n_pts+1)],BVals[kk][2*(n_pts+1):]]\n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel_LS(i, j):\n",
    "        result = sp.optimize.least_squares(residuals_S0, guess, args=[D[i, sl, j, :],bve_split_kk,bva_split_kk,Delta],\n",
    "                                  bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        return i, j, result.x\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst_LS[i, j] = x\n",
    "\n",
    "    Full_LS.append(NoiseEst_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e076cb82-a46a-4fe3-951c-e394a2a06142",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_LS = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[54,52,54],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = sma[:,sl,:]\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    bve_splitd_kk = [BVecsDev[kk][:7],BVecsDev[kk][7:13],BVecsDev[kk][13:]]\n",
    "    bva_splitd_kk = [BValsDev[kk][:7],BValsDev[kk][7:13],BValsDev[kk][13:]]\n",
    "\n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel_LS(i, j):\n",
    "        result = sp.optimize.least_squares(residuals_S0, guess, args=[D[i, sl, j, IndxArr[kk]],bve_splitd_kk,bva_splitd_kk,Delta],\n",
    "                                  bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        return i, j, result.x\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst_LS[i, j] = x\n",
    "\n",
    "    Min_LS.append(NoiseEst_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267832b5-230f-4122-afdb-4b91c99a330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dirs = ['NMSS_11_1year','NMSS_15','NMSS_16','NMSS_18','NMSS_19','Ctrl055_R01_28','Ctrl056_R01_29','Ctrl057_R01_30']\n",
    "BVecs = []\n",
    "BVals = []\n",
    "Deltas = []\n",
    "deltas = []\n",
    "S_masks = []\n",
    "Datas = []\n",
    "Outlines = []\n",
    "axial_middles = []\n",
    "for D in tqdm(Dirs):\n",
    "    F = pmt.read_mat(MSDir+D+'/data_loaded.mat')\n",
    "    affine = np.ones((4,4))\n",
    "    BVecs.append(F['direction'])\n",
    "    BVals.append(F['bval'])\n",
    "    Deltas.append(Delta)\n",
    "    deltas.append(delta)\n",
    "\n",
    "\n",
    "    \n",
    "    data, affine = reslice(F['data'], affine, (2,2,2), (2.5,2.5,2.5))\n",
    "\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    md, mk = median_otsu(data, vol_idx=range(0, 10), median_radius=5,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    Datas.append(md)\n",
    "    axial_middles.append(axial_middle)\n",
    "    Outlines.append(mk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9dc2a-1720-41f3-b90c-c77a59965573",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/8Indv_50_300k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/8Indv_50_300k_poisson.pickle\", \"rb\") as handle:\n",
    "        posterior = pickle.load(handle)\n",
    "else:\n",
    "\n",
    "    np.random.seed(12)\n",
    "    NumSamps = 300000\n",
    "    \n",
    "    # Directions\n",
    "    x1  = np.random.randn(NumSamps)\n",
    "    y1  = np.random.randn(NumSamps)\n",
    "    z1  =  np.random.randn(NumSamps)\n",
    "    V = np.vstack([x1,y1,z1])\n",
    "    V = (V/np.linalg.norm(V,axis=0)).T\n",
    "    Angs = np.array([SpherAng(v) for v in V])\n",
    "    \n",
    "    #Diffusion of restricted\n",
    "    Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "    Dperp = np.random.rand(NumSamps)*5e-3\n",
    "    \n",
    "    #Diffusion of hindered\n",
    "    Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "    Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "    Params = np.hstack([Params_abc,Params_rest])\n",
    "    DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "    DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "    \n",
    "    #Fraction of hindered\n",
    "    frac  = np.random.rand(NumSamps)\n",
    "    \n",
    "    mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "    \n",
    "    S0Rand =np.random.rand(NumSamps)*2475+25\n",
    "    \n",
    "    Choice = np.random.choice([1,2,3,4,5,6,7,8],NumSamps)\n",
    "\n",
    "    TrainParams = np.column_stack([V,Angs,Dpar,Dperp,DHind,frac,mean,S0Rand,Choice*100])\n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        s0 = S0Rand[i]\n",
    "        c = Choice[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "    \n",
    "        TrainSig1 = CombSignal_poisson(BVecs[c-1][:(n_pts+1)],bvals[:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(BVecs[c-1][(n_pts+1):2*(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(BVecs[c-1][2*(n_pts+1):],bvals[2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(np.append(AddNoise(TrainSig[-1],s0,Noise),c*100))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Obs = torch.tensor(NoisyTrainSig).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs=100)\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    with open(f\"{network_path}/8Indv_50_300k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538cb95-0936-43b2-9ba7-a75f90ccb7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "IndxArr  = []\n",
    "BVecsDev = []\n",
    "BValsDev = []\n",
    "for bve,bva in zip(BVecs,BVals): \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[:91][bva[:91]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[:91][bva[:91]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[:91][bva[:91]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[:91][bva[:91]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices1 = true_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[91:182][bva[91:182]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[91:182][bva[91:182]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[91:182][bva[91:182]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[91:182][bva[91:182]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices2 = true_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[182:][bva[182:]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[182:][bva[182:]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[182:][bva[182:]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[182:][bva[182:]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices3 = true_indices\n",
    "    \n",
    "    DevIndices = [0] + true_indices1 + true_indices2 + true_indices3\n",
    "    bvecs_Dev = bve[DevIndices]\n",
    "    bvals_Dev = bva[DevIndices]\n",
    "\n",
    "    IndxArr.append(DevIndices)\n",
    "    BVecsDev.append(bvecs_Dev)\n",
    "    BValsDev.append(bvals_Dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ea028-fddb-4700-ad79-75ffe1be539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/Dev_8Indv_50_300k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/Dev_8Indv_50_300k_poisson.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "\n",
    "    np.random.seed(12)\n",
    "    NumSamps = 300000\n",
    "    \n",
    "    # Directions\n",
    "    x1  = np.random.randn(NumSamps)\n",
    "    y1  = np.random.randn(NumSamps)\n",
    "    z1  =  np.random.randn(NumSamps)\n",
    "    V = np.vstack([x1,y1,z1])\n",
    "    V = (V/np.linalg.norm(V,axis=0)).T\n",
    "    Angs = np.array([SpherAng(v) for v in V])\n",
    "    \n",
    "    #Diffusion of restricted\n",
    "    Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "    Dperp = np.random.rand(NumSamps)*5e-3\n",
    "    \n",
    "    #Diffusion of hindered\n",
    "    Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "    Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "    Params = np.hstack([Params_abc,Params_rest])\n",
    "    DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "    DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "    \n",
    "    #Fraction of hindered\n",
    "    frac  = np.random.rand(NumSamps)\n",
    "    \n",
    "    mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "    \n",
    "    S0Rand =np.random.rand(NumSamps)*2475+25\n",
    "    \n",
    "    Choice = np.random.choice([1,2,3,4,5,6,7,8],NumSamps)\n",
    "    TrainParams = np.column_stack([V,Angs,Dpar,Dperp,DHind,frac,mean,S0Rand,Choice*100])\n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        #s = sig2[i]\n",
    "        s0 = S0Rand[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "        c = Choice[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "\n",
    "        TrainSig1 = CombSignal_poisson(BVecsDev[c-1][:7],BValsDev[c-1][:7],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(BVecsDev[c-1][7:13],BValsDev[c-1][7:13],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(BVecsDev[c-1][13:],BValsDev[c-1][13:],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(np.append(AddNoise(TrainSig[-1],s0,Noise),c*100))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "    \n",
    "    \n",
    "    Obs = torch.tensor(NoisyTrainSig).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs=100)\n",
    "    posteriorMin = inference.build_posterior(density_estimator)\n",
    "    with open(f\"{network_path}/Dev_8Indv_50_300k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posteriorMin, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49256fe0-6e8a-470b-87c6-d23410cb6b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_SBI_Extra = []\n",
    "for kk,(D,sl) in enumerate(zip(Datas,[48]*8)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    def optimize_chunk(pixels):\n",
    "        results = []\n",
    "        for i, j in pixels:\n",
    "            posterior_samples_1 = posterior.sample((1000,), x=np.append(D[i,j,sl, :],100*(kk+1)),show_progress_bars=False)\n",
    "            results.append((i, j, posterior_samples_1.mean(axis=0)))\n",
    "        return results\n",
    "    \n",
    "    chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    "    )\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [14])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for chunk in results:\n",
    "        for i, j, x in chunk:\n",
    "            NoiseEst[i, j] = x\n",
    "\n",
    "    Full_SBI_Extra.append(NoiseEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc582165-b69d-497a-bbac-f77e219a5b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_SBI_Extra = []\n",
    "for kk,(D,sl) in enumerate(zip(Datas,[48]*8)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    Arr = D[:,:,sl, IndxArr[kk]]\n",
    "    def optimize_chunk(pixels):\n",
    "        results = []\n",
    "        for i, j in pixels:\n",
    "            posterior_samples_1 = posteriorMin.sample((1000,), x=np.append(Arr[i,j],100*(kk+1)),show_progress_bars=False)\n",
    "            results.append((i, j, posterior_samples_1.mean(axis=0)))\n",
    "        return results\n",
    "    \n",
    "    chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    "    )\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [14])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for chunk in results:\n",
    "        for i, j, x in chunk:\n",
    "            NoiseEst[i, j] = x\n",
    "\n",
    "    Min_SBI_Extra.append(NoiseEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f606ca-a196-4169-ab03-2875466e656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{DatFolder}/temp_Full_LS.npy\"):\n",
    "    with open(f\"{DatFolder}/temp_Full_LS.npy\", \"rb\") as handle:\n",
    "        Full_LS_extra = np.load(DatFolder+'temp_Full_LS.npy')\n",
    "else:\n",
    "    Full_LS_extra = []\n",
    "    for kk,(D,sl) in enumerate(zip(Datas,[48]*8)):\n",
    "        # Compute the mask where the sum is not zero\n",
    "        mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "        \n",
    "        # Get the indices where mask is True\n",
    "        indices = np.argwhere(mask)\n",
    "    \n",
    "        bve_split_kk = [BVecs[kk][:(n_pts+1)],BVecs[kk][(n_pts+1):2*(n_pts+1)],BVecs[kk][2*(n_pts+1):]]\n",
    "        bva_split_kk = [BVals[kk][:(n_pts+1)],BVals[kk][(n_pts+1):2*(n_pts+1)],BVals[kk][2*(n_pts+1):]]\n",
    "        # Define the function for optimization\n",
    "        def optimize_pixel_LS(i, j):\n",
    "            result = sp.optimize.least_squares(residuals_S0, guess, args=[D[i,j,sl, :],bve_split_kk,bva_split_kk,Delta],\n",
    "                                      bounds=bounds,verbose=0,jac='3-point')\n",
    "            return i, j, result.x\n",
    "        \n",
    "    \n",
    "        \n",
    "        # Initialize NoiseEst with the appropriate shape\n",
    "        ArrShape = mask.shape\n",
    "        \n",
    "        # Use joblib to parallelize the optimization tasks\n",
    "        results = Parallel(n_jobs=8)(\n",
    "            delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "        \n",
    "        # Assign the optimization results to NoiseEst\n",
    "        for i, j, x in results:\n",
    "            NoiseEst_LS[i, j] = x\n",
    "    \n",
    "        Full_LS_extra.append(NoiseEst_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5e040-75b4-439d-bd54-88e5ce04a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{DatFolder}/temp_Min_LS.npy\"):\n",
    "    with open(f\"{DatFolder}/temp_Min_LS.npy\", \"rb\") as handle:\n",
    "        Min_LS_extra = np.load(DatFolder+'temp_Min_LS.npy')\n",
    "else:\n",
    "    Min_LS_extra = []\n",
    "    for kk,(D,sl) in enumerate(zip(Datas,[48]*8)):\n",
    "        # Compute the mask where the sum is not zero\n",
    "        mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "        \n",
    "        # Get the indices where mask is True\n",
    "        indices = np.argwhere(mask)\n",
    "        \n",
    "        bve_splitd_kk = [BVecsDev[kk][:7],BVecsDev[kk][7:13],BVecsDev[kk][13:]]\n",
    "        bva_splitd_kk = [BValsDev[kk][:7],BValsDev[kk][7:13],BValsDev[kk][13:]]\n",
    "    \n",
    "        # Define the function for optimization\n",
    "        def optimize_pixel_LS(i, j):\n",
    "            result = sp.optimize.least_squares(residuals_S0, guess, args=[D[i, j,sl, IndxArr[kk]],bve_splitd_kk,bva_splitd_kk,Delta],\n",
    "                                      bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "            return i, j, result.x\n",
    "            \n",
    "        # Initialize NoiseEst with the appropriate shape\n",
    "        ArrShape = mask.shape\n",
    "        \n",
    "        # Use joblib to parallelize the optimization tasks\n",
    "        results = Parallel(n_jobs=8)(\n",
    "            delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "        \n",
    "        # Assign the optimization results to NoiseEst\n",
    "        for i, j, x in results:\n",
    "            NoiseEst_LS[i, j] = x\n",
    "    \n",
    "        Min_LS_extra.append(NoiseEst_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e82317e-891a-46e0-83f1-58f670818f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "WMDir = './MS_data/WM_masks/'\n",
    "WMs = []\n",
    "for i,Name in tqdm(enumerate(['NMSS_11_1year','NMSS_15','NMSS_16','NMSS_18','NMSS_19','Ctrl055_R01_28','Ctrl056_R01_29','Ctrl057_R01_30'])):\n",
    "    \n",
    "    for k,x in enumerate(os.listdir(WMDir)):\n",
    "        if Name in x:\n",
    "            WM, affine, img = load_nifti(WMDir+x, return_img=True)\n",
    "            #WM, affine = reslice(WM, affine, (2,2,2), (2.5,2.5,2.5))\n",
    "            if(i<5):\n",
    "                WM_t = np.fliplr(np.swapaxes(WM,0,1))\n",
    "            else:\n",
    "                WM_t = np.fliplr(np.flipud(np.swapaxes(WM,0,1)))\n",
    "            WM_t,_ = reslice(WM_t, affine, (2,2,2), (2.5,2.5,2.5))\n",
    "            WMs.append(WM_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bafebc5-adc4-456d-8f89-dca420ccc266",
   "metadata": {},
   "outputs": [],
   "source": [
    "KK = [48]*8\n",
    "FA_Full_SBI = []\n",
    "MD_Full_SBI = []\n",
    "for jj in range(8):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(Par_frac)(i, j,Full_SBI_Extra[jj][...,4:10]) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, k in results:\n",
    "        temp1[i, j] = k[0]\n",
    "        temp2[i, j] = k[1]\n",
    "\n",
    "    FA_Full_SBI.append(temp1)\n",
    "    MD_Full_SBI.append(temp2)\n",
    "KK = [48]*8\n",
    "FA_Min_SBI = []\n",
    "MD_Min_SBI = []\n",
    "for jj in range(8):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "\n",
    "    Arr = Min_SBI_Extra[jj][...,4:10]\n",
    "         \n",
    "    results = Parallel(n_jobs=8,)(\n",
    "        delayed(Par_frac)(i, j,Min_SBI_Extra[jj][...,4:10]) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, k in results:\n",
    "        temp1[i, j] = k[0]\n",
    "        temp2[i, j] = k[1]\n",
    "\n",
    "    FA_Min_SBI.append(temp1)\n",
    "    MD_Min_SBI.append(temp2)\n",
    "KK = [48]*8\n",
    "FA_Full_LS = []\n",
    "MD_Full_LS = []\n",
    "for jj in range(8):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    results = Parallel(n_jobs=8,)(\n",
    "        delayed(Par_frac)(i, j,Full_LS_extra[jj][...,4:10]) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, k in results:\n",
    "        temp1[i, j] = k[0]\n",
    "        temp2[i, j] = k[1]\n",
    "\n",
    "    FA_Full_LS.append(temp1)\n",
    "    MD_Full_LS.append(temp2)\n",
    "\n",
    "KK = [48]*8\n",
    "FA_Min_LS = []\n",
    "MD_Min_LS = []\n",
    "for jj in range(8):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(Par_frac)(i, j,Min_LS_extra[jj][...,4:10]) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, k in results:\n",
    "        temp1[i, j] = k[0]\n",
    "        temp2[i, j] = k[1]\n",
    "\n",
    "    FA_Min_LS.append(temp1)\n",
    "    MD_Min_LS.append(temp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203c51c8-9b07-4632-8d7d-6f268800977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jj = -4\n",
    "SBI_comp_Frac = []\n",
    "KK = [48]*8\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Min_SBI_Extra[i][...,jj])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(Full_SBI_Extra[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_comp_Frac.append(masked_ssim.mean())\n",
    "\n",
    "LS_comp_Frac = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Min_LS_extra[i][...,jj])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(Full_LS_extra[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    LS_comp_Frac.append(masked_ssim.mean())\n",
    "\n",
    "SBI_LS_comp_Frac = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Full_SBI_Extra[i][...,jj])\n",
    "    NS2 = np.copy(Full_LS_extra[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_LS_comp_Frac.append(masked_ssim.mean())\n",
    "Prec7_SBI_Frac = []\n",
    "PrecFull_SBI_Frac = []\n",
    "\n",
    "Prec7_NLLS_Frac = []\n",
    "PrecFull_NLLS_Frac = []\n",
    "for i in range(8):\n",
    "    Prec7_SBI_Frac.append(np.std(Min_SBI_Extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_SBI_Frac.append(np.std(Full_SBI_Extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "\n",
    "    Prec7_NLLS_Frac.append(np.std(Min_LS_extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_NLLS_Frac.append(np.std(Full_LS_extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b143c64-e00c-4290-bce7-3c1f5ae8375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBI_comp_MD = []\n",
    "KK = [48]*8\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(MD_Min_SBI[i])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(MD_Full_SBI[i])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_comp_MD.append(masked_ssim.mean())\n",
    "\n",
    "LS_comp_MD = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(MD_Min_LS[i])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(MD_Full_LS[i])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    LS_comp_MD.append(masked_ssim.mean())\n",
    "\n",
    "SBI_LS_comp_MD = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(MD_Full_SBI[i])\n",
    "    NS2 = np.copy(MD_Full_LS[i])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_LS_comp_MD.append(masked_ssim.mean())\n",
    "Prec7_SBI_MD = []\n",
    "PrecFull_SBI_MD = []\n",
    "\n",
    "Prec7_NLLS_MD = []\n",
    "PrecFull_NLLS_MD = []\n",
    "for i in range(8):\n",
    "    Prec7_SBI_MD.append(np.std(MD_Min_SBI[i][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_SBI_MD.append(np.std(MD_Full_SBI[i][WMs[i].astype(bool)[:,:,48]]))\n",
    "\n",
    "    Prec7_NLLS_MD.append(np.std(MD_Min_LS[i][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_NLLS_MD.append(np.std(MD_Full_LS[i][WMs[i].astype(bool)[:,:,48]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a5b6f-f8ec-4868-8e4b-a628870d2e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jj = -4\n",
    "SBI_comp_Frac = []\n",
    "KK = [48]*8\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Min_SBI_Extra[i][...,jj])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(Full_SBI_Extra[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_comp_Frac.append(masked_ssim.mean())\n",
    "\n",
    "LS_comp_Frac = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Min_LS_extra[i][...,jj])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(Full_LS_extra[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    LS_comp_Frac.append(masked_ssim.mean())\n",
    "\n",
    "SBI_LS_comp_Frac = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Full_SBI_Extra[i][...,jj])\n",
    "    NS2 = np.copy(Full_LS_extra[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_LS_comp_Frac.append(masked_ssim.mean())\n",
    "Prec7_SBI_Frac = []\n",
    "PrecFull_SBI_Frac = []\n",
    "\n",
    "Prec7_NLLS_Frac = []\n",
    "PrecFull_NLLS_Frac = []\n",
    "for i in range(8):\n",
    "    Prec7_SBI_Frac.append(np.std(Min_SBI_Extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_SBI_Frac.append(np.std(Full_SBI_Extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "\n",
    "    Prec7_NLLS_Frac.append(np.std(Min_LS_extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_NLLS_Frac.append(np.std(Full_LS_extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f25f58-c06b-422e-831e-f4b3965e42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = 3\n",
    "SBI_comp_Dp = []\n",
    "KK = [48]*8\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Min_SBI_Extra[i][...,jj])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(Full_SBI_Extra[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_comp_Dp.append(masked_ssim.mean())\n",
    "\n",
    "LS_comp_Dp = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Min_LS_extra[i][...,jj])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(Full_LS_extra[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    LS_comp_Dp.append(masked_ssim.mean())\n",
    "\n",
    "SBI_LS_comp_Dp = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Full_SBI_Extra[i][...,jj])\n",
    "    NS2 = np.copy(Full_LS_extra[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_LS_comp_Dp.append(masked_ssim.mean())\n",
    "Prec7_SBI_Dp = []\n",
    "PrecFull_SBI_Dp = []\n",
    "\n",
    "Prec7_NLLS_Dp = []\n",
    "PrecFull_NLLS_Dp = []\n",
    "for i in range(8):\n",
    "    Prec7_SBI_Dp.append(np.std(Min_SBI_Extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_SBI_Dp.append(np.std(Full_SBI_Extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "\n",
    "    Prec7_NLLS_Dp.append(np.std(Min_LS_extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_NLLS_Dp.append(np.std(Full_LS_extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e64cee-fbd2-459c-9155-f6a4ebfde2f2",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a791e3be-24a2-4ba1-b57a-bd3b4b71ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "r = 1.0  # sphere radius\n",
    "vector = np.array([-0.5, -1, 1])   # arbitrary vector\n",
    "n = vector / np.linalg.norm(vector)  # unit vector in the direction of 'vector'\n",
    "intersection = n * r  # intersection of the vector with the sphere\n",
    "\n",
    "# Circle parameters (geodesic circle on the sphere)\n",
    "circle_angle_deg = 15  # angular radius in degrees\n",
    "alpha1 = [(S[:,2].mean()) for S in SBI_Errors][-1]\n",
    "\n",
    "# -----------------------------\n",
    "# Construct a circle on the sphere\n",
    "# -----------------------------\n",
    "# To draw a circle on the sphere centered at 'intersection',\n",
    "# we use the following idea:\n",
    "# For a given center n (a point on the unit sphere) and an angular radius alpha,\n",
    "# any point on the circle can be written as:\n",
    "#   P(t) = cos(alpha)*n + sin(alpha)*(cos(t)*u + sin(t)*w)\n",
    "# where u and w are any two orthonormal vectors spanning the tangent plane at n.\n",
    "\n",
    "# First, choose u as a vector perpendicular to n.\n",
    "# (If n is parallel to the z-axis, choose a different axis to avoid the zero vector.)\n",
    "if np.allclose(n, [0, 0, 1]):\n",
    "    u = np.array([1, 0, 0])\n",
    "else:\n",
    "    u = np.cross(n, [0, 0, 1])\n",
    "    u = u / np.linalg.norm(u)\n",
    "\n",
    "# Then, w is perpendicular to both n and u.\n",
    "w = np.cross(n, u)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Create the sphere mesh\n",
    "# -----------------------------\n",
    "phi = np.linspace(0, 2 * np.pi, 500)  # azimuthal angle\n",
    "theta = np.linspace(0, np.pi, 500)      # polar angle\n",
    "\n",
    "phi, theta = np.meshgrid(phi, theta)\n",
    "x_sphere = r * np.sin(theta) * np.cos(phi)\n",
    "y_sphere = r * np.sin(theta) * np.sin(phi)\n",
    "z_sphere = r * np.cos(theta)\n",
    "\n",
    "# -----------------------------\n",
    "# Plot everything\n",
    "# -----------------------------\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "# Plot the vector (using quiver)\n",
    "ax.quiver(0, 0, 0, intersection[0], intersection[1], intersection[2],\n",
    "          color='r', linewidth=2, arrow_length_ratio=0.1)\n",
    "\n",
    "# Plot the circle on the sphere\n",
    "# Create points around the circle\n",
    "t_vals = np.linspace(0, 2 * np.pi, 200)\n",
    "circle_points = np.array([\n",
    "    np.cos(alpha1) * n + np.sin(alpha1) * (np.cos(t) * u + np.sin(t) * w)\n",
    "    for t in t_vals\n",
    "])\n",
    "\n",
    "ax.plot(circle_points[:, 0], circle_points[:, 1], circle_points[:, 2], color='paleturquoise', linewidth=2,ls='--')\n",
    "\n",
    "circle_angle_deg = 15  # angular radius in degrees\n",
    "alpha2 = [(S[:,2].mean()) for S in SBI_Errors_Min][-1]\n",
    "\n",
    "# Plot the circle on the sphere\n",
    "# Create points around the circle\n",
    "t_vals = np.linspace(0, 2 * np.pi, 100)\n",
    "circle_points = np.array([\n",
    "    np.cos(alpha2) * n + np.sin(alpha2) * (np.cos(t) * u + np.sin(t) * w)\n",
    "    for t in t_vals\n",
    "])\n",
    "\n",
    "ax.plot(circle_points[:, 0], circle_points[:, 1], circle_points[:, 2], color='lightseagreen', linewidth=2,ls='--')\n",
    "\n",
    "alpha3 = [(S[:,2].mean()) for S in LS_Errors][-1]\n",
    "\n",
    "# Plot the circle on the sphere\n",
    "# Create points around the circle\n",
    "t_vals = np.linspace(0, 2 * np.pi, 100)\n",
    "circle_points = np.array([\n",
    "    np.cos(alpha3) * n + np.sin(alpha3) * (np.cos(t) * u + np.sin(t) * w)\n",
    "    for t in t_vals\n",
    "])\n",
    "\n",
    "ax.plot(circle_points[:, 0], circle_points[:, 1], circle_points[:, 2], color='sandybrown', linewidth=2,ls='--')\n",
    "\n",
    "alpha4 = [(S[:,2].mean()) for S in LS_Errors_Min][-1]\n",
    "\n",
    "# Plot the circle on the sphere\n",
    "# Create points around the circle\n",
    "t_vals = np.linspace(0, 2 * np.pi, 100)\n",
    "circle_points = np.array([\n",
    "    np.cos(alpha4) * n + np.sin(alpha4) * (np.cos(t) * u + np.sin(t) * w)\n",
    "    for t in t_vals\n",
    "])\n",
    "\n",
    "ax.plot(circle_points[:, 0], circle_points[:, 1], circle_points[:, 2], color='darkorange', linewidth=2,ls='--')\n",
    "\n",
    "# Set equal aspect ratio for all axes\n",
    "max_range = r * 1.2\n",
    "for axis in 'xyz':\n",
    "    getattr(ax, 'set_{}lim'.format(axis))((-max_range, max_range))\n",
    "\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot < np.cos(alpha3)) + (dot > np.cos(alpha4))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# -----------------------------\n",
    "# Plot everything\n",
    "# -----------------------------\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot > np.cos(alpha3)) + (dot < np.cos(alpha4))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='darkorange',shade=False, alpha=0.5, rstride=2, cstride=2, edgecolor='none')\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot > np.cos(alpha2)) + (dot < np.cos(alpha3))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='sandybrown',shade=False, alpha=0.5, rstride=2, cstride=2, edgecolor='none')\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot > np.cos(alpha1)) + (dot < np.cos(alpha2))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='lightseagreen',shade=False, alpha=0.5, rstride=2, cstride=2, edgecolor='none')\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot < np.cos(alpha1))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='paleturquoise',alpha=0.5,linewidth=0,rstride=1, cstride=1, shade=False,)\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot > np.cos(alpha4))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='gray', alpha=0.2, rstride=2, cstride=2, edgecolor='none')\n",
    "\n",
    "ax.axis('equal')\n",
    "ax.axis('off')\n",
    "ax.view_init(elev=20, azim=-85)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Reduced NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Reduced SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')\n",
    "\n",
    "ax.legend(\n",
    "    handles=[minLS_patch,minSBI_patch,fullLS_patch,fullSBI_patch],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=2,\n",
    "    bbox_to_anchor=(0.18, 0.09),fontsize=18,\n",
    "    columnspacing=0.5,\n",
    "    handlelength=0.8,\n",
    ")\n",
    "ax.set_title('Average angle diff.',x=0.52, y=0.825,fontsize=24)\n",
    "\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'AngleErr.pdf',bbox_inches='tight',format='pdf',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec08ac-7342-424c-a34e-0057dff99d72",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac37e106-f9ba-4ceb-9bfc-c1599e4f5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pos = np.array([0, 2, 4,6])*2\n",
    "POSITIONS = g_pos\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors[:,:,1]*1000,)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,4))    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "# Colors\n",
    "BG_WHITE = \"#fbf9f4\"\n",
    "GREY_LIGHT = \"#b4aea9\"\n",
    "GREY50 = \"#7F7F7F\"\n",
    "BLUE_DARK = \"#1B2838\"\n",
    "BLUE = \"#2a475e\"\n",
    "BLACK = \"#282724\"\n",
    "GREY_DARK = \"#747473\"\n",
    "RED_DARK = \"#850e00\"\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='turquoise'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors[:,:,1].T*1000,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors[:,:,1]*1000):\n",
    "    ax.scatter(x, y, s = 100, color='paleturquoise', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+0.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors_Min[:,:,1],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='cadetblue'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors_Min[:,:,1].T*1000,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors_Min[:,:,1]*1000):\n",
    "    ax.scatter(x, y, s = 100, color='darkturquoise', alpha=0.8)\n",
    "\n",
    "\n",
    "POSITIONS = g_pos+1.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors[:,:,1],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='sandybrown'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors[:,:,1].T*1000,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors[:,:,1]*1000):\n",
    "    ax.scatter(x, y, s = 100, color='peachpuff', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+2\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors_Min[:,:,1]*1000,)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "\n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='darkorange'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors_Min[:,:,1].T*1000,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors_Min[:,:,1]*1000):\n",
    "    ax.scatter(x, y, s = 100, color='orange', alpha=0.8)\n",
    "\n",
    "ax.set_xticks([1,5,9,13],['2','10','20','30'],fontsize=24)\n",
    "ax.set_xlabel('SNR',fontsize=32)\n",
    "ax.tick_params(axis='x', labelsize=24)\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Reduced NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Reduced SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')\n",
    "\n",
    "ax.legend(\n",
    "    handles=[minLS_patch,minSBI_patch,fullLS_patch,fullSBI_patch],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=2,\n",
    "    bbox_to_anchor=(0.12, 0.8),fontsize=24,\n",
    "    columnspacing=0.5,\n",
    "    handlelength=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09dfe0-0f94-4801-a2b8-0467c976ecf5",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03318b9b-1d29-4cde-8514-73c27c293a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pos = np.array([0, 2, 4,6])*2\n",
    "POSITIONS = g_pos\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors[:,:,-1],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='turquoise'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors[:,:,-1].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors[:,:,-1]):\n",
    "    ax.scatter(x, y, s = 100, color='paleturquoise', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+0.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors_Min[:,:,-1],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='cadetblue'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors_Min[:,:,-1].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors_Min[:,:,-1]):\n",
    "    ax.scatter(x, y, s = 100, color='darkturquoise', alpha=0.8)\n",
    "\n",
    "\n",
    "POSITIONS = g_pos+1.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors[:,:,-1],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='sandybrown'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors[:,:,-1].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors[:,:,-1]):\n",
    "    ax.scatter(x, y, s = 100, color='peachpuff', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+2\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors_Min[:,:,-1],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='darkorange'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors_Min[:,:,-1].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors_Min[:,:,-1]):\n",
    "    ax.scatter(x, y, s = 100, color='orange', alpha=0.8)\n",
    "\n",
    "ax.set_xticks([1,5,9,13],['2','10','20','30'],fontsize=24)\n",
    "ax.set_xlabel('SNR',fontsize=32)\n",
    "ax.tick_params(axis='x', labelsize=24)\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Minimum NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Minimum SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')\n",
    "\n",
    "plt.ylim([-0.1,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d8ba81-3923-45c6-bc85-9521bf95be9c",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc5d98-aeae-4868-aa21-16d2a1d13bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pos = np.array([0, 2, 4,6])*2\n",
    "POSITIONS = g_pos\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors[:,:,-4],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='turquoise'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors[:,:,-4].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors[:,:,-4]):\n",
    "    ax.scatter(x, y, s = 100, color='paleturquoise', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+0.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors_Min[:,:,-4],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='cadetblue'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors_Min[:,:,-4].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors_Min[:,:,-4]):\n",
    "    ax.scatter(x, y, s = 100, color='darkturquoise', alpha=0.8)\n",
    "\n",
    "\n",
    "POSITIONS = g_pos+1.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors[:,:,-4],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='sandybrown'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors[:,:,-4].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors[:,:,-4]):\n",
    "    ax.scatter(x, y, s = 100, color='peachpuff', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+2\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors_Min[:,:,-4],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='darkorange'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors_Min[:,:,-4].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors_Min[:,:,-4]):\n",
    "    ax.scatter(x, y, s = 100, color='orange', alpha=0.8)\n",
    "\n",
    "ax.set_xticks([1,5,9,13],['2','10','20','30'],fontsize=24)\n",
    "ax.set_xlabel('SNR',fontsize=32)\n",
    "ax.tick_params(axis='x', labelsize=24)\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Minimum NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Minimum SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b6402-e26a-4237-9e15-5e41ccd16887",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32490f42-b5f5-44e5-95d8-ed23ddea3a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pos = np.array([0, 2, 4,6])*2\n",
    "POSITIONS = g_pos\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors[:,:,-2],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='turquoise'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors[:,:,-2].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors[:,:,-2]):\n",
    "    ax.scatter(x, y, s = 100, color='paleturquoise', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+0.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors_Min[:,:,-2],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='cadetblue'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors_Min[:,:,-2].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors_Min[:,:,-2]):\n",
    "    ax.scatter(x, y, s = 100, color='darkturquoise', alpha=0.8)\n",
    "\n",
    "\n",
    "POSITIONS = g_pos+1.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors[:,:,-2],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='sandybrown'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors[:,:,-2].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors[:,:,-2]):\n",
    "    ax.scatter(x, y, s = 100, color='peachpuff', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+2\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors_Min[:,:,-2],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='darkorange'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors_Min[:,:,-2].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors_Min[:,:,-2]):\n",
    "    ax.scatter(x, y, s = 100, color='orange', alpha=0.8)\n",
    "\n",
    "ax.set_xticks([1,5,9,13],['2','10','20','30'],fontsize=24)\n",
    "ax.set_xlabel('SNR',fontsize=32)\n",
    "ax.tick_params(axis='x', labelsize=24)\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Minimum NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Minimum SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09132aa1-9779-4185-ab6c-57c626c7fb0b",
   "metadata": {},
   "source": [
    "## f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb507b6c-6dc1-49b3-bf2d-82b2825da4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "im = plt.imshow(1-NoiseEst2[...,-3],vmin=0,vmax=1,cmap='hot')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d9eda-08c3-44ee-b8fc-8fc4c0c378dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "im = plt.imshow(1-NoiseEst2_LS[...,-3],vmin=0,vmax=1,cmap='hot')\n",
    "cbar = plt.colorbar(im,fraction=0.035, pad=-0.1)\n",
    "cbar.ax.tick_params(labelsize=32)\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'FullSize_LS.pdf',bbox_inches='tight',format='pdf',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b509b-0e22-4d68-ac59-109d8aea055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "im = plt.imshow(1-NoiseEst_Min[...,-3],cmap='hot',vmin=0,vmax=1)\n",
    "#cbar.ax.tick_params(labelsize=14)\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DevSize.pdf',bbox_inches='tight',format='pdf',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a37c0-7ca3-4bd9-a5c4-883f93b11c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "im = plt.imshow(1-NoiseEst_LS_Min[...,-3],cmap='hot')\n",
    "cbar = plt.colorbar(im,fraction=0.035, pad=-0.1)\n",
    "cbar.ax.tick_params(labelsize=32)\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "if Save: plt.savefig((FigLoc+'DevSize_LS.pdf',bbox_inches='tight',format='pdf',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86641524-96d5-4cfc-8366-ad255f16f6ce",
   "metadata": {},
   "source": [
    "## g "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d63174d-6c42-4064-85a7-febc62f6381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "plt.imshow(np.flipud(NoiseEst2_CC[...,-1].T),cmap='gray')\n",
    "im = plt.imshow(np.flipud(NoiseEst2_CC[...,-2].T),cmap='hot',vmin=0,vmax=0.005)\n",
    "cbar = plt.colorbar(im,fraction=0.035, pad=0.01,format=ticker.FormatStrFormatter('%.e'))\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d92ec-0bdb-4ca1-a1ec-2e2d87ad5bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a65564-470b-4bb0-ba8e-6592150cec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "plt.imshow(np.flipud(NoiseEst2_Min_CC[...,-1].T),cmap='gray')\n",
    "im = plt.imshow(np.flipud(NoiseEst2_Min_CC[...,-2].T),cmap='hot',vmin=0,vmax=0.005)\n",
    "cbar = plt.colorbar(im,fraction=0.035, pad=0.01,format=ticker.FormatStrFormatter('%.e'))\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e9cad3-9bb8-4d36-a5b1-c23210b0ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "plt.imshow(np.flipud(NoiseEst2_LS_CC[...,-1].T),cmap='gray')\n",
    "im = plt.imshow(np.flipud(NoiseEst2_LS_CC[...,-2].T),cmap='hot',vmin=0,vmax=0.007)\n",
    "cbar = plt.colorbar(im,fraction=0.03, pad=0.01,format=ticker.FormatStrFormatter('%2.e'))\n",
    "cbar.ax.tick_params(labelsize=32)\n",
    "plt.axis('off')\n",
    "\n",
    "if Save: plt.savefig((FigLoc+'FullSize_CC_LS.pdf',bbox_inches='tight',format='pdf',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44828766-fd5d-4455-ab91-aee7fa9a1d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "plt.imshow(np.flipud(NoiseEst2_LS_Min_CC[...,-1].T),cmap='gray')\n",
    "im = plt.imshow(np.flipud(NoiseEst2_LS_Min_CC[...,-2].T),cmap='hot',vmin=0,vmax=0.007)\n",
    "cbar = plt.colorbar(im,fraction=0.03, pad=0.01,format=ticker.FormatStrFormatter('%.e'))\n",
    "cbar.ax.tick_params(labelsize=32)\n",
    "plt.axis('off')\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'MinSize_CC_LS.pdf',bbox_inches='tight',format='pdf',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56658a4-7ed7-4bbe-9fc2-b6811dc1485c",
   "metadata": {},
   "source": [
    "## h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d4f24-4b64-4ccd-a6d1-47764974c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pos = np.array([0,0.25,0.5])\n",
    "\n",
    "colors = ['lightseagreen','lightseagreen','lightseagreen']\n",
    "colors2 = ['paleturquoise','paleturquoise','paleturquoise']\n",
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "y_data = [1000*abs(Min_SBI[i][CMasks[i]][:,-3]-Full_SBI[i][CMasks[i]][:,-3]) for i in range(3)]\n",
    "\n",
    "\n",
    "BoxPlots2(y_data,g_pos,colors,colors2,ax)\n",
    "\n",
    "\n",
    "g_pos = np.array([2,2.25,2.5])\n",
    "colors = ['darkorange','darkorange','darkorange']\n",
    "colors2 = ['peachpuff','peachpuff','peachpuff']\n",
    "y_data = [1000*abs(Full_LS[i][CMasks[i]][:,-2]-Min_LS[i][CMasks[i]][:,-2]) for i in range(3)]\n",
    "\n",
    "BoxPlots2(y_data,g_pos,colors,colors2,ax)\n",
    "\n",
    "ax.set_xticks([0.25,2.25],['SBI Comp','NLLS Comp'],fontsize =24)\n",
    "\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'CC_3Indv_Comp.pdf',bbox_inches='tight',format='pdf',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e9aba5-b879-4860-b374-39add238e9f9",
   "metadata": {},
   "source": [
    "## i-j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d46fb-a848-44a2-9ed8-f63daf7b694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SBI_comp_Frac)\n",
    "g_pos = np.array([1.3])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(LS_comp_Frac)\n",
    "g_pos = np.array([1.9])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.xticks([1.3,1.9],['SBI','NNLS'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_SSIM_Frac.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8c881-8389-42be-9e74-4562f71ad090",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(1,1,figsize=(3.2,4.8))\n",
    "\n",
    "y_data = np.array(PrecFull_SBI_Frac)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_Frac)\n",
    "g_pos = np.array([1.1])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(PrecFull_NLLS_Frac)\n",
    "g_pos = np.array([1.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_Frac)\n",
    "g_pos = np.array([2.15])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([0.65,1.1,1.8,2.15],['Full','Red.','Full','Red.'],fontsize=32,rotation=90)\n",
    "\n",
    "x = np.arange(1.7,2.3,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_Frac)[~np.isnan(PrecFull_NLLS_Frac)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_Frac)[~np.isnan(PrecFull_NLLS_Frac)], 77)\n",
    "plt.fill_between(x,y1,y2,color='sandybrown',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.25,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_Frac)[~np.isnan(PrecFull_SBI_Frac)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_Frac)[~np.isnan(PrecFull_SBI_Frac)], 77)\n",
    "plt.fill_between(x,y1,y2,color='mediumturquoise',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "#ax1.set_xlim(0.3,2.8)\n",
    "ax1.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax1.set_yticks([0,0.1,0.2])\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_Prec_Frac.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c52039-1101-456c-b1ee-c58c9321b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SBI_comp_MD)\n",
    "g_pos = np.array([1.3])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(LS_comp_MD)\n",
    "g_pos = np.array([1.9])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.xticks([1.3,1.9],['SBI','NNLS'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_SSIM_MD.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279d3ea-7ccb-41f1-94db-a5030b539a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(1,1,figsize=(3.2,4.8))\n",
    "\n",
    "y_data = np.array(PrecFull_SBI_MD)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_MD)\n",
    "g_pos = np.array([1.1])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(PrecFull_NLLS_MD)\n",
    "g_pos = np.array([1.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_MD)\n",
    "g_pos = np.array([2.15])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([0.65,1.1,1.8,2.15],['Full','Red.','Full','Red.'],fontsize=32,rotation=90)\n",
    "\n",
    "x = np.arange(1.7,2.3,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_MD)[~np.isnan(PrecFull_NLLS_MD)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_MD)[~np.isnan(PrecFull_NLLS_MD)], 77)\n",
    "plt.fill_between(x,y1,y2,color='sandybrown',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.25,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_MD)[~np.isnan(PrecFull_SBI_MD)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_MD)[~np.isnan(PrecFull_SBI_MD)], 77)\n",
    "plt.fill_between(x,y1,y2,color='mediumturquoise',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "#ax1.set_xlim(0.3,2.8)\n",
    "ax1.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_Prec_MD.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1bd71e-7979-49b6-8b1b-e2d693b73098",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SBI_comp_Dp)\n",
    "g_pos = np.array([1.3])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(LS_comp_Dp)\n",
    "g_pos = np.array([1.9])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.xticks([1.3,1.9],['SBI','NNLS'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_SSIM_Dperp.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef20dfce-1d3f-4762-b1d5-c4aa41cf310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(1,1,figsize=(3.2,4.8))\n",
    "\n",
    "y_data = np.array(PrecFull_SBI_Dp)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_Dp)\n",
    "g_pos = np.array([1.1])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(PrecFull_NLLS_Dp)\n",
    "g_pos = np.array([1.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_Dp)\n",
    "g_pos = np.array([2.15])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([0.65,1.1,1.8,2.15],['Full','Red.','Full','Red.'],fontsize=32,rotation=90)\n",
    "\n",
    "x = np.arange(1.7,2.3,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_Dp)[~np.isnan(PrecFull_NLLS_Dp)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_Dp)[~np.isnan(PrecFull_NLLS_Dp)], 77)\n",
    "plt.fill_between(x,y1,y2,color='sandybrown',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.25,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_Dp)[~np.isnan(PrecFull_SBI_Dp)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_Dp)[~np.isnan(PrecFull_SBI_Dp)], 77)\n",
    "plt.fill_between(x,y1,y2,color='mediumturquoise',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "#ax1.set_xlim(0.3,2.8)\n",
    "ax1.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_Prec_Dperp.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d212b15-2e07-403d-8dc9-158cd9360a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
