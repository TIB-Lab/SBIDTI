{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d07518-aa92-4be5-ac9c-94fe32a61953",
   "metadata": {},
   "source": [
    "# Frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c00d60-fbcd-4716-8510-a1b6a795f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.legend_handler import HandlerPatch\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from dipy.denoise.localpca import mppca\n",
    "\n",
    "# Define font properties\n",
    "font = {\n",
    "    'family': 'sans-serif',  # Use sans-serif family\n",
    "    'sans-serif': ['Helvetica'],  # Specify Helvetica as the sans-serif font\n",
    "    'size': 14  # Set the default font size\n",
    "}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# Set tick label sizes\n",
    "plt.rc('ytick', labelsize=24)\n",
    "plt.rc('xtick', labelsize=24)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n",
    "# Customize axes spines and legend appearance\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from dwMRI_BasicFuncs import *\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import i0\n",
    "\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.special import j0, jv\n",
    "from scipy.optimize import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f82dba-764a-4d0c-8176-3a8c4c588b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_path = './Networks/'\n",
    "image_path   = '../Figures/'\n",
    "if not os.path.exists(image_path):\n",
    "    os.mkdir(image_path)\n",
    "NoiseLevels = [None,20,10,5,2]\n",
    "\n",
    "TrainingSamples = 50000\n",
    "InferSamples    = 500\n",
    "\n",
    "lower_abs,upper_abs = -0.07,0.07\n",
    "lower_rest,upper_rest = -0.015,0.015\n",
    "lower_S0 = 25\n",
    "upper_S0 = 2000\n",
    "Save = True\n",
    "\n",
    "TrueCol  = 'k'\n",
    "NoisyCol = 'k'\n",
    "WLSFit   = 'sandybrown'#np.array([225,190,106])/255\n",
    "SBIFit   = np.array([64,176,166])/255\n",
    "\n",
    "Errors_name = ['RK comparison','FA comparison','eig. comparison','Frobenius','Signal comparison','Correlation','Signal comparison','Correlation2']\n",
    "custom_prior = DTIPriorS0(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0)\n",
    "priorS0, *_ = process_prior(custom_prior) \n",
    "\n",
    "NLLSFit   = np.array([225,190,106])/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3952da9f-d878-499e-980b-17d8a7e15ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoxPlots(y_data, positions, colors, colors2, ax,hatch = False,scatter=False,scatter_alpha=0.5, **kwargs):\n",
    "\n",
    "    GREY_DARK = \"#747473\"\n",
    "    jitter = 0.02\n",
    "    # Clean data to remove NaNs column-wise\n",
    "    is_1d = np.ndim(y_data) == 1\n",
    "    if is_1d:\n",
    "        cleaned_data = y_data[~np.isnan(y_data)]\n",
    "    else:\n",
    "        cleaned_data = [np.asarray(d)[~np.isnan(d)] for d in y_data]\n",
    "    \n",
    "    # Define properties for the boxes (patch objects)\n",
    "    boxprops = dict(\n",
    "        linewidth=2, \n",
    "        facecolor='none',       # use facecolor for filling (set to 'none' if you want no fill)\n",
    "        edgecolor='turquoise'   # edgecolor for the outline\n",
    "    )\n",
    "\n",
    "    # Define properties for the medians (Line2D objects)\n",
    "    # Ensure GREY_DARK is defined (or replace it with a color string)\n",
    "    medianprops = dict(\n",
    "        linewidth=2, \n",
    "        color=GREY_DARK,\n",
    "        solid_capstyle=\"butt\"\n",
    "    )\n",
    "\n",
    "    # For whiskers, since they are Line2D objects, use 'color'\n",
    "    whiskerprops = dict(\n",
    "        linewidth=2, \n",
    "        color='turquoise'\n",
    "    )\n",
    "\n",
    "    bplot = ax.boxplot(\n",
    "        cleaned_data,\n",
    "        positions=positions, \n",
    "        showfliers=False,\n",
    "        showcaps = False,\n",
    "        medianprops=medianprops,\n",
    "        whiskerprops=whiskerprops,\n",
    "        boxprops=boxprops,\n",
    "        patch_artist=True,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Update the color of each box (these are patch objects)\n",
    "    for i, box in enumerate(bplot['boxes']):\n",
    "        box.set_edgecolor(colors[i])\n",
    "        if(hatch):\n",
    "            box.set_hatch('//')\n",
    "    \n",
    "    \n",
    "    # Update the color of the whiskers (each box has 2 whiskers)\n",
    "    for i in range(len(positions)):\n",
    "        bplot['whiskers'][2*i].set_color(colors[i])\n",
    "        bplot['whiskers'][2*i+1].set_color(colors[i])\n",
    "    \n",
    "    # If caps are enabled, update their color (Line2D objects)\n",
    "    if 'caps' in bplot:\n",
    "        for i, cap in enumerate(bplot['caps']):\n",
    "            cap.set_color(colors[i//2])  # two caps per box\n",
    "\n",
    "    if scatter:\n",
    "        if is_1d:\n",
    "            x_data = np.array([positions[0]] * len(cleaned_data))\n",
    "            x_jittered = x_data + stats.t(df=6, scale=jitter).rvs(len(x_data))\n",
    "            ax.scatter(x_jittered, cleaned_data, s=100, color=colors2, alpha=scatter_alpha)\n",
    "        else:\n",
    "            x_data = [np.array([positions[i]] * len(d)) for i, d in enumerate(cleaned_data)]\n",
    "            x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "            for x, y, c in zip(x_jittered, cleaned_data, colors2):\n",
    "                ax.scatter(x, y, s=100, color=c, alpha=scatter_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad02461-3bfa-46b5-9ff3-f9b05f202099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThinPatchHandler(HandlerPatch):\n",
    "    def create_artists(self, legend, orig_handle,\n",
    "                       xdescent, ydescent, width, height, fontsize, trans):\n",
    "        # mRKe the legendâ€patch only 20% as tall as a normal one\n",
    "        thin_height = height * 0.2\n",
    "        # center it vertically\n",
    "        y = ydescent + (height - thin_height) / 2\n",
    "        patch = Rectangle((xdescent, y),\n",
    "                          width, thin_height,\n",
    "                          facecolor=orig_handle.get_facecolor(),\n",
    "                          edgecolor=orig_handle.get_edgecolor(),\n",
    "                          hatch=orig_handle.get_hatch(),\n",
    "                          linewidth=orig_handle.get_linewidth(),\n",
    "                          transform=trans)\n",
    "        return [patch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38869d29-88a6-4c3f-9c43-659d06ac5028",
   "metadata": {},
   "outputs": [],
   "source": [
    "Save = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5717d-f673-4a1c-990b-fb6065c82c27",
   "metadata": {},
   "source": [
    "# Fig 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f7156-c9a1-4dd9-b191-b5fdde181fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S1/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.mRKedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac44e2c-26b0-4c99-b00e-7ad77bcc87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial = HemiSphere(xyz=bvecs[1:])\n",
    "hsph_initial20 = HemiSphere(xyz=bvecs[1:20])\n",
    "hsph_initial7 = HemiSphere(xyz=bvecs[1:7])\n",
    "hsph_updated,potentials = disperse_charges(hsph_initial,5000)\n",
    "hsph_updated20,potentials = disperse_charges(hsph_initial20,5000)\n",
    "hsph_updated7,potentials = disperse_charges(hsph_initial7,5000)\n",
    "\n",
    "gtabSimF = gradient_table(np.array([0]+[1000]*64).squeeze(), np.vstack([[0,0,0],hsph_updated.vertices]))\n",
    "gtabSim20 = gradient_table(np.array([0]+[1000]*19).squeeze(), np.vstack([[0,0,0],hsph_updated20.vertices]))\n",
    "gtabSim7 = gradient_table(np.array([0]+[1000]*6).squeeze(), np.vstack([[0,0,0],hsph_updated7.vertices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d87960f-e186-4c39-8557-6eeabea27a89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FullDat = []\n",
    "S0Full  = []\n",
    "DTIFull = []\n",
    "for i in tqdm(range(1,6)):\n",
    "    fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    print('maskdata.shape (%d, %d, %d, %d)' % maskdata.shape)\n",
    "    \n",
    "    TestData = maskdata[:, :, axial_middle, :]\n",
    "    FlatTD = TestData.reshape(maskdata.shape[0]*maskdata.shape[1],69)\n",
    "    FlatTD = FlatTD[FlatTD.sum(axis=-1)>0]\n",
    "    FlatTD = FlatTD[~np.array(FlatTD<0).any(axis=-1)]\n",
    "    FullDat.append(FlatTD)\n",
    "    # Fit the tensor model to the DWI data with return_S0_hat=True\n",
    "    tenmodel = dti.TensorModel(gtabHCP, return_S0_hat=True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(FlatTD)\n",
    "    DTIHCP = tenfit.quadratic_form\n",
    "    DTIFull.append(DTIHCP)\n",
    "    # Get the estimated S0_hat values\n",
    "    S0HCP = tenfit.S0_hat\n",
    "    S0Full.append(S0HCP)\n",
    "DTIFull = np.concatenate(DTIFull)\n",
    "FullDat = np.concatenate(FullDat)\n",
    "S0Full = np.hstack(S0Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c1680-7277-4406-925c-4237f2058431",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "Samples  = []\n",
    "DTISim = []\n",
    "S0Sim    = []\n",
    "# Define the lower and upper bounds\n",
    "\n",
    "lower_abs,upper_abs = -0.07,0.07\n",
    "lower_rest,upper_rest = -0.015,0.015\n",
    "lower_S0 = 25\n",
    "upper_S0 = 2000\n",
    "\n",
    "custom_prior = DTIPriorS0(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0)\n",
    "prior, *_ = process_prior(custom_prior) \n",
    "\n",
    "params = prior.sample([5000])\n",
    "for i in tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim.append(dt)\n",
    "    S0Sim.append(params[i,-1])\n",
    "    Samples.append([CustomSimulator(dt,gtabSimF, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples = np.array(Samples).squeeze()\n",
    "Samples = np.moveaxis(Samples, 0, -1)\n",
    "\n",
    "DTISim = np.array(DTISim)\n",
    "\n",
    "RKSim = [np.mean(np.linalg.eigh(B)[0]) for B in DTISim]\n",
    "RKHCP = [np.mean(np.linalg.eigh(B)[0]) for B in DTIFull]\n",
    "\n",
    "FASim = [FracAni(np.linalg.eigh(B)[0],m) for B,m in zip(DTISim,RKSim)]\n",
    "FAHCP = [FracAni(np.linalg.eigh(B)[0],m) for B,m in zip(DTIFull,RKHCP)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4fba7a-f6ce-49c3-ad9b-d0b553e5c398",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715f36e-0ba0-467c-8e69-25b93eb9b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(S0Sim,density=True,stacked=True,alpha=0.75,label='Simulated',color=SBIFit,bins=100)\n",
    "plt.hist(S0Full,density=True,stacked=True,alpha=0.75,label='HCP',color='gray',bins=100)\n",
    "plt.legend(fontsize=32,loc=1,bbox_to_anchor=(0.95,1.),columnspacing=0.3,handlelength=0.8,handletextpad=0.1)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.gca().yaxis.get_offset_text().set_fontsize(32)\n",
    "plt.yticks(fontsize=32)\n",
    "plt.xticks(fontsize=32)\n",
    "plt.xlim(0,2000)\n",
    "plt.xticks([0,1000])\n",
    "if Save: plt.savefig(FigLoc+'S0Dist.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea91123-144e-41a6-9be9-bd9b34e2e2a5",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e058b469-bebd-40db-a672-e4f42d1c34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(RKSim,density=True,stacked=True,label='Simulated samples',color=SBIFit,bins=100)\n",
    "plt.hist(RKHCP,density=True,stacked=True,alpha=0.75,label='HPC subset',color='gray',bins=100)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.gca().ticklabel_format(axis='x',style='sci',scilimits=(-1,1))\n",
    "plt.gca().xaxis.get_offset_text().set_visible(False)\n",
    "\n",
    "plt.yticks(fontsize=32)\n",
    "plt.xticks(fontsize=32)\n",
    "plt.xticks([0,0.003],['0','3e-3'])\n",
    "if Save: plt.savefig(FigLoc+'RKDist.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd90f1-915b-4c48-a194-95b59b40958a",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baeb45d-8795-4a46-be16-6e21964e0428",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(FASim,density=True,label='Simulated samples',color=SBIFit,bins=100)\n",
    "plt.hist(FAHCP,density=True,alpha=0.75,label='HPC subset',color='gray',bins=100)\n",
    "plt.yticks(fontsize=32)\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'FADist.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8e2ca-03f8-4fb9-a996-c22eb4786e8e",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35367838-cbed-4e74-9404-33b1478e99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(3,3,figsize=(12,12))\n",
    "ax = axs.ravel()\n",
    "ax[0].hist(DTISim[:,0,0],density=True,color=SBIFit,bins=100)\n",
    "ax[1].hist(DTISim[:,0,1],density=True,color=SBIFit,bins=100)\n",
    "ax[2].hist(DTISim[:,0,2],density=True,color=SBIFit,bins=100)\n",
    "ax[4].hist(DTISim[:,1,1],density=True,color=SBIFit,bins=100)\n",
    "ax[5].hist(DTISim[:,1,2],density=True,color=SBIFit,bins=100)\n",
    "ax[-1].hist(DTISim[:,2,2],density=True,color=SBIFit,bins=100)\n",
    "\n",
    "\n",
    "ax[0].hist(DTIFull[:,0,0],density=True,alpha=0.75,color='gray',bins=100)\n",
    "ax[1].hist(DTIFull[:,0,1],density=True,alpha=0.75,color='gray',bins=100)\n",
    "ax[2].hist(DTIFull[:,0,2],density=True,alpha=0.75,color='gray',bins=100)\n",
    "ax[4].hist(DTIFull[:,1,1],density=True,alpha=0.75,color='gray',bins=100)\n",
    "ax[5].hist(DTIFull[:,1,2],density=True,alpha=0.75,color='gray',bins=100)\n",
    "ax[-1].hist(DTIFull[:,2,2],density=True,alpha=0.75,color='gray',bins=100)\n",
    "ax[3].axis('off')\n",
    "ax[-2].axis('off')\n",
    "ax[-3].axis('off')\n",
    "\n",
    "for a in ax:\n",
    "    a.tick_params(axis='x', labelsize=32)\n",
    "    a.tick_params(axis='y', labelsize=32)\n",
    "    a.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    a.ticklabel_format(axis='x',style='sci',scilimits=(-1,1))\n",
    "    a.yaxis.get_offset_text().set_fontsize(32)\n",
    "ax[0].set_xticks([0,2.5e-3],['0','2.5e-3'])\n",
    "ax[1].set_xlim([-1.2e-3,1e-3])\n",
    "ax[1].set_xticks([-1e-3,0,1e-3],['-1e-3','0','1e-3'])\n",
    "\n",
    "ax[2].set_xlim([-8e-4,8e-4])\n",
    "ax[2].set_yticks([0,10000])\n",
    "ax[2].set_xticks([-5e-4,0,5e-4],['-5e-4','0','5e-4'])\n",
    "\n",
    "ax[4].set_xticks([0,2.5e-3],['0','2.5e-3'])\n",
    "\n",
    "ax[5].set_xlim([-1.2e-3,1e-3])\n",
    "ax[5].set_xticks([-1e-3,0,1e-3],['-1e-3','0','1e-3'])\n",
    "\n",
    "ax[-1].set_xticks([0,2.5e-3],['0','2.5e-3'])\n",
    "\n",
    "ax[0].set_xlabel('$D_{11}$',fontsize=32)\n",
    "ax[1].set_xlabel('$D_{12}$',fontsize=32)\n",
    "ax[2].set_xlabel('$D_{13}$',fontsize=32)\n",
    "ax[4].set_xlabel('$D_{22}$',fontsize=32)\n",
    "ax[5].set_xlabel('$D_{23}$',fontsize=32)\n",
    "ax[-1].set_xlabel('$D_{33}$',fontsize=32)\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'DTDist.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b34de9f-ecbf-483c-9310-f40c097504c2",
   "metadata": {},
   "source": [
    "# Fig 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc218abd-2583-4454-996f-ae2073249278",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S2/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.mkdir(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db4943-03bb-4151-a2d3-9c1e4e56293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 1\n",
    "fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "\n",
    "data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "# Get the indices of True values\n",
    "true_indices = np.argwhere(mask)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = true_indices.min(axis=0)\n",
    "max_coords = true_indices.max(axis=0)\n",
    "\n",
    "maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "FlatTD = TestData.reshape(maskdata.shape[0]*maskdata.shape[1],138)\n",
    "FlatTD = FlatTD[FlatTD[:,:69].sum(axis=-1)>0]\n",
    "FlatTD = FlatTD[~np.array(FlatTD<0).any(axis=-1)]\n",
    "\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabExt)\n",
    "tenfit = dkimodel.fit(FlatTD)\n",
    "DKIHCP = tenfit.kt\n",
    "DTIHCP = tenfit.lower_triangular()\n",
    "DKIFull = np.array(DKIHCP)\n",
    "DTIFull = np.array(DTIHCP)\n",
    "\n",
    "\n",
    "DTIFilt1 = DTIFull[(abs(DKIFull)<10).all(axis=1)]\n",
    "DKIFilt1 = DKIFull[(abs(DKIFull)<10).all(axis=1)]\n",
    "DTIFilt = DTIFilt1[(DKIFilt1>-3/7).all(axis=1)]\n",
    "DKIFilt = DKIFilt1[(DKIFilt1>-3/7).all(axis=1)]\n",
    "\n",
    "TrueMets = []\n",
    "FA       = []\n",
    "for (dt,kt) in tqdm(zip(DTIFilt,DKIFilt)):\n",
    "    TrueMets.append(DKIMetrics(dt,kt))\n",
    "    FA.append(FracAni(np.linalg.eigh(vals_to_mat(dt))[0],np.mean(np.linalg.eigh(vals_to_mat(dt))[0])))\n",
    "TrueMets = np.array(TrueMets)\n",
    "TrueFA = np.array(FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e8e941-6396-471f-9eaf-556fbf610088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full fit\n",
    "DT1_full,DT2_full = FitDT(DTIFilt,1)\n",
    "x4_full,R1_full,x2_full,R2_full = FitKT(DKIFilt,1)\n",
    "\n",
    "# LowFA Fit\n",
    "DT1_lfa,DT2_lfa = FitDT(DTIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "x4_lfa,R1_lfa,x2_lfa,R2_lfa = FitKT(DKIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "\n",
    "# HighFA Fit\n",
    "DT1_hfa,DT2_hfa = FitDT(DTIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "x4_hfa,R1_hfa,x2_hfa,R2_hfa = FitKT(DKIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "\n",
    "# UltraLowFA Fit\n",
    "DT1_ulfa,DT2_ulfa = FitDT(DTIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa = FitKT(DKIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "\n",
    "# HigherRK Fit\n",
    "DT1_hRK,DT2_hRK = FitDT(DTIFilt[TrueMets[:,1]>0.9,:],1)\n",
    "x4_hRK,R1_hRK,x2_hRK,R2_hRK = FitKT(DKIFilt[TrueMets[:,1]>0.9,:],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6496fc08-1edf-4618-b239-dad30e6c7f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "FullDat = []\n",
    "S0Full  = []\n",
    "DKIFull = []\n",
    "DTIFull = []\n",
    "for i in tqdm(range(1,6)):\n",
    "    fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    FlatTD = TestData.reshape(maskdata.shape[0]*maskdata.shape[1],138)\n",
    "    FlatTD = FlatTD[FlatTD[:,:69].sum(axis=-1)>0]\n",
    "    FlatTD = FlatTD[~np.array(FlatTD<0).any(axis=-1)]\n",
    "    FullDat.append(FlatTD)\n",
    "    \n",
    "    dkimodel = dki.DiffusionKurtosisModel(gtabExt)\n",
    "    tenfit = dkimodel.fit(FlatTD)\n",
    "    DKIHCP = tenfit.kt\n",
    "    DTIHCP = tenfit.lower_triangular()\n",
    "    DTIFull.append(DTIHCP)\n",
    "    DKIFull.append(DKIHCP)\n",
    "    # Get the estimated S0_hat values\n",
    "    S0HCP = tenfit.S0_hat\n",
    "    S0Full.append(S0HCP)\n",
    "DKIFull = np.concatenate(DKIFull)\n",
    "DTIFull = np.concatenate(DTIFull)\n",
    "\n",
    "DTIFilt_all = DTIFull[(abs(DKIFull)<10).all(axis=1)]\n",
    "DKIFilt_all = DKIFull[(abs(DKIFull)<10).all(axis=1)]\n",
    "DTIFilt_all = DTIFilt_all[(DKIFilt_all>-3/7).all(axis=1)]\n",
    "DKIFilt_all = DKIFilt_all[(DKIFilt_all>-3/7).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c5897e-70e3-4e9f-b089-4a9d076ffb10",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30531b8-0a11-45c8-8ae1-e77de1ecb7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(DTIFilt[:,5],bins=30,density=True,color=WLSFit,alpha=0.5,label='1 HCP Indv.')\n",
    "plt.hist(DTIFilt_all[:,5],bins=30,density=True,color=SBIFit,alpha=0.5,label='All HCP')\n",
    "plt.legend(fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.3,loc=1)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Comp1.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d758ef7-970b-478c-bdef-13147626d47a",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868472e5-b8d1-4e3c-9aa0-fb37eedc1423",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DTIFilt[:,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[:,5],bins=30,density=True,color=WLSFit)\n",
    "plt.hist(np.array(DTISim)[:,0,0],bins=30,density=True,alpha=0.5,color='gray')\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.0014,600,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Normal1.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "#DT_rest\n",
    "data = DTIFilt[:,1]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti2_fitted = stats.norm(loc=loc, scale=scale)\n",
    "\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.norm(loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[:,1],bins=30,density=True,color=WLSFit,label='HCP data')\n",
    "plt.hist(DTISim[:,1,0],bins=30,density=True,alpha=0.5,color='gray',label='DTI prior')\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit,label='stat. fit')\n",
    "plt.text(0.00011,2000,\"Normal, \\n $\\mu$ = {:.2f},\\n $\\sigma$ = {:.2e} \\n\".format(loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks([-0.0005,0,0.0005],[-5e-4,0,5e-4],fontsize=32)\n",
    "\n",
    "plt.legend(fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1,loc=1,bbox_to_anchor=(0.52,1))\n",
    "if Save: plt.savefig(FigLoc+'Normal2.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "data = DKIFilt[:,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "x4_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DKIFilt[:,0],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,x4_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(1,0.8,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Normal3.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Fitting R1\n",
    "data = DKIFilt[:,3]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "R1_fitted = stats.norm(loc,scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "plt.hist(DKIFilt[:,3],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,R1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.1,3,\"Normal, \\n $\\mu$ = {:.2f},\\n $\\sigma$ = {:.2e} \\n\".format(loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Normal4.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f4a7b-fbc1-49fe-bad8-27871f457d99",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007431cd-1d5a-4ed9-9168-863a38607926",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = TrueMets[:,-1]<0.3\n",
    "data = DTIFilt[mask,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[mask,5],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.0014,600,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'lowFA1.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "#DT_rest\n",
    "data = DTIFilt[mask,1]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti2_fitted = stats.norm(loc=loc, scale=scale)\n",
    "\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.norm(loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[mask,1],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.00011,2000,\"Normal, \\n $\\mu$ = {:.2f},\\n $\\sigma$ = {:.2e} \\n\".format(loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'lowFA2.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = DKIFilt[mask,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "x4_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DKIFilt[mask,0],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,x4_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(1,0.8,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'lowFA3.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Fitting R1\n",
    "data = DKIFilt[mask,3]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "R1_fitted = stats.norm(loc,scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "plt.hist(DKIFilt[mask,3],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,R1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.05,3,\"Normal, \\n $\\mu$ = {:.2f},\\n $\\sigma$ = {:.2e} \\n\".format(loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'lowFA4.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d353b47-54ca-4951-89ba-10f2fb7a7cc6",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9caa72a-4a4a-4fdc-bebf-82aed5509203",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = TrueMets[:,-1]>0.7\n",
    "data = DTIFilt[mask,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[mask,0],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.0008,1400,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=24)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'HighFA1.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "#DT_rest\n",
    "data = DTIFilt[mask,1]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti2_fitted = stats.norm(loc=loc, scale=scale)\n",
    "\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.norm(loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[mask,1],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.00013,1600,\"Normal, \\n $\\mu$ = {:.2f},\\n $\\sigma$ = {:.2e} \\n\".format(loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'HighFA2.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = DKIFilt[mask,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "x4_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DKIFilt[mask,0],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,x4_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(1.3,0.5,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=24)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'HighFA3.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Fitting R1\n",
    "data = DKIFilt[mask,3]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "R1_fitted = stats.norm(loc,scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "plt.hist(DKIFilt[mask,3],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,R1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.3,1,\"Normal, \\n $\\mu$ = {:.2f},\\n $\\sigma$ = {:.2e} \\n\".format(loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'HighFA4.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51568f26-57ee-4e97-99af-2c52716fc04f",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510db85f-81fc-4bae-beec-70f322f19770",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j=0,0\n",
    "mask = (TrueMets[:,-1]<0.7)*(TrueMets[:,-1]>0.3)\n",
    "plt.scatter(DTIFilt[mask,i],DKIFilt[mask,j],color=np.clip(np.array(col.to_rgb(WLSFit))-0.5,0,1),label='HCP data')\n",
    "mask = TrueMets[:,-1]>0.7\n",
    "plt.scatter(DTIFilt[mask,i],DKIFilt[mask,j],color=np.clip(np.array(col.to_rgb(WLSFit)),0,1),marker='v'\n",
    "            ,label='HCP data (KFA$>$0.7)')\n",
    "mask = TrueMets[:,-1]<0.3\n",
    "plt.scatter(DTIFilt[mask,i],DKIFilt[mask,j],color=np.clip(np.array(col.to_rgb(WLSFit))-0.3,0,1),marker='^'\n",
    "            ,label='HCP data (KFA$<$0.3)')\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.legend(fontsize=20,loc=1,bbox_to_anchor=(1,1),handlelength=0.4,handletextpad=0.4,markerscale=2)\n",
    "if Save: plt.savefig(FigLoc+'Scatter1Dat.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51508d26-9e1c-4351-90eb-5ee9a69ebb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j=9,0\n",
    "mask = (TrueMets[:,-1]<0.7)*(TrueMets[:,-1]>0.3)\n",
    "plt.scatter(DKIFilt[mask,i],DKIFilt[mask,j],color=np.clip(np.array(col.to_rgb(WLSFit))-0.5,0,1))\n",
    "mask = TrueMets[:,-1]>0.7\n",
    "plt.scatter(DKIFilt[mask,i],DKIFilt[mask,j],color=np.clip(np.array(col.to_rgb(WLSFit))+0.2,0,1),marker='v')\n",
    "mask = TrueMets[:,-1]<0.3\n",
    "plt.scatter(DKIFilt[mask,i],DKIFilt[mask,j],color=np.clip(np.array(col.to_rgb(WLSFit))-0.3,0,1),marker='^')\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "if Save: plt.savefig(FigLoc+'Scatter2Dat.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6edfbf-41af-42e9-bf41-748df654019f",
   "metadata": {},
   "source": [
    "# Fig 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f44910-2953-49e5-87c9-569b10a874b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S3/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.mkdir(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71389ea8-8df8-40f5-adf9-8f23dbc54eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdwi = './HCP_data/Pat'+str(1)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(1)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(1)+'/bvecs_1k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = [0]+selected_indices\n",
    "\n",
    "bvalsHCP7 = bvalsHCP[selected_indices]\n",
    "bvecsHCP7 = bvecsHCP[selected_indices]\n",
    "gtabHCP7 = gradient_table(bvalsHCP7, bvecsHCP7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f5262f-a67c-4c4f-808a-ffb7b0ab3435",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prior = DTIPriorS0Noise(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0,0,30)\n",
    "priorS0Noise, *_ = process_prior(custom_prior) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f93880-20e7-4398-b016-ba1de11b182d",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862d2cc-b316-422b-9395-77917a946f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIHCPFull.pickle\"):\n",
    "    with open(f\"{network_path}/DTIHCPFull.pickle\", \"rb\") as handle:\n",
    "        posterior2 = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    bvals = gtabHCP.bvals\n",
    "    bvecs = gtabHCP.bvecs\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm(range(TrainingSamples)):\n",
    "        params = priorS0.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        dt = ForceLowFA(dt)\n",
    "        Obs.append(CustomSimulator(dt,gtabHCP,params[-1],np.random.rand()*30 + 20))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior2 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{save_path}/DTIHCPFull.pickle\"):\n",
    "        with open(f\"{save_path}/DTIHCPFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior2, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406948f3-7ef2-4563-9297-33ba81cb2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel(i, j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posterior2.sample((1000,), x=maskdata[i,j,axial_middle, :],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbdd84-49b5-4081-a69d-5cd5e5cd7622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NoiseEst = np.zeros(list(ArrShape) + [7])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    NoiseEst[i, j,-2] = np.clip(NoiseEst[i, j,-2],0,100)\n",
    "    NoiseEst[i, j,-3] = np.clip(NoiseEst[i, j,-3],0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8aa42-676b-4530-91bc-39bdd75d9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(94):\n",
    "    for j in range(104):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1]])\n",
    "MD_SBI = np.zeros([94,104])\n",
    "FA_SBI = np.zeros([94,104])\n",
    "for i in range(94):\n",
    "    for j in range(104):\n",
    "        Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "        MD_SBI[i,j] = np.mean(Eigs)\n",
    "        FA_SBI[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "FA_SBI[np.isnan(FA_SBI)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9e241-7941-44a3-861b-5d961fa29caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(MD_SBI)\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot')\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_MD_US.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7219d1-a821-47eb-ae28-f07e50a5baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenmodel = dti.TensorModel(gtabHCP,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(maskdata[:,:,axial_middle])\n",
    "FAFull = dti.fractional_anisotropy(tenfit.evals)\n",
    "MDFull = dti.mean_diffusivity(tenfit.evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe321bb-bb37-4712-845c-63973a7f3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(MDFull)\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot')\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_NLLS_MD_US.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a73a6-d6b5-4ad5-9cb6-fc74597d6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MD_SBI.T-MDFull.T\n",
    "data[~mask.T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=np.nanmin(data), vcenter=0, vmax=np.nanmax(data))\n",
    "plt.imshow(data,cmap='seismic',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "ticks = [np.nanmin(data), 0, np.nanmax(data)]  # Adjust the number of ticks as needed\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_MD_Diff.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d3d58-445f-4f58-addb-727ccd32a703",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a4995-b6ac-49ec-aa98-305f602a049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIHCPMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTIHCPMin.pickle\", \"rb\") as handle:\n",
    "        posterior7_2 = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    bvals = gtabHCP.bvals\n",
    "    bvecs = gtabHCP.bvecs\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm(range(TrainingSamples)):\n",
    "        params = priorS0.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        dt = ForceLowFA(dt)\n",
    "        Obs.append(CustomSimulator(dt,gtabHCP7,params[-1],np.random.rand()*30 + 20))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior7_2 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{save_path}/DTIHCPMin.pickle\"):\n",
    "        with open(f\"{save_path}/DTIHCPMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7_2, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e4e60-3321-4295-bd60-97fbab2e9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel(i, j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posterior7_2.sample((1000,), x=maskdata[i,j,axial_middle, selected_indices],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e632824-3d68-4b4f-b54f-42e06b1a2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NoiseEst = np.zeros(list(ArrShape) + [7])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    NoiseEst[i, j,-2] = np.clip(NoiseEst[i, j,-2],0,100)\n",
    "    NoiseEst[i, j,-3] = np.clip(NoiseEst[i, j,-3],0,1)\n",
    "\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(94):\n",
    "    for j in range(104):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1]])\n",
    "MD_SBI7 = np.zeros([94,104])\n",
    "FA_SBI7 = np.zeros([94,104])\n",
    "for i in range(94):\n",
    "    for j in range(104):\n",
    "        Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "        MD_SBI7[i,j] = np.mean(Eigs)\n",
    "        FA_SBI7[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "FA_SBI7[np.isnan(FA_SBI7)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8363cde0-5a9d-4671-bc5b-ea0a00d02bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(MD_SBI7)\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot',vmin=0,vmax=0.005)\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_MD_7_US.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e1b3e-10a0-4189-9124-9df46178bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenmodel = dti.TensorModel(gtabHCP7,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(maskdata[:,:,axial_middle,selected_indices])\n",
    "FA7 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD7 = dti.mean_diffusivity(tenfit.evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1cedff-abe0-4923-978d-6d5ae2becbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(MD7)\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot',vmin=0,vmax=0.005)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_NLLS_MD_7_US.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424d381-5fa7-4e0d-81e2-6ae8635eb622",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs(MDFull.T-MD7.T)\n",
    "norm = TwoSlopeNorm(vmin=0,vcenter=0.00075, vmax=0.0015)\n",
    "data[~mask.T] = np.nan\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "\n",
    "ticks = [0, 0.001]\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'DTI_MDWLSErr_US.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = np.abs(MD_SBI.T-MD_SBI7.T)\n",
    "data[~mask.T] = np.nan\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'DTI_MDSBIErr.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f611aaa-6a4d-4d46-b1d1-13873bdf6f9e",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03895494-5724-4edb-ba56-9c628e8625f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(FA_SBI)\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot')\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_FA_US.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f2121-5c2c-4093-8c46-8967eedceebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(FAFull)\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot')\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_NLLS_FA_US.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb2c61-4a08-4ae4-b59b-f7f432094cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FA_SBI.T-FAFull.T\n",
    "data[~mask.T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=np.nanmin(data), vcenter=0, vmax=1)\n",
    "plt.imshow(data,cmap='seismic',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "ticks = [np.nanmin(data), 0, 1]  # Adjust the number of ticks as needed\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_FA_Diff.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbabcd82-8792-47f1-b86b-62b52c32a744",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89d364-2b4d-48ce-9e50-1b9dc23dddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(FA_SBI7)\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot')\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_FA_7_US.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801895c-a9bb-405b-a31c-6a42d2997206",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(FA7)\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot')\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_NLLS_FA_7_US.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922b507-f246-4a13-b705-43194d899754",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs(FAFull.T-FA7.T)\n",
    "norm = TwoSlopeNorm(vmin=0,vcenter=0.5, vmax=1)\n",
    "data[~mask.T] = np.nan\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "\n",
    "ticks = [0, 1]\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'DTI_FAWLSErr_US.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = np.abs(FA_SBI.T-FA_SBI7.T)\n",
    "data[~mask.T] = np.nan\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'DTI_FASBIErr_US.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f17832f-30a2-470d-9183-2d764e705d3a",
   "metadata": {},
   "source": [
    "# Fig 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7990e6-df58-4188-9175-72ee95352e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S4/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.mkdir(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4681831-5d8e-4a00-bd32-a645699feff3",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6bdfb3-0dc6-430a-9698-2a65ce5499bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial = HemiSphere(xyz=bvecs[1:])\n",
    "hsph_updated,_ = disperse_charges(hsph_initial,5000)\n",
    "bvecs = np.vstack([[0,0,0],hsph_updated.vertices])\n",
    "bvalsExt = np.hstack([bvals, 3000*np.ones_like(bvals)])\n",
    "bvecsExt = np.vstack([bvecs, bvecs])\n",
    "bvalsExt[65] = 0\n",
    "gtabSim = gradient_table(bvalsExt, bvecsExt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400fd7b4-ef5b-4b97-b1fb-4c358ad72f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "gtabSimDirs = []\n",
    "for i in range(10):\n",
    "    gtabSimDirs.append(gradient_table(np.array([0]+[1000]*6).squeeze(), np.vstack([[0,0,0],hsph_updated.vertices[np.random.choice(np.arange(1,64),6)]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e186fc-e623-4cc4-bf1d-bfbcfcdedceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "RandDirs = []\n",
    "for i in range(10):\n",
    "    RandDirs.append(np.random.choice(np.arange(1,64),6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848456c3-13fa-4dc3-a5dc-9efd0f69d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.linspace(0, 2 * np.pi, 100)     # Azimuthal angle\n",
    "v = np.linspace(0, np.pi / 2, 100)     # Polar angle â€” only upper hemisphere\n",
    "\n",
    "x1 = 2*np.outer(np.cos(u), np.sin(v))\n",
    "y1 = 2*np.outer(np.sin(u), np.sin(v))\n",
    "z1 = 2*np.outer(np.ones_like(u), np.cos(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c1937d-102e-4e12-be27-850c13dbd5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "plt.quiver([0]*6,[0]*6,[0]*6,2*gtabSimDirs[0].bvecs[1:,0],2*gtabSimDirs[0].bvecs[1:,1],2*gtabSimDirs[0].bvecs[1:,2],color='red',lw=3,label='Random')\n",
    "plt.quiver([0]*6,[0]*6,[0]*6,2*gtabSim7.bvecs[1:,0],2*gtabSim7.bvecs[1:,1],2*gtabSim7.bvecs[1:,2],color='k',lw=3,label='Optimal')\n",
    "ax.plot_surface(x1, y1, z1,  rstride=4, cstride=4, color=np.array([140, 100, 200]) / 255 , linewidth=0, alpha=0.25)\n",
    "plt.axis('off')\n",
    "ax.set_box_aspect((1.8, 1.8, 1))\n",
    "ax.view_init(elev=45., azim=-102)\n",
    "plt.legend(loc=2, bbox_to_anchor=(0.1,1.19),fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "if Save: plt.savefig(FigLoc+'DirectionsEg.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319f9c0-8e3e-4e34-90ff-e53aa0332abf",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2981b3-f201-41d3-98f2-cdd6cb37686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimMin.pickle\", \"rb\") as handle:\n",
    "        posterior7 = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSim7,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior7 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTISimMin.pickle\"):\n",
    "        with open(f\"{network_path}/DTISimMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1808d3-70b4-47af-97ed-3bc5e40609f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "posterior7_RD = []\n",
    "for kk in range(10):\n",
    "    if os.path.exists(f\"{network_path}/DTISimMin_Dir\"+str(kk)+\".pickle\"):\n",
    "        with open(f\"{network_path}/DTISimMin_Dir\"+str(kk)+\".pickle\", \"rb\") as handle:\n",
    "            posterior7_RD.append(pickle.load(handle))\n",
    "    else:\n",
    "        Obs = []\n",
    "        Par = []\n",
    "        for i in tqdm(range(TrainingSamples)):\n",
    "            params = priorNoise.sample()\n",
    "            dt = ComputeDTI(params)\n",
    "            dt = ForceLowFA(dt)\n",
    "            a = params[-1]\n",
    "            Obs.append(CustomSimulator(dt,gtabSiRKirs[kk],200,a))\n",
    "            Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "        \n",
    "        Obs = np.array(Obs)\n",
    "        Par = np.array(Par)\n",
    "        Obs = torch.tensor(Obs).float()\n",
    "        Par = torch.tensor(Par).float()\n",
    "        \n",
    "        # Create inference object. Here, NPE is used.\n",
    "        inference = SNPE(prior=priorNoise)\n",
    "        \n",
    "        # generate simulations and pass to the inference object\n",
    "        inference = inference.append_simulations(Par, Obs)\n",
    "        \n",
    "        # train the density estimator and build the posterior\n",
    "        density_estimator = inference.train()\n",
    "        posterior7_RD.append(inference.build_posterior(density_estimator))\n",
    "        if not os.path.exists(f\"{network_path}/DTISimMin_Dir\"+str(kk)+\".pickle\"):\n",
    "            with open(f\"{network_path}/DTISimMin_Dir\"+str(kk)+\".pickle\", \"wb\") as handle:\n",
    "                pickle.dump(posterior7_RD[-1], handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4e450-57ad-46c0-86a6-1141b819e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "Samples7  = []\n",
    "Samples7Dirs  = []\n",
    "DTISim = []\n",
    "\n",
    "params = priorS0.sample([500])\n",
    "for i in tqdm(range(500)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim.append(dt)\n",
    "    Samples7.append([CustomSimulator(dt,gtabSim7, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    temp_Samples = []\n",
    "    for k in range(10):\n",
    "        temp_Samples.append([CustomSimulator(dt,gtabSimDirs[k], S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    temp_Samples = np.array(temp_Samples).squeeze()\n",
    "    temp_Samples = np.moveaxis(temp_Samples, 0, -1)\n",
    "    Samples7Dirs.append(temp_Samples)\n",
    "Samples7 = np.array(Samples7).squeeze()\n",
    "Samples7 = np.moveaxis(Samples7, 0, -1)\n",
    "Samples7Dirs = np.array(Samples7Dirs)\n",
    "Samples7Dirs = np.moveaxis(Samples7Dirs, 0, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ae2b5-be1f-42fd-8198-22d4b0dfc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "Error7 = []\n",
    "NoiseApprox7 = []\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tparams = mat_to_vals(DTISim[i])\n",
    "        tObs = Samples7[k,:,i]\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSim7, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posterior7.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSim7,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApprox7.append(ENoise)\n",
    "    Error7.append(ErrorN2)\n",
    "\n",
    "NoiseApprox7 = np.array(NoiseApprox7)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeae23a-ebd2-490f-85ba-8e4655037a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "Error7_RD = []\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    for i in range(200):\n",
    "        tparams = mat_to_vals(DTISim[i])\n",
    "        tObs = Samples7Dirs[k,:,i,0]\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSim, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)[:7]\n",
    "        posterior_samples_1 = posterior7_RD[0].sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSimDirs[0],true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    Error7_RD.append(ErrorN2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab507409-a47e-4cfe-9079-355285990529",
   "metadata": {},
   "outputs": [],
   "source": [
    "Error_s = []\n",
    "for k,gtab,Samps,DTIS in zip([7,7],[gtabSimDirs[0],gtabSim7],[Samples7Dirs[...,0],Samples7],[DTISim,DTISim]):\n",
    "    tenmodel = dti.TensorModel(gtab,fit_method='NLLS')\n",
    "    Error_n = []\n",
    "    for S,Noise in zip(Samps,NoiseLevels):\n",
    "        Error = []\n",
    "        for i in range(500):\n",
    "            tenfit = tenmodel.fit(S[:,i])\n",
    "            tensor_vals = dti.lower_triangular(tenfit.quadratic_form)\n",
    "            DT_test = vals_to_mat(tensor_vals)\n",
    "            Error.append(Errors(DT_test,DTIS[i],gtab,Samps[0][:,i],S[:,i]))\n",
    "        Error_n.append(Error)\n",
    "    Error_s.append(Error_n)\n",
    "Error_s = np.array(Error_s)\n",
    "Error_s = np.swapaxes(Error_s,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19eda9d-cdda-42a5-92d0-316a372a3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,axs = plt.subplots(2,1,figsize=(4.5,6))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,E2,t) in enumerate(zip(ax,np.array(Error7).T,np.array(Error7_RD).T,Errors_name)):\n",
    "    plt.sca(a) \n",
    "    g_pos = np.array([1,3,5,7])\n",
    "    colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "    colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "    BoxPlots(E[:,1:].T,g_pos,colors,colors2,a,widths=0.3,scatter=False)\n",
    "\n",
    "\n",
    "    g_pos = np.array([1.3,3.3,5.3,7.3])\n",
    "\n",
    "    colors = ['mediumturquoise','mediumturquoise','mediumturquoise','mediumturquoise']\n",
    "    colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "    BoxPlots(E2[:,1:].T,g_pos,colors,colors2,a,widths=0.3,scatter=False,hatch=True)\n",
    "\n",
    "    g_pos = np.array([1.6,3.6,5.6,7.6])\n",
    "    colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "    colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "    BoxPlots(Error_s[1:,1,:,ll],g_pos,colors,colors2,a,widths=0.3,scatter=False)\n",
    "\n",
    "    g_pos = np.array([1.9,3.9,5.9,7.9])\n",
    "    colors = ['burlywood','burlywood','burlywood','burlywood']\n",
    "    colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "    BoxPlots(Error_s[1:,0,:,ll],g_pos,colors,colors2,a,widths=0.3,scatter=False,hatch='x')\n",
    "        \n",
    "    plt.sca(a)\n",
    "    plt.xticks([1.45, 3.45, 5.45, 7.45,], NoiseLevels[1:],fontsize=32)\n",
    "    #ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.yticks(fontsize=32)\n",
    "    if(ll==1):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(-0.05,1.15),\n",
    "                   fontsize=28,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if ll == 0:\n",
    "        handles = [\n",
    "            Line2D([0], [0],\n",
    "                   color='sandybrown', lw=4,\n",
    "                   label='NLLS'),\n",
    "            Rectangle((0,0), 1, 0.2,\n",
    "                            facecolor='peachpuff',\n",
    "                            edgecolor='burlywood',\n",
    "                            hatch='///',\n",
    "                            label='Rand. Dir')\n",
    "        ]\n",
    "        plt.legend(handles=handles,handler_map={Rectangle: ThinPatchHandler()},\n",
    "                   loc=2,\n",
    "                   bbox_to_anchor=(-0.05, 1.15),\n",
    "                   fontsize=28,\n",
    "                   columnspacing=0.3,\n",
    "                   handlelength=0.6,\n",
    "                   handletextpad=0.3,\n",
    "                  labelspacing=0.0 )\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'DirectionErrors.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f6800-46d7-441d-9d96-4149dc4b29f2",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec506cc-3cdc-4562-abf7-abd2e718bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "Sensitivity = []\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    temp = []\n",
    "    for i in tqdm(range(500)):\n",
    "        tObs = Samples7[k,:,i]\n",
    "        posterior_samples_1 = posterior7.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        evals = np.linalg.eigh(mat_guess)[0]\n",
    "        RK_guess = np.mean(evals)\n",
    "        FA_guess = FracAni(evals,RK_guess)\n",
    "        temp2 = []\n",
    "        for kk in range(10):\n",
    "            tObs2 = Samples7Dirs[k,:,i,kk]\n",
    "            posterior_samples_1 = posterior7_RD[kk].sample((InferSamples,), x=tObs2,show_progress_bars=False)\n",
    "            mat_guess2 = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "            evals2 = np.linalg.eigh(mat_guess2)[0]\n",
    "            RK_guess2 = np.mean(evals2)\n",
    "            FA_guess2 = FracAni(evals2,RK_guess2)\n",
    "            temp2.append((np.abs(RK_guess-RK_guess2),np.abs(FA_guess-FA_guess2)))\n",
    "        temp.append(temp2)\n",
    "    Sensitivity.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3cf67-cb19-4ec2-b8c8-620e7d9a3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "Sensitivity_NLLS = []\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    temp = []\n",
    "    tenmodel = dti.TensorModel(gtabSim7,fit_method='NLLS')\n",
    "    for i in tqdm(range(500)):\n",
    "        tObs = Samples7[k,:,i]\n",
    "        tenfit = tenmodel.fit(tObs)\n",
    "        RK_guess = tenfit.md\n",
    "        FA_guess = tenfit.fa\n",
    "        temp2 = []\n",
    "        for kk in range(10):\n",
    "            tObs2 = Samples7Dirs[k,:,i,kk]\n",
    "            tenmodel2 = dti.TensorModel(gtabSimDirs[kk],fit_method='NLLS')\n",
    "            tenfit = tenmodel2.fit(tObs2)\n",
    "            RK_guess2 = tenfit.md\n",
    "            FA_guess2 = tenfit.fa\n",
    "            temp2.append((np.abs(RK_guess-RK_guess2),np.abs(FA_guess-FA_guess2)))\n",
    "        temp.append(temp2)\n",
    "    Sensitivity_NLLS.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2fcca4-20fe-47e8-a405-7d8b31101fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensitivity = np.array(Sensitivity)\n",
    "Sensitivity_NLLS = np.array(Sensitivity_NLLS)\n",
    "\n",
    "S_RK = Sensitivity[...,0]\n",
    "S_RK_NLLS = Sensitivity_NLLS[...,0]\n",
    "\n",
    "S_FA = Sensitivity[...,1]\n",
    "S_FA_NLLS = Sensitivity_NLLS[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c4673-dff9-4ea9-834b-225a05bf59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(2,1,figsize=(4.5,6))\n",
    "ax = axs.ravel()\n",
    "plt.sca(ax[0])\n",
    "for i in range(1,5):\n",
    "    g_pos = np.array([1,2,3,4])\n",
    "    colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "    colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "    BoxPlots(S_RK[1:,:,:].mean(axis=1),g_pos,colors,colors2,ax[0],widths=0.3,scatter=True)\n",
    "    g_pos = np.array([1.3,2.3,3.3,4.3])\n",
    "    colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "    colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "    BoxPlots(S_RK_NLLS[1:,:,:].mean(axis=1),g_pos,colors,colors2,ax[0],widths=0.3,scatter=True)\n",
    "    \n",
    "    plt.semilogy()\n",
    "    plt.xticks([1.15,2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    plt.yticks(rotation=90,va='center')\n",
    "    #plt.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.grid()\n",
    "\n",
    "plt.sca(ax[1])\n",
    "for i in range(1,5):\n",
    "    g_pos = np.array([1,2,3,4])\n",
    "    colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "    colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "    BoxPlots(S_FA[1:,:,:].mean(axis=1),g_pos,colors,colors2,ax[1],widths=0.3,scatter=True)\n",
    "    g_pos = np.array([1.3,2.3,3.3,4.3])\n",
    "    colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "    colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "    BoxPlots(S_FA_NLLS[1:,:,:].mean(axis=1),g_pos,colors,colors2,ax[1],widths=0.3,scatter=True)\n",
    "    \n",
    "    plt.xticks([1.15,2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    #plt.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'DirectionComp.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03862e27-aacb-427d-9a28-5a346ad9853a",
   "metadata": {},
   "source": [
    "## f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee7d57-12b6-433d-a24d-81d4ef77bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdwi = './HCP_data/Pat'+str(1)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(1)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(1)+'/bvecs_1k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=True, dilate=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e428b1-91a4-4b70-9ae1-e17e30447084",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "RandomDir_HCP = np.random.choice(np.arange(1,69),6)\n",
    "RandomDir_HCP = np.insert(RandomDir_HCP,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27dbec2-bd0e-4dc3-a209-044339251690",
   "metadata": {},
   "outputs": [],
   "source": [
    "bvalsHCP7 = bvalsHCP[RandomDir_HCP]\n",
    "bvecsHCP7 = bvecsHCP[RandomDir_HCP]\n",
    "gtabHCP7 = gradient_table(bvalsHCP7, bvecsHCP7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f0d20a-6569-4340-ae4c-7a0156ad16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prior = DTIPriorS0Noise(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0,0,30)\n",
    "priorS0Noise, *_ = process_prior(custom_prior) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a46981-02da-464b-8820-df74b0e73e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIHCPMin_RD.pickle\"):\n",
    "    with open(f\"{network_path}/DTIHCPMin_RD.pickle\", \"rb\") as handle:\n",
    "        posterior7_2 = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    bvals = gtabHCP.bvals\n",
    "    bvecs = gtabHCP.bvecs\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm(range(TrainingSamples)):\n",
    "        params = priorS0.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        dt = ForceLowFA(dt)\n",
    "        Obs.append(CustomSimulator(dt,gtabHCP7,params[-1],np.random.rand()*30 + 20))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior7_2 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIHCPMin_RD.pickle\"):\n",
    "        with open(f\"{snetwork_path}/DTIHCPMin_RD.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7_2, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f05f11-1cb9-43b2-9a77-8be481717da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel(i, j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posterior7_2.sample((1000,), x=maskdata[i,j,axial_middle, RandomDir_HCP],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b9d4d-e93a-4fda-88f2-7f7b59100c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NoiseEst = np.zeros(list(ArrShape) + [7])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    NoiseEst[i, j,-2] = np.clip(NoiseEst[i, j,-2],0,100)\n",
    "    NoiseEst[i, j,-3] = np.clip(NoiseEst[i, j,-3],0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b302fbb-3fe5-42c6-b444-7dd6d06f16a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(55):\n",
    "    for j in range(64):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1]])\n",
    "RK_SBI7 = np.zeros([55,64])\n",
    "FA_SBI7 = np.zeros([55,64])\n",
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "        RK_SBI7[i,j] = np.mean(Eigs)\n",
    "        FA_SBI7[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "FA_SBI7[np.isnan(FA_SBI7)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62276d8-b097-4327-84e7-bb38575fb6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenmodel = dti.TensorModel(gtabHCP7,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(maskdata[:,:,axial_middle,RandomDir_HCP])\n",
    "FA7 = dti.fractional_anisotropy(tenfit.evals)\n",
    "RK7 = dti.mean_diffusivity(tenfit.evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ee4b5-722b-49f4-90f5-793efbdb44f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            FA7[i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b97e7-e15c-4598-a5e7-8b447d25bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(RK_SBI7)\n",
    "\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot')\n",
    "plt.axis('off')\n",
    "vmin, vmax = img.get_clim()\n",
    "#plt.colorbar()\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_RK_7_RD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f328b-b4a2-4e55-99f8-5c9081105c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(RK7)\n",
    "\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot',vmin=vmin,vmax=vmax)\n",
    "cbar = plt.colorbar(fraction=0.032, pad=0.04)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'HCP_NLLS_RK_7_RD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86728810-05b3-4e15-86fa-81ab76846a71",
   "metadata": {},
   "source": [
    "## g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff2be4-bfee-4712-8ef4-a3c5f59cb66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(FA_SBI7)\n",
    "\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_FA_7_RD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea14fdc-3939-4388-8d2f-2627ce7579da",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(FA7)\n",
    "\n",
    "temp[~mask] = math.nan\n",
    "img = plt.imshow(temp.T,cmap='hot',vmin=0,vmax=1)\n",
    "cbar = plt.colorbar(fraction=0.032, pad=0.04)\n",
    "plt.axis('off')\n",
    "cbar.set_ticks([0,0.2,0.4,0.6,0.8,1])\n",
    "if Save: plt.savefig(FigLoc+'HCP_NLLS_FA_7_RD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a524ed-dfb4-4e5e-badf-2af39d14fe02",
   "metadata": {},
   "source": [
    "# Fig 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28a6b6-3a3f-4831-906a-c7c50d822ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S3/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.mkdir(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb65be2-68c5-4907-b093-9c823c293956",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial = HemiSphere(xyz=bvecs[1:])\n",
    "hsph_initial20 = HemiSphere(xyz=bvecs[1:20])\n",
    "hsph_initial7 = HemiSphere(xyz=bvecs[1:7])\n",
    "hsph_updated,potentials = disperse_charges(hsph_initial,5000)\n",
    "hsph_updated20,potentials = disperse_charges(hsph_initial20,5000)\n",
    "hsph_updated7,potentials = disperse_charges(hsph_initial7,5000)\n",
    "\n",
    "gtabSimF = gradient_table(np.array([0]+[1000]*64).squeeze(), np.vstack([[0,0,0],hsph_updated.vertices]))\n",
    "gtabSim20 = gradient_table(np.array([0]+[1000]*19).squeeze(), np.vstack([[0,0,0],hsph_updated20.vertices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba47671-bb16-4f80-87eb-41c6a6baade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "Samples  = []\n",
    "DTISim = []\n",
    "S0Sim    = []\n",
    "\n",
    "params = priorS0.sample([500])\n",
    "for i in tqdm(range(500)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim.append(dt)\n",
    "    S0Sim.append(params[i,-1])\n",
    "    Samples.append([CustomSimulator(dt,gtabSimF, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples = np.array(Samples).squeeze()\n",
    "Samples = np.moveaxis(Samples, 0, -1)\n",
    "\n",
    "Samples20  = []\n",
    "DTISim20 = []\n",
    "S0Sim20    = []\n",
    "\n",
    "params = priorS0.sample([500])\n",
    "for i in tqdm(range(500)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim20.append(dt)\n",
    "    S0Sim20.append(params[i,-1])\n",
    "    Samples20.append([CustomSimulator(dt,gtabSim20, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples20 = np.array(Samples20).squeeze()\n",
    "Samples20 = np.moveaxis(Samples20, 0, -1)\n",
    "\n",
    "Samples7  = []\n",
    "DTISim7 = []\n",
    "S0Sim7    = []\n",
    "\n",
    "params = priorS0.sample([500])\n",
    "for i in tqdm(range(500)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim7.append(dt)\n",
    "    S0Sim7.append(params[i,-1])\n",
    "    Samples7.append([CustomSimulator(dt,gtabSim7, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples7 = np.array(Samples7).squeeze()\n",
    "Samples7 = np.moveaxis(Samples7, 0, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90beb6e6-d91c-4460-b143-557fec76d0fb",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f5be63-7a03-47b7-81ea-5b2b324996de",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimFull.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSimF,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{save_path}/DTISimFull.pickle\"):\n",
    "        with open(f\"{save_path}/DTISimFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe5a2b-c492-4a8e-acb2-c55f717d6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "ErrorFull = []\n",
    "NoiseApproxFull = []\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim[i])\n",
    "        tObs = Samples[k,:,i]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSimF, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        mat_guess = clip_negative_eigenvalues(mat_guess)\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSimF,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApproxFull.append(ENoise)\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "NoiseApproxFull = np.array(NoiseApproxFull)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e55e5-6499-4009-82a8-d10e1c5daf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "k,gtab,Samps,DTIS = 65,gtabSimF,Samples,DTISim\n",
    "tenmodel = dti.TensorModel(gtab,fit_method='NLLS')\n",
    "Error_n = []\n",
    "for S,Noise in zip(Samps,NoiseLevels):\n",
    "    Error = []\n",
    "    for i in range(500):\n",
    "        tenfit = tenmodel.fit(S[:,i])\n",
    "        tensor_vals = dti.lower_triangular(tenfit.quadratic_form)\n",
    "        DT_test = vals_to_mat(tensor_vals)\n",
    "        Error.append(Errors(DT_test,DTIS[i],gtab,Samps[0][:,i],S[:,i]))\n",
    "    Error_n.append(Error)\n",
    "Error_n = np.array(Error_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91891b60-9c81-4bca-8828-039adb234819",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,6,figsize=(27,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(ErrorFull).T[2:],Errors_name[2:])):\n",
    "    y_data = E[:,1:]\n",
    "    g_pos = np.array([1.3,2.3,3.3,4.3])\n",
    "    colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "    colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "    \n",
    "    BoxPlots(y_data.T,g_pos,colors,colors2,a,widths=0.3,scatter=False)\n",
    "    y_data = Error_n[1:,:,ll+2].T\n",
    "    g_pos = np.array([1,2,3,4])\n",
    "    colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "    colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "    \n",
    "    BoxPlots(y_data.T,g_pos,colors,colors2,a,widths=0.3,scatter=False)\n",
    "    plt.sca(a)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    plt.yticks(fontsize=32)\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    if(ll==1):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.05),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(ll==0):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=WLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'SiRKatDTIErrors2.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2eb9cd-9160-413c-9fb9-bad9712ed111",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5c712-7d21-4730-b1f7-16eea2f83eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimMid.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimMid.pickle\", \"rb\") as handle:\n",
    "        posterior20 = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSim20,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior20 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTISimMid.pickle\"):\n",
    "        with open(f\"{network_path}/DTISimMid.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior20, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994437c8-26dd-4807-9614-8f95d53eaa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "Samples20  = []\n",
    "DTISim20 = []\n",
    "S0Sim20    = []\n",
    "\n",
    "params = priorS0.sample([500])\n",
    "for i in tqdm(range(500)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim20.append(dt)\n",
    "    S0Sim20.append(params[i,-1])\n",
    "    Samples20.append([CustomSimulator(dt,gtabSim20, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples20 = np.array(Samples20).squeeze()\n",
    "Samples20 = np.moveaxis(Samples20, 0, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce290847-ba9d-4d7b-b1b1-032e4896f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "Error20 = []\n",
    "NoiseApprox20 = []\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim20[i])\n",
    "        tObs = Samples20[k,:,i]\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSim20, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posterior20.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSim20,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApprox20.append(ENoise)\n",
    "    Error20.append(ErrorN2)\n",
    "\n",
    "NoiseApprox20 = np.array(NoiseApprox20)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73c5d5-aee4-4ee8-83a9-b6b3c68e3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "k,gtab,Samps,DTIS = 20,gtabSim20,Samples20,DTISim20\n",
    "tenmodel = dti.TensorModel(gtab,fit_method='NLLS')\n",
    "Error_n = []\n",
    "for S,Noise in zip(Samps,NoiseLevels):\n",
    "    Error = []\n",
    "    for i in range(500):\n",
    "        tenfit = tenmodel.fit(S[:,i])\n",
    "        tensor_vals = dti.lower_triangular(tenfit.quadratic_form)\n",
    "        DT_test = vals_to_mat(tensor_vals)\n",
    "        Error.append(Errors(DT_test,DTIS[i],gtab,Samps[0][:,i],S[:,i]))\n",
    "    Error_n.append(Error)\n",
    "Error_n = np.array(Error_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4cbad1-4672-4f89-820c-7126359dd32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,4,figsize=(18,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(Error20).T,Errors_name)):\n",
    "    y_data = E[:,1:]\n",
    "    g_pos = np.array([1.3,2.3,3.3,4.3])\n",
    "    colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "    colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "    \n",
    "    BoxPlots(y_data.T,g_pos,colors,colors2,a,widths=0.3,scatter=False)\n",
    "    y_data = Error_n[1:,:,ll].T\n",
    "    g_pos = np.array([1,2,3,4])\n",
    "    colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "    colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "    \n",
    "    BoxPlots(y_data.T,g_pos,colors,colors2,a,widths=0.3,scatter=False)\n",
    "    \n",
    "    plt.sca(a) \n",
    "    if(ll>3):\n",
    "        plt.xlabel('SNR', fontsize=24)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    plt.yticks(fontsize=32)\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'SiRKatDTIErrors1_20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65446b-91d4-48b6-b490-9860223c6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,4,figsize=(18,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(Error20).T[4:],Errors_name[4:])):\n",
    "    y_data = E[:,1:]\n",
    "    g_pos = np.array([1.3,2.3,3.3,4.3])\n",
    "    colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "    colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "    \n",
    "    BoxPlots(y_data.T,g_pos,colors,colors2,a,widths=0.3,scatter=False)\n",
    "    y_data = Error_n[1:,:,ll+4].T\n",
    "    g_pos = np.array([1,2,3,4])\n",
    "    colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "    colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "    \n",
    "    BoxPlots(y_data.T,g_pos,colors,colors2,a,widths=0.3,scatter=False)\n",
    "    plt.sca(a)\n",
    "    if(ll == 0 or ll == 2):\n",
    "        plt.yticks([0,20])\n",
    "    if(ll>3):\n",
    "        plt.xlabel('SNR', fontsize=24)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "\n",
    "    plt.yticks(fontsize=32)\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'SiRKatDTIErrors2_20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae709e-1758-48d7-810d-2d18908be2a9",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c270404d-75c7-4710-9260-753141c0546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimMin.pickle\", \"rb\") as handle:\n",
    "        posterior7 = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSim7,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior7 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTISimMin.pickle\"):\n",
    "        with open(f\"{network_path}/DTISimMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e05543a-05b8-46fc-90e3-c4024cb63ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "Error7 = []\n",
    "NoiseApprox7 = []\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim7[i])\n",
    "        tObs = Samples7[k,:,i]\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSim7, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posterior7.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSim7,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApprox7.append(ENoise)\n",
    "    Error7.append(ErrorN2)\n",
    "\n",
    "NoiseApprox7 = np.array(NoiseApprox7)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54572578-688c-49f3-aa04-402b4c29cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "k,gtab,Samps,DTIS = 7,gtabSim7,Samples7,DTISim7\n",
    "tenmodel = dti.TensorModel(gtab,fit_method='NLLS')\n",
    "Error_n = []\n",
    "for S,Noise in zip(Samps,NoiseLevels):\n",
    "    Error = []\n",
    "    for i in range(500):\n",
    "        tenfit = tenmodel.fit(S[:,i])\n",
    "        tensor_vals = dti.lower_triangular(tenfit.quadratic_form)\n",
    "        DT_test = vals_to_mat(tensor_vals)\n",
    "        Error.append(Errors(DT_test,DTIS[i],gtab,Samps[0][:,i],S[:,i]))\n",
    "    Error_n.append(Error)\n",
    "Error_n = np.array(Error_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e4296-1dcd-4bf3-94fd-2b66aeebafd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,6,figsize=(27,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(ErrorFull).T[2:],Errors_name[2:])):\n",
    "    y_data = E[:,1:]\n",
    "    g_pos = np.array([1.3,2.3,3.3,4.3])\n",
    "    colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "    colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "    \n",
    "    BoxPlots(y_data.T,g_pos,colors,colors2,a,widths=0.3,scatter=False)\n",
    "    y_data = Error_n[1:,:,ll+2].T\n",
    "    g_pos = np.array([1,2,3,4])\n",
    "    colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "    colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "    \n",
    "    BoxPlots(y_data.T,g_pos,colors,colors2,a,widths=0.3,scatter=False)\n",
    "    plt.sca(a)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    plt.yticks(fontsize=32)\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'SiRKatDTIErrors2_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c0a00-7af6-4103-9f93-c96ba76688d5",
   "metadata": {},
   "source": [
    "# Fig 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6940d0-7ff5-40e4-8321-9fc4daf08a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S4/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.mRKedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26b3ba-3668-42f7-9960-b9484da68536",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT1_hRK,DT2_hRK = FitDT(DTIFilt[TrueMets[:,1]>0.99,:],1)\n",
    "x4_hRK,R1_hRK,x2_hRK,R2_hRK = FitKT(DKIFilt[TrueMets[:,1]>0.99,:],1)\n",
    "DT5,KT5 = GenDTKT([DT1_hRK,DT2_hRK],[x4_hRK,R1_hRK,x2_hRK,R2_hRK],12,300)\n",
    "ParMets = []\n",
    "for d,k in tqdm(zip(DT5,KT5)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest5 = np.array(ParMets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2681d-fe9e-4a5b-94aa-1f644aac4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full fit\n",
    "DT1_full,DT2_full = FitDT(DTIFilt,1)\n",
    "x4_full,R1_full,x2_full,R2_full = FitKT(DKIFilt,1)\n",
    "\n",
    "# LowFA Fit\n",
    "DT1_lfa,DT2_lfa = FitDT(DTIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "x4_lfa,R1_lfa,x2_lfa,R2_lfa = FitKT(DKIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "\n",
    "# HighFA Fit\n",
    "DT1_hfa,DT2_hfa = FitDT(DTIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "x4_hfa,R1_hfa,x2_hfa,R2_hfa = FitKT(DKIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "\n",
    "# UltraLowFA Fit\n",
    "DT1_ulfa,DT2_ulfa = FitDT(DTIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa = FitKT(DKIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "\n",
    "# HigherRK Fit\n",
    "DT1_hRK,DT2_hRK = FitDT(DTIFilt[TrueMets[:,1]>0.8,:],1)\n",
    "x4_hRK,R1_hRK,x2_hRK,R2_hRK = FitKT(DKIFilt[TrueMets[:,1]>0.8,:],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad065f-911a-4ec4-a5d8-b98a958d0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,500)\n",
    "DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,500)\n",
    "DT5,KT5 = GenDTKT([DT1_hRK,DT2_hRK],[x4_hRK,R1_hRK,x2_hRK,R2_hRK],12,1000)\n",
    "\n",
    "\n",
    "DT = np.vstack([DT2,DT3,DT5])\n",
    "KT = np.vstack([KT2,KT3,KT5])\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm(zip(DT1,KT1)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest1 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm(zip(DT2,KT2)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest2 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm(zip(DT3,KT3)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest3 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm(zip(DT4,KT4)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest4 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm(zip(DT5,KT5)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest5 = np.array(ParMets)\n",
    "ParMets = []\n",
    "for d,k in tqdm(zip(DT,KT)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest = np.array(ParMets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b2182-56ce-4b14-96c7-39679fb90f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.hist(ParTest[:,i],density=True,range=[0,1],color=SBIFit,label='Simulated',bins=50)\n",
    "    plt.hist(TrueMets[:,i],alpha=0.8,density=True,range=[0,1],color='gray',label='HCP',bins=50)\n",
    "    if(i==0):\n",
    "        plt.legend(fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1,bbox_to_anchor=(0.7,1),loc=1)\n",
    "    plt.xticks(fontsize=32)\n",
    "    plt.yticks(fontsize=32)\n",
    "    if Save: plt.savefig(FigLoc+'EgMetricDKI_'+str(i)+'.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d7074-cf00-4037-b835-213eba52dd51",
   "metadata": {},
   "source": [
    "# Fig 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee0ea6b-dafa-46ef-8f82-b71124212e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdwi = '../HCP_data/Pat'+str(1)+'/diff_1k.nii.gz'\n",
    "bvalloc = '../HCP_data/Pat'+str(1)+'/bvals_1k.txt'\n",
    "bvecloc = '../HCP_data/Pat'+str(1)+'/bvecs_1k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = [0]+selected_indices\n",
    "\n",
    "bvalsHCP7 = bvalsHCP[selected_indices]\n",
    "bvecsHCP7 = bvecsHCP[selected_indices]\n",
    "gtabHCP7 = gradient_table(bvalsHCP7, bvecsHCP7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265820e7-8a32-4a10-961c-f46ce65127d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIHCPMin_Denoise.pickle\"):\n",
    "    with open(f\"{network_path}/DTIHCPMin_Denoise.pickle\", \"rb\") as handle:\n",
    "        posterior7_2 = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    bvals = gtabHCP.bvals\n",
    "    bvecs = gtabHCP.bvecs\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm(range(TrainingSamples)):\n",
    "        params = priorS0.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        dt = ForceLowFA(dt)\n",
    "        Obs.append(CustomSimulator(dt,gtabHCP7,params[-1],50))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior7_2 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIHCPMin_Denoise.pickle\"):\n",
    "        with open(f\"{network_path}/DTIHCPMin_Denoise.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7_2, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02cdbd-c5d3-4ed8-aafc-b1c948e0f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseLevels = [10,20,30,40,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db121701-4b30-4a28-bc1d-4a465c4d03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoisyImgPlot = np.copy(maskdata[:,:,axial_middle])\n",
    "NoisyImgPlot[~mask[:,:,axial_middle]] = math.nan\n",
    "plt.imshow(NoisyImgPlot[:,:,selected_indices[3]].T,cmap='gray')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(image_path+'Noise_base.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934a6db-e90f-4a0f-9163-9c2e39a211be",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoisyImgs = []\n",
    "for N in NoiseLevels:\n",
    "    np.random.seed(15)\n",
    "    NoisyImg = np.zeros_like(maskdata[:,:,axial_middle])\n",
    "    for i in range(55):\n",
    "        for j in range(64):\n",
    "            if(maskdata[i,j,axial_middle,0]>0):\n",
    "                NoisyImg[i,j] = AddNoise(maskdata[i,j,axial_middle],maskdata[i,j,axial_middle,0],N)\n",
    "            else:\n",
    "                NoisyImg[i,j] = 0\n",
    "    NoisyImgs.append(NoisyImg)\n",
    "    NoisyImgPlot = np.copy(NoisyImg)\n",
    "    NoisyImgPlot[~mask[:,:,axial_middle]] = math.nan\n",
    "    plt.imshow(NoisyImgPlot[:,:,selected_indices[3]].T,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if Save: plt.savefig(image_path+'Noise_'+str(N)+'.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a15c58-0ee0-4743-a962-f45e62ce1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "Denoised_Arr = []\n",
    "for N in NoiseLevels:\n",
    "    NoisyImg4D = np.zeros_like(maskdata[:,:,:3])\n",
    "    for i in range(55):\n",
    "        for j in range(64):\n",
    "            for ll,k in enumerate([axial_middle-1,axial_middle,axial_middle+1]):\n",
    "                if(maskdata[i,j,k,0]>0):\n",
    "                    NoisyImg4D[i,j,ll] = AddNoise(maskdata[i,j,k],maskdata[i,j,k,0],N)\n",
    "                else:\n",
    "                    NoisyImg4D[i,j,ll] = 0\n",
    "    Denoised_Arr.append(mppca(NoisyImg4D[...,selected_indices], patch_radius=1, return_sigma=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595937c-8433-45d6-9cb6-ab7b8a606796",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbi_Arr = []\n",
    "for Imgs in NoisyImgs:\n",
    "    # Compute the mask where the sum is not zero\n",
    "    masks = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(masks)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = posterior7_2.sample((1000,), x=Imgs[i,j,selected_indices],show_progress_bars=False)\n",
    "        return i, j, posterior_samples_1.mean(axis=0)\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = masks.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "\n",
    "    NoiseEst = np.zeros(list(ArrShape) + [7])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "    \n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j,-2] = np.clip(NoiseEst[i, j,-2],0,100)\n",
    "        NoiseEst[i, j,-3] = np.clip(NoiseEst[i, j,-3],0,1)\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(55):\n",
    "        for j in range(64):    \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1]])\n",
    "    MD_SBI7 = np.zeros([55,64])\n",
    "    FA_SBI7 = np.zeros([55,64])\n",
    "    for i in range(55):\n",
    "        for j in range(64):\n",
    "            Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "            MD_SBI7[i,j] = np.mean(Eigs)\n",
    "            FA_SBI7[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "    FA_SBI7[np.isnan(FA_SBI7)] = 0\n",
    "    NoiseEst3_7 =  np.zeros((55,64,69))\n",
    "    for i in range(55):\n",
    "        for j in range(64):    \n",
    "            NoiseEst3_7[i,j] = CustomSimulator(vals_to_mat(NoiseEst2[i,j,:-1]),gtabHCP, S0=NoiseEst2[i,j,-1])\n",
    "    sbi_Arr.append(NoiseEst3_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55590664-ccb7-477e-8985-c176953b0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 3\n",
    "NoisyImgPlot = np.copy(Denoised_Arr[0][:,:,1,j])\n",
    "NoisyImgPlot[~mask[:,:,axial_middle]] = math.nan\n",
    "plt.imshow(1-NoisyImgPlot.T,cmap='Purples')\n",
    "plt.axis('off')\n",
    "plt.savefig(image_path+'pca_denoise_10.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "NoisyImgPlot = np.copy(Denoised_Arr[-1][:,:,1,j])\n",
    "NoisyImgPlot[~mask[:,:,axial_middle]] = math.nan\n",
    "plt.imshow(1-NoisyImgPlot.T,cmap='Purples')\n",
    "plt.axis('off')\n",
    "plt.savefig(image_path+'pca_denoise_50.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d6ea2-4bae-4196-ac2b-47df523ea70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2febbe7-9782-46af-9656-062672ae7d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    \"#eaf7f7\",  # almost white with a hint of teal\n",
    "    \"#cceeee\",\n",
    "    \"#a6e0e0\",\n",
    "    \"#7acccc\",\n",
    "    \"#4db8b8\",\n",
    "    \"#249f9f\",\n",
    "    \"#007f7f\",\n",
    "    \"#005f5f\"   # deep teal\n",
    "]\n",
    "\n",
    "teals = LinearSegmentedColormap.from_list(\"Teals\", colors, N=256)\n",
    "colors = [\n",
    "    \"#fff9e6\",  # very light creamy gold\n",
    "    \"#fff0c2\",\n",
    "    \"#ffe59e\",\n",
    "    \"#ffda70\",\n",
    "    \"#ffca38\",\n",
    "    \"#ffba00\",\n",
    "    \"#d4a700\",\n",
    "    \"#a88000\"   # deep gold/brown\n",
    "]\n",
    "\n",
    "golds = LinearSegmentedColormap.from_list(\"Golds\", colors, N=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b516bf-3f7c-4bf0-8e42-181cbf16fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 3\n",
    "NoisyImgPlot = np.copy(sbi_Arr[0][:,:,selected_indices[j]])\n",
    "NoisyImgPlot[~mask[:,:,axial_middle]] = math.nan\n",
    "plt.imshow(1-NoisyImgPlot.T,cmap=teals)\n",
    "plt.axis('off')\n",
    "plt.savefig(image_path+'sbi_denoise_10.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "NoisyImgPlot = np.copy(sbi_Arr[-1][:,:,selected_indices[j]])\n",
    "NoisyImgPlot[~mask[:,:,axial_middle]] = math.nan\n",
    "plt.imshow(1-NoisyImgPlot.T,cmap=teals)\n",
    "plt.axis('off')\n",
    "plt.savefig(image_path+'sbi_denoise_50.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0c79e-02f2-4621-a603-a69840907bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 3\n",
    "tenmodel = dti.TensorModel(gtabHCP7,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(NoisyImgs[0][...,selected_indices])\n",
    "NoisyImgPlot = tenfit.predict(gtabHCP)[...,selected_indices[j]]\n",
    "NoisyImgPlot[~mask[:,:,axial_middle]] = math.nan\n",
    "plt.imshow(1-NoisyImgPlot.T,cmap=teals)\n",
    "plt.axis('off')\n",
    "plt.imshow(1-NoisyImgPlot.T,cmap=golds)\n",
    "plt.savefig(image_path+'nlls_denoise_10.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tenfit = tenmodel.fit(NoisyImgs[-1][...,selected_indices])\n",
    "NoisyImgPlot = tenfit.predict(gtabHCP)[...,selected_indices[j]]\n",
    "NoisyImgPlot[~mask[:,:,axial_middle]] = math.nan\n",
    "plt.imshow(1-NoisyImgPlot.T,cmap=teals)\n",
    "plt.axis('off')\n",
    "plt.imshow(1-NoisyImgPlot.T,cmap=golds)\n",
    "plt.savefig(image_path+'nlls_denoise_50.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1a8b1-3a1b-443a-8a06-08b39a007ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_ssim_Arr = []\n",
    "sbi_ssim_Arr = []\n",
    "nlls_ssim_Arr = []\n",
    "for k in range(5):\n",
    "    pca_ssim = []\n",
    "    sbi_ssim = []\n",
    "    nlls_ssim = []\n",
    "    tenfit = tenmodel.fit(NoisyImgs[k][...,selected_indices])\n",
    "    NoiseN = tenfit.predict(gtabHCP)[...,selected_indices[j]].T\n",
    "    for j in range(7):\n",
    "        pca_ssim.append(ssim(maskdata[:, :, axial_middle, selected_indices[j]],Denoised_Arr[k][:,:,1,j],data_range=600))\n",
    "        sbi_ssim.append(ssim(maskdata[:, :, axial_middle, selected_indices[j]],sbi_Arr[k][:,:,selected_indices[j]],data_range=600))\n",
    "        NoiseN = tenfit.predict(gtabHCP)[...,selected_indices[j]]\n",
    "        nlls_ssim.append(ssim(maskdata[:, :, axial_middle, selected_indices[j]],NoiseN,data_range=600))\n",
    "    pca_ssim_Arr.append(pca_ssim)\n",
    "    sbi_ssim_Arr.append(sbi_ssim)\n",
    "    nlls_ssim_Arr.append(nlls_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5452ba-ce3f-46cc-9991-47d97f65d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar([1,1.1,1.2,1.3,1.4],np.array(pca_ssim_Arr).mean(axis=-1),yerr=stats.sem(np.array(pca_ssim_Arr).mean(axis=-1),axis=-1),\n",
    "             lw=3,c='tab:purple',label = 'MP-PCA')\n",
    "#for i in range(32):\n",
    "#    plt.scatter(np.ones(7)*(1+0.1*i),np.array(pca_ssim_Arr)[i],marker='^',color='b')\n",
    "plt.errorbar([1,1.1,1.2,1.3,1.4],np.array(sbi_ssim_Arr).mean(axis=-1),yerr=stats.sem(np.array(pca_ssim_Arr).mean(axis=-1),axis=-1),\n",
    "             lw=3,c=SBIFit,label = 'SBI')\n",
    "#for i in range(32):\n",
    "#    plt.scatter(np.ones(7)*(1+0.1*i),np.array(sbi_ssim_Arr)[i],marker='o',color='orange')\n",
    "plt.errorbar([1,1.1,1.2,1.3,1.4],np.array(nlls_ssim_Arr).mean(axis=-1),yerr=stats.sem(np.array(pca_ssim_Arr).mean(axis=-1),axis=-1),\n",
    "             lw=3,c=NLLSFit,label = 'NLLS')\n",
    "plt.grid()\n",
    "plt.yticks([0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "plt.xticks([1,1.1,1.2,1.3,1.4],[10,20,30,40,50])\n",
    "\n",
    "plt.legend(loc=4,\n",
    "           bbox_to_anchor=(1.05, -0.1),\n",
    "           fontsize=32,\n",
    "           columnspacing=0.3,\n",
    "           handlelength=0.4,\n",
    "           handletextpad=0.3,\n",
    "          labelspacing=0.3, \n",
    "            ncols=2)\n",
    "#for i in range(32):\n",
    "#    plt.scatter(np.ones(7)*(1+0.1*i),np.array(nlls_ssim_Arr)[i],marker='o',color='green')\n",
    "plt.savefig(image_path+'ssim_denoise.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4cfbbe-6f24-49c7-bbca-54250cd872fb",
   "metadata": {},
   "source": [
    "# Fig 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8159950-4ec9-4b0e-8ee1-9d2b23d3add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S6/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.mkdir(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786c27e-fb33-45fa-8dfb-3750a7144497",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "_, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "\n",
    "\n",
    "data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "# Get the indices of True values\n",
    "true_indices = np.argwhere(mask)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = true_indices.min(axis=0)\n",
    "max_coords = true_indices.max(axis=0)\n",
    "\n",
    "maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "axial_middle = maskdata.shape[2] // 2\n",
    "maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587bf7d2-0262-4817-9901-9bf3add59691",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices7 = [0]+selected_indices\n",
    "\n",
    "bvalsHCP7_1 = bvalsHCP[selected_indices7]\n",
    "bvecsHCP7_1 = bvecsHCP[selected_indices7]\n",
    "\n",
    "i=3\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(14):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx = []\n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "true_indx = selected_indices7+[t+69 for t in true_indx]\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03395b6b-70f9-4dc3-a7ba-ef912e71b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIHCPMin.pickle\"):\n",
    "    with open(f\"{network_path}/DKIHCPMin.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(3*13000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(3*13000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hRK,DT2_hRK],[x4_hRK,R1_hRK,x2_hRK,R2_hRK],12,int(3*26000))   \n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT2,DT3,DT5])\n",
    "    KT = np.vstack([KT2,KT3,KT5])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([3*52000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabHCP7.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm(indx): \n",
    "            Obs[i] = CustoRKKISimulator(DT[i],KT[i],gtabHCP7,S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    if not os.path.exists(f\"{network_path}/DKIHCPMin.pickle\"):\n",
    "        with open(f\"{network_path}/DKIHCPMinA2.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee199da-8839-4b9b-a4b5-36962ad96e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(TestData4D[:,:,axial_middle,:], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel(i, j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posteriorFull.sample((500,), x=TestData4D[i,j,axial_middle, true_indx],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a57387-7f35-4f34-8feb-c6fa98759664",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst7 = np.zeros([62, 68 ,22])\n",
    "for i, j, x in results:\n",
    "    NoiseEst7[i, j] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22605aad-1f1b-4ea2-976a-47a4b9d3ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 =  np.zeros_like(NoiseEst7)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst7[i,j]))),NoiseEst7[i,j,6:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc0175-c1a1-46b7-8db8-01282024d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK_SBI7  = np.zeros([62, 68])\n",
    "AK_SBI7  = np.zeros([62, 68])\n",
    "RK_SBI7  = np.zeros([62, 68])\n",
    "MKT_SBI7 = np.zeros([62, 68])\n",
    "KFA_SBI7 = np.zeros([62, 68])\n",
    "for i in tqdm(range(62)):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "        MK_SBI7[i,j] = Metrics[0]\n",
    "        AK_SBI7[i,j] = Metrics[1]\n",
    "        RK_SBI7[i,j] = Metrics[2]\n",
    "        MKT_SBI7[i,j] = Metrics[3]\n",
    "        KFA_SBI7[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55fed25-3bb0-4fcb-8dab-fec1d5a4895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KFA_SBI7[np.isnan(KFA_SBI7)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2202d-a8d4-445e-b32a-75ccd7fd83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout = mask[...,axial_middle]\n",
    "cutout = cutout[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbafe00-d89b-46d1-b18c-5de304cba400",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(RK_SBI7)\n",
    "temp[~cutout] = math.nan\n",
    "plt.imshow(temp.T,cmap='hot',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(fraction=0.032, pad=0.04)\n",
    "cbar.ax.set_ylim(0,1)\n",
    "if Save: plt.savefig('../Figures/Fig_3/RKSBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508a8b7-ee7a-4505-b71c-eec197a0e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.copy(KFA_SBI7)\n",
    "temp[~cutout] = math.nan\n",
    "plt.imshow(temp.T,cmap='hot',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(fraction=0.032, pad=0.04)\n",
    "cbar.ax.set_ylim(0,1)\n",
    "if Save: plt.savefig('../Figures/Fig_3/KFASBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259df57b-edee-4dfb-b823-097753908d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatFolder = './SavedDat/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eacc4b9-3a58-4b82-b5da-5e8ff2176fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RKTMinArr = np.load(DatFolder+'Min_MKT_HCP.npy',allow_pickle=True)\n",
    "RKTMidArr = np.load(DatFolder+'Mid_MKT_HCP.npy',allow_pickle=True)\n",
    "RKTFullArr = np.load(DatFolder+'Full_MKT_HCP.npy',allow_pickle=True)\n",
    "\n",
    "KFAMinArr = np.load(DatFolder+'Min_KFA_HCP.npy',allow_pickle=True)\n",
    "KFAMidArr = np.load(DatFolder+'Mid_KFA_HCP.npy',allow_pickle=True)\n",
    "KFAFullArr = np.load(DatFolder+'Full_KFA_HCP.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df913bf-1f1f-4b8b-a932-006c3a3bf0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices7 = [0]+selected_indices\n",
    "\n",
    "bvalsHCP7_1 = bvalsHCP[selected_indices7]\n",
    "bvecsHCP7_1 = bvecsHCP[selected_indices7]\n",
    "\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(14):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx = []\n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "selected_indices7 = selected_indices7+[t+69 for t in true_indx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86916cc-a81a-4601-bd93-dfd4e5ef82af",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "\n",
    "temp_bvecs = bvecsHCP[bvalsHCP>0]\n",
    "temp_bvals = bvalsHCP[bvalsHCP>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(18):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "temp = selected_indices\n",
    "\n",
    "bvalsHCP7_1 = np.insert(temp_bvals[temp],0,0)\n",
    "bvecsHCP7_1 = np.insert(temp_bvecs[temp],0,[0,0,0],axis=0)\n",
    "\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(27):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP20 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx_one = []\n",
    "for b in bvecsHCP7_1:\n",
    "    true_indx_one.append(np.linalg.norm(b-bvecsHCP,axis=1).argmin())\n",
    "true_indx = []        \n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "selected_indices20 = true_indx_one+[t+69 for t in true_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c6586-47ee-447b-814a-9b73c84cb5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gTabsF = []\n",
    "gTabs7 = []\n",
    "gTabs20 = []\n",
    "\n",
    "FullDat   = []\n",
    "\n",
    "for i in tqdm(range(1,33)):\n",
    "    fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    gTabsF.append(gtabExt)\n",
    "    \n",
    "    bvalsHCP7 = gtabExt.bvals[selected_indices7]\n",
    "    bvecsHCP7 = gtabExt.bvecs[selected_indices7]\n",
    "    gtabHCP7 = gradient_table(bvalsHCP7, bvecsHCP7)\n",
    "    gTabs7.append(gtabHCP7)\n",
    "\n",
    "    bvalsHCP20 = gtabExt.bvals[selected_indices20]\n",
    "    bvecsHCP20 = gtabExt.bvecs[selected_indices20]\n",
    "    gtabHCP20 = gradient_table(bvalsHCP20, bvecsHCP20)\n",
    "    gTabs20.append(gtabHCP20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee86ea32-e991-407f-a7f2-0d9fa985aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TD = []\n",
    "axial_middles = []\n",
    "masks = []\n",
    "WMs = []\n",
    "for kk in tqdm(range(32)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk+1)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk+1)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk+1)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middles.append(axial_middle)\n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    TD.append(TestData4D)\n",
    "    masks.append(mask[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,axial_middle])\n",
    "    WM, affine, img = load_nifti('./flipped/c2Pat'+str(kk+1)+'_FP.nii', return_img=True)\n",
    "    WMs.append(np.fliplr(WM[:,:,axial_middle]>0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7664d8d3-d2ea-4c23-aecb-bab876e909ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "TD = []\n",
    "axial_middles = []\n",
    "masks = []\n",
    "WMs = []\n",
    "for kk in tqdm(range(32)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk+1)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk+1)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk+1)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middles.append(axial_middle)\n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    TD.append(TestData4D)\n",
    "    masks.append(mask[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,axial_middle])\n",
    "    WM, affine, img = load_nifti('./flipped/c2Pat'+str(kk+1)+'_FP.nii', return_img=True)\n",
    "    WMs.append(np.fliplr(WM[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,axial_middles[kk]]>0.8))\n",
    "    print(WMs[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0aa0b5-2e2d-4892-9399-e9e70f8c79f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullNLArr = []\n",
    "AKFullNLArr = []\n",
    "RKFullNLArr = []\n",
    "MKTFullNLArr = []\n",
    "KFAFullNLArr = []\n",
    "for kk in tqdm(range(32)):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabsF[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(TD[kk][:,:,axial_middles[kk]])\n",
    "    ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7[i,j] = Metrics[0]\n",
    "            AK_NL7[i,j] = Metrics[1]\n",
    "            RK_NL7[i,j] = Metrics[2]\n",
    "            MKT_NL7[i,j] = Metrics[3]\n",
    "            KFA_NL7[i,j] = Metrics[4]\n",
    "    MKFullNLArr.append(RK_NL7)\n",
    "    AKFullNLArr.append(RK_NL7)\n",
    "    RKFullNLArr.append(RK_NL7)\n",
    "    MKTFullNLArr.append(RKT_NL7)\n",
    "    KFAFullNLArr.append(KFA_NL7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2d308-0b1a-4f00-9060-26467b550060",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKMidNLArr = []\n",
    "AKMidNLArr = []\n",
    "RKMidNLArr = []\n",
    "MKTMidNLArr = []\n",
    "KFAMidNLArr = []\n",
    "for kk in tqdm(range(32)):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabs20[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(TD[kk][:,:,axial_middles[kk],selected_indices20])\n",
    "    ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7[i,j] = Metrics[0]\n",
    "            AK_NL7[i,j] = Metrics[1]\n",
    "            RK_NL7[i,j] = Metrics[2]\n",
    "            MKT_NL7[i,j] = Metrics[3]\n",
    "            KFA_NL7[i,j] = Metrics[4]\n",
    "    MKMidNLArr.append(RK_NL7)\n",
    "    AKMidNLArr.append(RK_NL7)\n",
    "    RKMidNLArr.append(RK_NL7)\n",
    "    MKTMidNLArr.append(RKT_NL7)\n",
    "    KFAMidNLArr.append(KFA_NL7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13399e6c-7301-474a-b300-eacf7798482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKMinNLArr = []\n",
    "AKMinNLArr = []\n",
    "RKMinNLArr = []\n",
    "MKTMinNLArr = []\n",
    "KFAMinNLArr = []\n",
    "for kk in tqdm(range(32)):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabs7[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(TD[kk][:,:,axial_middles[kk],selected_indices7])\n",
    "    ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7[i,j] = Metrics[0]\n",
    "            AK_NL7[i,j] = Metrics[1]\n",
    "            RK_NL7[i,j] = Metrics[2]\n",
    "            MKT_NL7[i,j] = Metrics[3]\n",
    "            KFA_NL7[i,j] = Metrics[4]\n",
    "    MKMinNLArr.append(RK_NL7)\n",
    "    AKMinNLArr.append(RK_NL7)\n",
    "    RKMinNLArr.append(RK_NL7)\n",
    "    MKTMinNLArr.append(RKT_NL7)\n",
    "    KFAMinNLArr.append(KFA_NL7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d8d27-3192-4bb1-9e71-1650ccf473e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccM7 = []\n",
    "for i in range(32):\n",
    "    M7 =MKTMinArr[i]\n",
    "    MF =MKTFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    AccM7.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "AccM20 = []\n",
    "for i in range(32):\n",
    "    M7 =MKTMidArr[i]\n",
    "    MF =MKTFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    AccM20.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "AccMFulls = []\n",
    "for i in range(32):\n",
    "    M7 =MKTFullArr[i]\n",
    "    MF =MKTFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    AccMFulls.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "AccM7NL = []\n",
    "for i in range(32):\n",
    "    M7 =MKTMinNLArr[i]\n",
    "    M7[np.isnan(M7)] = 0\n",
    "    MF =MKTFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    AccM7NL.append(np.mean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "AccM20NL = []\n",
    "for i in range(32):\n",
    "    M7 =MKTMidNLArr[i]\n",
    "    M7[np.isnan(M7)] = 0\n",
    "    MF =MKTFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    AccM20NL.append(np.nanmean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "SSIM7 = []\n",
    "SSIM20 = []\n",
    "SSIMFulls = []\n",
    "\n",
    "SSIM7NL = []\n",
    "SSIM20NL = []\n",
    "for i in tqdm(range(32)):\n",
    "    NS1 =MKTMinArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =MKTFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM7.append(result)\n",
    "\n",
    "    NS1 =MKTMidArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =MKTFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM20.append(result)\n",
    "    \n",
    "    NS1 =MKTFullArr[i]\n",
    "    NS2 =MKTFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIMFulls.append(result)\n",
    "\n",
    "    NS1 =MKTMinNLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =MKTFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM7NL.append(result)\n",
    "\n",
    "    NS1 =MKTMidNLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =MKTFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM20NL.append(result)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d889f663-6361-4cb4-acb3-3e9ae455cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, ax = plt.subplots(1,1, figsize=(3.2,4.8))\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "\n",
    "\n",
    "y_data = np.array(AccM7NL)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "y_data = np.array(AccMFulls)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20)\n",
    "g_pos = np.array([1.55])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM7)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20NL)\n",
    "g_pos = np.array([2.65])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "ax.set_ylim(0,0.6)\n",
    "plt.xticks([0.8,1.55,1.95,2.65,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "ax.set_xlim(0.5,3.5)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Acc_MKT.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f871b0-de9b-4f06-bd60-bd43ebdb9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SSIMFulls)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20)\n",
    "g_pos = np.array([1.5])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM7)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20NL)\n",
    "g_pos = np.array([2.6])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.xticks([0.8,1.5,1.95,2.6,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "ax.legend(\n",
    "    handles=[leg_patch3],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=1,\n",
    "fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,bbox_to_anchor= (-0.1,-0.05))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_RKT.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e203e-b5b4-454e-8a75-e5356f233696",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prec7_SBI = []\n",
    "Prec20_SBI = []\n",
    "PrecFull_SBI = []\n",
    "\n",
    "Prec7_NLLS = []\n",
    "Prec20_NLLS = []\n",
    "PrecFull_NLLS = []\n",
    "for i in range(32):\n",
    "    Prec7_SBI.append(np.std(MKTMinArr[i][WMs[i]]))\n",
    "    Prec20_SBI.append(np.std(MKTMidArr[i][WMs[i]]))\n",
    "    PrecFull_SBI.append(np.std(MKTFullArr[i][WMs[i]]))\n",
    "\n",
    "    Prec7_NLLS.append(np.std(MKTMinNLArr[i][WMs[i]]))\n",
    "    Prec20_NLLS.append(np.std(MKTMidNLArr[i][WMs[i]]))\n",
    "    PrecFull_NLLS.append(np.std(MKTFullNLArr[i][WMs[i]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b14eb-492e-4512-be96-bdcb2a12c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(1,1,figsize=(3.2,4.8))\n",
    "\n",
    "y_data = np.array(PrecFull_SBI)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_SBI)\n",
    "g_pos = np.array([1.0])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI)\n",
    "g_pos = np.array([1.35])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(PrecFull_NLLS)\n",
    "g_pos = np.array([1.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_NLLS)\n",
    "g_pos = np.array([2.15])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_NLLS)\n",
    "g_pos = np.array([2.5])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([0.65,1,1.35,1.8,2.15,2.5],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "x = np.arange(1.7,2.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS)[~np.isnan(PrecFull_NLLS)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS)[~np.isnan(PrecFull_NLLS)], 77)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.5,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI)[~np.isnan(PrecFull_SBI)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI)[~np.isnan(PrecFull_SBI)], 77)\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "ax1.set_xlim(0.3,2.8)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKI_RKT_Prec.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d667e0e-e243-4890-ab19-dcd6837c4e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccM7 = []\n",
    "for i in range(32):\n",
    "    M7 =KFAMinArr[i]\n",
    "    MF =KFAFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    AccM7.append(np.nanmean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "AccM20 = []\n",
    "for i in range(32):\n",
    "    M7 =KFAMidArr[i]\n",
    "    MF =KFAFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    AccM20.append(np.nanmean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "AccMFulls = []\n",
    "for i in range(32):\n",
    "    M7 =KFAFullArr[i]\n",
    "    MF =KFAFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    AccMFulls.append(np.nanmean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "AccM7NL = []\n",
    "for i in range(32):\n",
    "    M7 =KFAMinNLArr[i]\n",
    "    M7[np.isnan(M7)] = 0\n",
    "    MF =KFAFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    AccM7NL.append(np.nanmean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "AccM20NL = []\n",
    "for i in range(32):\n",
    "    M7 =KFAMidNLArr[i]\n",
    "    M7[np.isnan(M7)] = 0\n",
    "    MF =KFAFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    AccM20NL.append(np.nanmean(np.abs(M7-MF)[Ma]))\n",
    "\n",
    "SSIM7 = []\n",
    "SSIM20 = []\n",
    "SSIMFulls = []\n",
    "\n",
    "SSIM7NL = []\n",
    "SSIM20NL = []\n",
    "for i in tqdm(range(32)):\n",
    "    NS1 =KFAMinArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =KFAFullArr[i]\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM7.append(result)\n",
    "\n",
    "    NS1 =KFAMidArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =KFAFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM20.append(result)\n",
    "    \n",
    "    NS1 =KFAFullArr[i]\n",
    "    NS2 =KFAFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIMFulls.append(result)\n",
    "\n",
    "    NS1 =KFAMinNLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =KFAFullNLArr[i]\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM7NL.append(result)\n",
    "\n",
    "    NS1 =KFAMidNLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =KFAFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM20NL.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8389e26-c222-4f35-a383-b4c9a5670a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, ax = plt.subplots(1,1, figsize=(3.2,4.8))\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "\n",
    "\n",
    "y_data = np.array(AccM7NL)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "y_data = np.array(AccMFulls)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20)\n",
    "g_pos = np.array([1.55])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM7)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(AccM20NL)\n",
    "g_pos = np.array([2.65])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "ax.set_ylim(0,0.6)\n",
    "plt.xticks([0.8,1.55,1.95,2.65,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "ax.set_xlim(0.5,3.5)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Acc_KFA.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fcf553-6333-4331-b60f-e7d1456536f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SSIMFulls)\n",
    "g_pos = np.array([0.8])\n",
    "colors = ['black']\n",
    "colors2 = ['gray']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20)\n",
    "g_pos = np.array([1.5])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(SSIM7)\n",
    "g_pos = np.array([1.95])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM20NL)\n",
    "g_pos = np.array([2.6])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL)\n",
    "g_pos = np.array([3.05])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM7NL)\n",
    "g_pos = np.array([3.1])\n",
    "colors = ['burlywood']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.xticks([0.8,1.5,1.95,2.6,3.05],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "ax.legend(\n",
    "    handles=[leg_patch3],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=1,\n",
    "fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,bbox_to_anchor= (-0.1,-0.05))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_KFA.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eab4ca-9f54-4da8-818a-d7b9a8e555ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prec7_SBI = []\n",
    "Prec20_SBI = []\n",
    "PrecFull_SBI = []\n",
    "\n",
    "Prec7_NLLS = []\n",
    "Prec20_NLLS = []\n",
    "PrecFull_NLLS = []\n",
    "for i in range(32):\n",
    "    Prec7_SBI.append(np.nanstd(KFAMinArr[i][WMs[i]]))\n",
    "    Prec20_SBI.append(np.nanstd(KFAMidArr[i][WMs[i]]))\n",
    "    PrecFull_SBI.append(np.nanstd(KFAFullArr[i][WMs[i]]))\n",
    "\n",
    "    Prec7_NLLS.append(np.std(KFAMinNLArr[i][WMs[i]]))\n",
    "    Prec20_NLLS.append(np.std(KFAMidNLArr[i][WMs[i]]))\n",
    "    PrecFull_NLLS.append(np.std(KFAFullNLArr[i][WMs[i]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8301789-d9d5-41e4-8136-ed45ead7ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(1,1,figsize=(3.2,4.8))\n",
    "\n",
    "y_data = np.array(PrecFull_SBI)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_SBI)\n",
    "g_pos = np.array([1.0])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI)\n",
    "g_pos = np.array([1.35])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(PrecFull_NLLS)\n",
    "g_pos = np.array([1.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec20_NLLS)\n",
    "g_pos = np.array([2.15])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_NLLS)\n",
    "g_pos = np.array([2.5])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([0.65,1,1.35,1.8,2.15,2.5],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "x = np.arange(1.7,2.6,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS)[~np.isnan(PrecFull_NLLS)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS)[~np.isnan(PrecFull_NLLS)], 77)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.5,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI)[~np.isnan(PrecFull_SBI)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI)[~np.isnan(PrecFull_SBI)], 77)\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "ax1.set_xlim(0.3,2.8)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKI_KFA_Prec.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5ba82-e8ee-4cd1-a04c-3be88d04c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial15 = HemiSphere(xyz=bvecs[1:16])\n",
    "hsph_initial7 = HemiSphere(xyz=bvecs[1:7])\n",
    "hsph_updated15,_ = disperse_charges(hsph_initial15,5000)\n",
    "hsph_updated7,_ = disperse_charges(hsph_initial7,5000)\n",
    "gtabSimSub = gradient_table(np.array([0]+[1000]*6+[3000]*15).squeeze(), np.vstack([[0,0,0],hsph_updated7.vertices,hsph_updated15.vertices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33ee74-fa6a-464f-bef5-e1777953c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],1,40)\n",
    "DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],1,40)\n",
    "DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],1,40)\n",
    "DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],1,40)\n",
    "DT5,KT5 = GenDTKT([DT1_hRK,DT2_hRK],[x4_hRK,R1_hRK,x2_hRK,R2_hRK],12,40)\n",
    "\n",
    "SampsDT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "SampsKT = np.vstack([KT1,KT2,KT3,KT4,KT5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe994d9f-3e00-4f14-8f8a-9462a254c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "Samples7  = []\n",
    "\n",
    "for Sd,Sk in zip(SampsDT,SampsKT):\n",
    "    Samples7.append([CustoRKKISimulator(Sd,Sk,gtabSimSub, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "\n",
    "Samples7 = np.array(Samples7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c59b47e-ac4e-49c8-b52b-dda50c386b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DKISimMin.pickle\"):\n",
    "    with open(f\"{network_path}/DKISimMin.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*6000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*2000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*6000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*6000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hRK,DT2_hRK],[x4_hRK,R1_hRK,x2_hRK,R2_hRK],12,int(2.5*6000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabSimSub.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm(indx): \n",
    "            Obs[i] = CustoRKKISimulator(DT[i],KT[i],gtabSimSub,200,np.random.rand()*30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>800).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKISimMin.pickle\"):\n",
    "        with open(f\"{network_path}/DKISimMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52f0f1-2163-4b10-9a02-ee6d946af871",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "ErrorFull = []\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples7[i,k,:]\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessSBI = posterior_samples_1.mean(axis=0)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(GuessSBI[:6],GuessSBI[6:],SampsDT[i],SampsKT[i]))\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "Error_s = []\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples7[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(tenfit.lower_triangular(),tenfit.kt,SampsDT[i],SampsKT[i]))\n",
    "    Error_s.append(ErrorN2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b63bf6-cfa1-4f9c-ba2b-ac2c2b438590",
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorFull = np.array(ErrorFull)\n",
    "Error_s = np.array(Error_s)\n",
    "ErrorNames = ['MK Error', 'AK Error', 'RK Error', 'MKT Error', 'KFA Error']\n",
    "fig,ax = plt.subplots(1,2,figsize=(9,3))\n",
    "for i in range(2):\n",
    "    plt.sca(ax[i])\n",
    "    g_pos = np.array([1.3,2.3,3.3,4.3])\n",
    "    colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "    colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "    BoxPlots(ErrorFull[1:,:,i+3],g_pos,colors,colors2,ax[i],widths=0.3,scatter=False)\n",
    "    g_pos = np.array([1,2,3,4])\n",
    "    colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "    colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "    BoxPlots(Error_s[1:,:,i+3],g_pos,colors,colors2,ax[i],widths=0.3,scatter=False)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    plt.yticks(fontsize=32)\n",
    "    if(i==0):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),Line2D([0], [0], color=WLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(-0.1,1.1),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,labelspacing=0.1)\n",
    "if Save: plt.savefig(FigLoc+'ErrorsMin_MKTKFA.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
