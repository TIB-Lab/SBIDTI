{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4a9924",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef919d0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T23:42:34.127628Z",
     "start_time": "2025-12-28T23:42:30.951086Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from AxCaliber_funcs import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8530f",
   "metadata": {},
   "source": [
    "## network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b35dda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T23:45:39.446618Z",
     "start_time": "2025-12-28T23:45:39.393275Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(NetworkDir+\"AxCal_Network.pickle\"):\n",
    "    with open(NetworkDir+\"AxCal_Network.pickle\", \"rb\") as handle:\n",
    "        Network = pickle.load(handle)\n",
    "        print('loaded')\n",
    "else:\n",
    "    NumSamps = 2_000_000\n",
    "    np.random.seed(12)\n",
    "    x1  = np.random.randn(NumSamps)\n",
    "    y1  = np.random.randn(NumSamps)\n",
    "    z1  =  np.random.randn(NumSamps)\n",
    "    VS = np.vstack([x1,y1,z1])\n",
    "    VS = (VS/np.linalg.norm(VS,axis=0)).T\n",
    "    Angs = np.array([SpherAng(v) for v in VS])\n",
    "\n",
    "    #Diffusion of restricted\n",
    "    Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "    Dperp = np.random.rand(NumSamps)*5e-3\n",
    "\n",
    "    #Diffusion of hindered - needs to be updated\n",
    "    MD_prior = np.random.rand(NumSamps)*0.005\n",
    "    FA_prior = np.random.rand(NumSamps)*0.999\n",
    "    DHind = [mat_to_vals(random_diffusion_tensor(m, f)) for m,f in zip(MD_prior,FA_prior)]\n",
    "\n",
    "    mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "    sig2 = np.random.rand(NumSamps) * (4e-7 - 9e-8) + 9e-8\n",
    "\n",
    "    #Fraction of hindered\n",
    "    frac  = np.random.rand(NumSamps)\n",
    "    #frac  = np.hstack([np.random.rand(NumSamps//5)*0.5+0.5,np.random.rand(4*NumSamps//5)])\n",
    "    S0Rand =np.random.rand(NumSamps)*2475+25\n",
    "    TrainParams = np.column_stack([Angs,Dpar,Dperp,DHind,frac,mean,S0Rand])\n",
    "    \n",
    "    \n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps),position=0,leave=True):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "\n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "\n",
    "        a = mean[i]\n",
    "        s0 = S0Rand[i]\n",
    "\n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "\n",
    "        TrainSig1 = AxCaliber(bvecs[:(n_pts+1)],bvals[:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = AxCaliber(bvecs[(n_pts+1):2*(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = AxCaliber(bvecs[2*(n_pts+1):],bvals[2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(AddNoise(TrainSig[-1],s0,Noise))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "\n",
    "    Obs = torch.tensor(NoisyTrainSig).float()\n",
    "    \n",
    "    Vecs = []\n",
    "    for _ in range(NumSamps):\n",
    "        r = [1,1,1]\n",
    "        # random lengths for the 3 vectors from b2000\n",
    "        #len1 = np.random.randint(1, len(b2000))\n",
    "        # random base lengths for b4000\n",
    "        #len2_raw = np.random.randint(1, len(b4000))\n",
    "        # enforce the \"max(6 - len(x1), random)\" logic\n",
    "        #len2 = np.maximum(6 - len1, len2_raw)\n",
    "        X = np.hstack([\n",
    "                [0],                                    # force first element = 0\n",
    "                np.random.choice(b2000, 3, replace=False),\n",
    "                np.random.choice(b4000, 3, replace=False),\n",
    "            ])\n",
    "        # choose which of X[i] to include, with offsets\n",
    "        offsets = [0, 91, 182]\n",
    "        temp_parts = [\n",
    "            X + offsets[i]\n",
    "            for i,_ in enumerate(r)\n",
    "        ]\n",
    "\n",
    "        temp = np.hstack(temp_parts)\n",
    "        Vecs.append(temp)\n",
    "        \n",
    "        feats = []\n",
    "        for v,TS in zip(Vecs,NoisyTrainSig):\n",
    "            feats.append(AxcaliberFeatures(bvecs[v], bvals[v], Deltas[v], TS[v]))\n",
    "        features = np.array(feats)\n",
    "        Par = torch.tensor(TrainParams).float()\n",
    "        \n",
    "        low = Par.min(axis=0)[0] - 10*torch.sign(Par.min(axis=0)[0])*Par.min(axis=0)[0]\n",
    "        low = np.clip(low,low,-1)\n",
    "        high = Par.max(axis=0)[0] + 10*Par.max(axis=0)[0]\n",
    "        \n",
    "        Obs_feats = torch.tensor(features).float()\n",
    "        Par = torch.tensor(Par).float()\n",
    "\n",
    "        inference = SNPE(device='mps')\n",
    "\n",
    "        # generate simulations and pass to the inference object\n",
    "        inference = inference.append_simulations(Par, Obs_feats,data_device='cpu')\n",
    "\n",
    "        # train the density estim ator and build the posterior\n",
    "        density_estimator = inference.train(training_batch_size = 1024)\n",
    "        prior_bounds = BoxUniform(low=low, high=high)\n",
    "        Network = DirectPosterior(density_estimator.cpu(), prior=prior_bounds) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40299f29",
   "metadata": {},
   "source": [
    "# Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d708e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T23:45:50.234835Z",
     "start_time": "2025-12-28T23:45:46.678985Z"
    }
   },
   "outputs": [],
   "source": [
    "Dir = MSDir+'/Ctrl055_R01_28/'\n",
    "dat = pmt.read_mat(Dir+'data_loaded.mat')\n",
    "bvecs = dat['direction']\n",
    "bvals = dat['bval']\n",
    "\n",
    "n_pts = 90\n",
    "\n",
    "Deltas = np.concatenate([\n",
    "    np.full(n_pts + 1, Delta[0]),\n",
    "    np.full(n_pts + 1, Delta[1]),\n",
    "    np.full(n_pts + 1, Delta[2]),\n",
    "])\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs2000 = bvecs[:91][bvals[:91]==2000]\n",
    "distance_matrix = squareform(pdist(bvecs2000))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaini'ng point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs2000_selected = bvecs[:91][bvals[:91]==2000][selected_indices]\n",
    "true_indices = []\n",
    "for b in bvecs2000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs4000 = bvecs[:91][bvals[:91]==4000]\n",
    "distance_matrix = squareform(pdist(bvecs4000))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs4000_selected = bvecs[:91][bvals[:91]==4000][selected_indices]\n",
    "for b in bvecs4000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "MinIdices = np.hstack([0,true_indices])\n",
    "DevilIndices = np.hstack([MinIdices,MinIdices+91,MinIdices+182])\n",
    "bvecs_Dev = bvecs[DevilIndices]\n",
    "bvals_Dev = bvals[DevilIndices]\n",
    "\n",
    "bve_split = [bvecs[:(n_pts+1)],bvecs[(n_pts+1):2*(n_pts+1)],bvecs[2*(n_pts+1):]]\n",
    "bva_split = [bvals[:(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],bvals[2*(n_pts+1):]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360f37d",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be003ed7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:51:59.052865Z",
     "start_time": "2025-12-10T21:51:58.850783Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "TestSamps = 20\n",
    "\n",
    "# Directions\n",
    "x1  = np.random.randn(TestSamps)\n",
    "y1  = np.random.randn(TestSamps)\n",
    "z1  =  np.random.randn(TestSamps)\n",
    "V = np.vstack([x1,y1,z1])\n",
    "V = (V/np.linalg.norm(V,axis=0)).T\n",
    "Angs = np.array([SpherAng(v) for v in V])\n",
    "\n",
    "#Diffusion of restricted\n",
    "Dpar  = np.random.rand(TestSamps)*5e-3\n",
    "Dperp = np.random.rand(TestSamps)*5e-3\n",
    "\n",
    "#Diffusion of hindered\n",
    "MD_prior = np.random.rand(TestSamps)*0.005\n",
    "FA_prior = np.random.rand(TestSamps)*0.999\n",
    "DHind = [mat_to_vals(random_diffusion_tensor(m, f)) for m,f in zip(MD_prior,FA_prior)]\n",
    "\n",
    "#Fraction of hindered\n",
    "frac  = np.random.rand(TestSamps)\n",
    "\n",
    "mean = np.random.rand(TestSamps)*0.005+1e-4\n",
    "sig2 = np.random.rand(TestSamps) * (4e-7 - 9e-8) + 9e-8\n",
    "\n",
    "S0Rand =np.ones(TestSamps)\n",
    "\n",
    "TestParams = np.column_stack([Angs,Dpar,Dperp,DHind,frac,mean])\n",
    "\n",
    "TestSig = []\n",
    "NoisyTestSig = []\n",
    "for i in tqdm(range(TestSamps)):\n",
    "    v = np.array([Angs[i]])\n",
    "    dpar = Dpar[i]\n",
    "    dperp = Dperp[i]\n",
    "    \n",
    "    dh   = DHind[i]\n",
    "    f    = [frac[i],1-frac[i]]\n",
    "\n",
    "    a = mean[i]\n",
    "    s = sig2[i]\n",
    "    alpha     = a * a / s\n",
    "    scale = s / a\n",
    "    rv = stats.gamma(a=alpha,scale=scale)\n",
    "    \n",
    "    R = np.linspace(0.0001,0.005, 30)\n",
    "    weights = rv.pdf(R)\n",
    "    weights = weights/np.sum(weights)\n",
    "    s0 = 200\n",
    "\n",
    "    TestSig1 = AxCaliber(bvecs[:(n_pts+1)],bvals[:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "    TestSig2 = AxCaliber(bvecs[(n_pts+1):2*(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "    TestSig3 = AxCaliber(bvecs[2*(n_pts+1):],bvals[2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "    TestSig.append(np.hstack([TestSig1,TestSig2,TestSig3]))\n",
    "    Noisy = []\n",
    "    for Noise in [2,10,20,30]:\n",
    "        Noisy.append(AddNoise(TestSig[-1],s0,Noise))\n",
    "    NoisyTestSig.append(Noisy)\n",
    "NoisyTestSig = np.array(NoisyTestSig)\n",
    "NoisyTestSig = np.swapaxes(NoisyTestSig,0,1)\n",
    "TestSig = np.array(TestSig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e37ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T08:35:30.242018Z",
     "start_time": "2025-12-27T08:35:30.186069Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "mean = np.random.rand(1)*0.005+1e-4\n",
    "MD_prior = np.random.rand(1)*0.005\n",
    "FA_prior = np.random.rand(1)*0.999\n",
    "DHind_guess = [mat_to_vals(random_diffusion_tensor(m, f)) for m,f in zip(MD_prior,FA_prior)]\n",
    "\n",
    "Dpar_guess = np.random.rand()*1e-3            # mm^2/s\n",
    "Dperp_guess = np.random.rand()*1e-3             # mm^2/s\n",
    "phi = 0#np.random.rand()*pi\n",
    "cos_theta = 0#np.random.rand()  # uniform in [0,1]\n",
    "theta = np.arccos(cos_theta)         # in [0, pi/2]\n",
    "Angs_guess = np.vstack([theta,phi]).T\n",
    "\n",
    "mean_guess = np.random.rand()*0.005 + 1e-4\n",
    "\n",
    "frac_guess = np.random.rand()\n",
    "guess = np.column_stack([Angs_guess,Dpar_guess,Dperp_guess,DHind_guess,frac_guess,mean_guess]).squeeze()\n",
    "bounds = np.array([[-np.inf,np.inf]]*12).T\n",
    "bounds[:,0] = [0,np.pi/2]\n",
    "bounds[:,1] = [-np.pi,np.pi]\n",
    "bounds[:,2] = [0,5e-3]\n",
    "bounds[:,3] = [0,5e-3]\n",
    "bounds[:,4] = [-5e-3,5e-3]\n",
    "bounds[:,5] = [-5e-3,5e-3]\n",
    "bounds[:,6] = [-5e-3,5e-3]\n",
    "bounds[:,7] = [-5e-3,5e-3]\n",
    "bounds[:,8] = [-5e-3,5e-3]\n",
    "bounds[:,9] = [-5e-3,5e-3]\n",
    "bounds[:,10] = [0,1]\n",
    "bounds[:,11] = [1e-4,0.005+1e-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22990ecc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:52:09.204142Z",
     "start_time": "2025-12-10T21:52:05.192799Z"
    }
   },
   "outputs": [],
   "source": [
    "LS_result = np.zeros([4,20,12])\n",
    "bve_split = [bvecs[:(n_pts+1)],bvecs[(n_pts+1):2*(n_pts+1)],bvecs[2*(n_pts+1):]]\n",
    "bva_split = [bvals[:(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],bvals[2*(n_pts+1):]]\n",
    "for i in tqdm(range(20),position = 0, leave = True):\n",
    "    for j in range(4):\n",
    "        result = sp.optimize.least_squares(residuals, guess, args=[NoisyTestSig[j,i],bve_split,bva_split,Delta,False],\n",
    "                                      bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        LS_result[j,i] = result.x\n",
    "LS_Errors = []\n",
    "for N in tqdm(LS_result,position = 0, leave = True):\n",
    "    temp = []\n",
    "    for n_guess,n_true,sig in zip(N,TestParams,TestSig):\n",
    "        temp.append(AxCal_Errors(sig,n_true,n_guess,Delta,bve_split,bva_split))\n",
    "    LS_Errors.append(temp)\n",
    "LS_Errors = np.array(LS_Errors)\n",
    "\n",
    "bve_splitd = [bvecs_Dev[:7],bvecs_Dev[7:14],bvecs_Dev[14:]]\n",
    "bva_splitd = [bvals_Dev[:7],bvals_Dev[7:14],bvals_Dev[14:]]\n",
    "for i in tqdm(range(20),position = 0, leave = True):\n",
    "    for j in range(4):\n",
    "        result = sp.optimize.least_squares(residuals, guess, args=[NoisyTestSig[j,i][DevilIndices],bve_splitd,bva_splitd,Delta],\n",
    "                                      bounds=bounds,verbose=1,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        LS_result[j,i] = result.x\n",
    "        \n",
    "LS_Errors_Min = []\n",
    "for N in tqdm(LS_result,position = 0, leave = True):\n",
    "    temp = []\n",
    "    for n_guess,n_true,sig in zip(N,TestParams,TestSig):\n",
    "        temp.append(AxCal_Errors(sig,n_true,n_guess,Delta,bve_split,bva_split))\n",
    "    LS_Errors_Min.append(temp)\n",
    "LS_Errors_Min = np.array(LS_Errors_Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113424b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T09:12:55.705804Z",
     "start_time": "2025-12-09T09:12:45.049963Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the function for optimization\n",
    "def fit_SBI(i,j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = Network.sample((1000,), x=AxcaliberFeatures(bvecs,bvals,Deltas,NoisyTestSig[i,j]),show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "y_indx = np.repeat(np.arange(20),4)\n",
    "x_indx = np.tile(np.arange(4),20)\n",
    "indices = np.column_stack([x_indx,y_indx])\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(fit_SBI)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "SBI_Res = np.zeros([4,20,13])\n",
    "\n",
    "for i, j, x in results:\n",
    "    SBI_Res[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    SBI_Res[i, j,-2] = np.clip(SBI_Res[i, j,-2],0,100)\n",
    "    \n",
    "SBI_Errors = []\n",
    "for N in tqdm(SBI_Res):\n",
    "    temp = []\n",
    "    for n_guess,n_true,sig in zip(N,TestParams,TestSig):\n",
    "        temp.append(AxCal_Errors(sig,n_true,n_guess,Delta,bve_split,bva_split))\n",
    "    SBI_Errors.append(temp)\n",
    "SBI_Errors = np.array(SBI_Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982f107",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T09:12:56.330338Z",
     "start_time": "2025-12-09T09:12:55.706892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the function for optimization\n",
    "def fit_SBI(i,j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = Network.sample((1000,), x=AxcaliberFeatures(bvecs[DevilIndices],bvals[DevilIndices],Deltas[DevilIndices],NoisyTestSig[i,j,DevilIndices]),show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "y_indx = np.repeat(np.arange(20),4)\n",
    "x_indx = np.tile(np.arange(4),20)\n",
    "indices = np.column_stack([x_indx,y_indx])\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(fit_SBI)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "SBI_Res = np.zeros([4,20,13])\n",
    "\n",
    "for i, j, x in results:\n",
    "    SBI_Res[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    SBI_Res[i, j,-2] = np.clip(SBI_Res[i, j,-2],0,100)\n",
    "    \n",
    "SBI_Errors_Min = []\n",
    "for N in tqdm(SBI_Res):\n",
    "    temp = []\n",
    "    for n_guess,n_true,sig in zip(N,TestParams,TestSig):\n",
    "        temp.append(AxCal_Errors(sig,n_true,n_guess,Delta,bve_split,bva_split))\n",
    "    SBI_Errors_Min.append(temp)\n",
    "SBI_Errors_Min = np.array(SBI_Errors_Min)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c33eb03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T09:40:39.796060Z",
     "start_time": "2025-12-10T09:40:28.642893Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "r = 1.0  # sphere radius\n",
    "vector = np.array([-0.5, -1, 1])   # arbitrary vector\n",
    "n = vector / np.linalg.norm(vector)  # unit vector in the direction of 'vector'\n",
    "intersection = n * r  # intersection of the vector with the sphere\n",
    "\n",
    "# Circle parameters (geodesic circle on the sphere)\n",
    "circle_angle_deg = 15  # angular radius in degrees\n",
    "alpha1 = [(S[:,2].mean()) for S in SBI_Errors][-1]\n",
    "\n",
    "# -----------------------------\n",
    "# Construct a circle on the sphere\n",
    "# -----------------------------\n",
    "# To draw a circle on the sphere centered at 'intersection',\n",
    "# we use the following idea:\n",
    "# For a given center n (a point on the unit sphere) and an angular radius alpha,\n",
    "# any point on the circle can be written as:\n",
    "#   P(t) = cos(alpha)*n + sin(alpha)*(cos(t)*u + sin(t)*w)\n",
    "# where u and w are any two orthonormal vectors spanning the tangent plane at n.\n",
    "\n",
    "# First, choose u as a vector perpendicular to n.\n",
    "# (If n is parallel to the z-axis, choose a different axis to avoid the zero vector.)\n",
    "if np.allclose(n, [0, 0, 1]):\n",
    "    u = np.array([1, 0, 0])\n",
    "else:\n",
    "    u = np.cross(n, [0, 0, 1])\n",
    "    u = u / np.linalg.norm(u)\n",
    "\n",
    "# Then, w is perpendicular to both n and u.\n",
    "w = np.cross(n, u)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Create the sphere mesh\n",
    "# -----------------------------\n",
    "phi = np.linspace(0, 2 * np.pi, 500)  # azimuthal angle\n",
    "theta = np.linspace(0, np.pi, 500)      # polar angle\n",
    "\n",
    "phi, theta = np.meshgrid(phi, theta)\n",
    "x_sphere = r * np.sin(theta) * np.cos(phi)\n",
    "y_sphere = r * np.sin(theta) * np.sin(phi)\n",
    "z_sphere = r * np.cos(theta)\n",
    "\n",
    "# -----------------------------\n",
    "# Plot everything\n",
    "# -----------------------------\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "# Plot the vector (using quiver)\n",
    "ax.quiver(0, 0, 0, intersection[0], intersection[1], intersection[2],\n",
    "          color='r', linewidth=2, arrow_length_ratio=0.1)\n",
    "\n",
    "# Plot the circle on the sphere\n",
    "# Create points around the circle\n",
    "t_vals = np.linspace(0, 2 * np.pi, 200)\n",
    "circle_points = np.array([\n",
    "    np.cos(alpha1) * n + np.sin(alpha1) * (np.cos(t) * u + np.sin(t) * w)\n",
    "    for t in t_vals\n",
    "])\n",
    "\n",
    "ax.plot(circle_points[:, 0], circle_points[:, 1], circle_points[:, 2], color='paleturquoise', linewidth=2,ls='--')\n",
    "\n",
    "circle_angle_deg = 15  # angular radius in degrees\n",
    "alpha2 = [(S[:,2].mean()) for S in SBI_Errors_Min][-1]\n",
    "\n",
    "# Plot the circle on the sphere\n",
    "# Create points around the circle\n",
    "t_vals = np.linspace(0, 2 * np.pi, 100)\n",
    "circle_points = np.array([\n",
    "    np.cos(alpha2) * n + np.sin(alpha2) * (np.cos(t) * u + np.sin(t) * w)\n",
    "    for t in t_vals\n",
    "])\n",
    "\n",
    "ax.plot(circle_points[:, 0], circle_points[:, 1], circle_points[:, 2], color='lightseagreen', linewidth=2,ls='--')\n",
    "\n",
    "alpha3 = [(S[:,2].mean()) for S in LS_Errors][-1]\n",
    "\n",
    "# Plot the circle on the sphere\n",
    "# Create points around the circle\n",
    "t_vals = np.linspace(0, 2 * np.pi, 100)\n",
    "circle_points = np.array([\n",
    "    np.cos(alpha3) * n + np.sin(alpha3) * (np.cos(t) * u + np.sin(t) * w)\n",
    "    for t in t_vals\n",
    "])\n",
    "\n",
    "ax.plot(circle_points[:, 0], circle_points[:, 1], circle_points[:, 2], color='sandybrown', linewidth=2,ls='--')\n",
    "\n",
    "alpha4 = [(S[:,2].mean()) for S in LS_Errors_Min][-1]\n",
    "\n",
    "# Plot the circle on the sphere\n",
    "# Create points around the circle\n",
    "t_vals = np.linspace(0, 2 * np.pi, 100)\n",
    "circle_points = np.array([\n",
    "    np.cos(alpha4) * n + np.sin(alpha4) * (np.cos(t) * u + np.sin(t) * w)\n",
    "    for t in t_vals\n",
    "])\n",
    "\n",
    "ax.plot(circle_points[:, 0], circle_points[:, 1], circle_points[:, 2], color='darkorange', linewidth=2,ls='--')\n",
    "\n",
    "# Set equal aspect ratio for all axes\n",
    "max_range = r * 1.2\n",
    "for axis in 'xyz':\n",
    "    getattr(ax, 'set_{}lim'.format(axis))((-max_range, max_range))\n",
    "\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot < np.cos(alpha3)) + (dot > np.cos(alpha4))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# -----------------------------\n",
    "# Plot everything\n",
    "# -----------------------------\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot > np.cos(alpha3)) + (dot < np.cos(alpha4))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='darkorange',shade=False, alpha=0.5, rstride=2, cstride=2, edgecolor='none')\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot > np.cos(alpha2)) + (dot < np.cos(alpha3))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='sandybrown',shade=False, alpha=0.5, rstride=2, cstride=2, edgecolor='none')\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot > np.cos(alpha1)) + (dot < np.cos(alpha2))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='lightseagreen',shade=False, alpha=0.5, rstride=2, cstride=2, edgecolor='none')\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot < np.cos(alpha1))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='paleturquoise',alpha=0.5,linewidth=0,rstride=1, cstride=1, shade=False,)\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot > np.cos(alpha4))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='gray', alpha=0.2, rstride=2, cstride=2, edgecolor='none')\n",
    "\n",
    "ax.axis('equal')\n",
    "ax.axis('off')\n",
    "ax.view_init(elev=20, azim=-85)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Reduced NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Reduced SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')\n",
    "\n",
    "ax.legend(\n",
    "    handles=[minLS_patch,minSBI_patch,fullLS_patch,fullSBI_patch],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=2,\n",
    "    bbox_to_anchor=(0.18, 0.09),fontsize=18,\n",
    "    columnspacing=0.5,\n",
    "    handlelength=0.8,\n",
    ")\n",
    "ax.set_title('Average angle diff.',x=0.52, y=0.825,fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c15aad0",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9e1c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T09:42:56.856638Z",
     "start_time": "2025-12-10T09:42:56.654280Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,4))  \n",
    "g_pos = np.array([0, 2, 4,6])*2\n",
    "colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "BoxPlots(SBI_Errors[:,:,1],g_pos,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "POSITIONS = g_pos+0.5\n",
    "colors = ['cadetblue','cadetblue','cadetblue','cadetblue']\n",
    "colors2 = ['darkturquoise','darkturquoise','darkturquoise','darkturquoise']\n",
    "BoxPlots(SBI_Errors_Min[:,:,1],POSITIONS,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "POSITIONS = g_pos+1.5\n",
    "colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "BoxPlots(LS_Errors[:,:,1],POSITIONS,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "POSITIONS = g_pos+2\n",
    "colors = ['darkorange','darkorange','darkorange','darkorange']\n",
    "colors2 = ['orange','orange','orange','orange']\n",
    "BoxPlots(LS_Errors_Min[:,:,1],POSITIONS,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "ax.set_xticks([1,5,9,13],['2','10','20','30'],fontsize=24)\n",
    "ax.set_xlabel('SNR',fontsize=32)\n",
    "ax.tick_params(axis='x', labelsize=24)\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Reduced NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Reduced SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')\n",
    "\n",
    "ax.legend(\n",
    "    handles=[minLS_patch,minSBI_patch],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=2,\n",
    "    bbox_to_anchor=(0.12, 0.88),fontsize=24,\n",
    "    columnspacing=0.5,\n",
    "    handlelength=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de8b87",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca14938",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T09:43:34.103598Z",
     "start_time": "2025-12-10T09:43:33.910211Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,4))  \n",
    "g_pos = np.array([0, 2, 4,6])*2\n",
    "colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "BoxPlots(SBI_Errors[:,:,-1],g_pos,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "POSITIONS = g_pos+0.5\n",
    "colors = ['cadetblue','cadetblue','cadetblue','cadetblue']\n",
    "colors2 = ['darkturquoise','darkturquoise','darkturquoise','darkturquoise']\n",
    "BoxPlots(SBI_Errors_Min[:,:,-1],POSITIONS,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "POSITIONS = g_pos+1.5\n",
    "colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "BoxPlots(LS_Errors[:,:,-1],POSITIONS,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "POSITIONS = g_pos+2\n",
    "colors = ['darkorange','darkorange','darkorange','darkorange']\n",
    "colors2 = ['orange','orange','orange','orange']\n",
    "BoxPlots(LS_Errors_Min[:,:,-1],POSITIONS,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "ax.set_xticks([1,5,9,13],['2','10','20','30'],fontsize=24)\n",
    "ax.set_xlabel('SNR',fontsize=32)\n",
    "ax.tick_params(axis='x', labelsize=24)\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Reduced NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Reduced SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')\n",
    "\n",
    "ax.legend(\n",
    "    handles=[fullLS_patch,fullSBI_patch],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=2,\n",
    "    bbox_to_anchor=(0.18, 0.88),fontsize=24,\n",
    "    columnspacing=0.5,\n",
    "    handlelength=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0d2f5a",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6020df5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T09:43:55.820490Z",
     "start_time": "2025-12-10T09:43:55.632926Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,4))  \n",
    "g_pos = np.array([0, 2, 4,6])*2\n",
    "colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "BoxPlots(SBI_Errors[:,:,-5],g_pos,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "POSITIONS = g_pos+0.5\n",
    "colors = ['cadetblue','cadetblue','cadetblue','cadetblue']\n",
    "colors2 = ['darkturquoise','darkturquoise','darkturquoise','darkturquoise']\n",
    "BoxPlots(SBI_Errors_Min[:,:,-5],POSITIONS,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "POSITIONS = g_pos+1.5\n",
    "colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "BoxPlots(LS_Errors[:,:,-5],POSITIONS,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "POSITIONS = g_pos+2\n",
    "colors = ['darkorange','darkorange','darkorange','darkorange']\n",
    "colors2 = ['orange','orange','orange','orange']\n",
    "BoxPlots(LS_Errors_Min[:,:,-5],POSITIONS,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "ax.set_xticks([1,5,9,13],['2','10','20','30'],fontsize=24)\n",
    "ax.set_xlabel('SNR',fontsize=32)\n",
    "ax.tick_params(axis='x', labelsize=24)\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Reduced NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Reduced SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e29552",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1dc57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T11:22:35.559986Z",
     "start_time": "2025-12-10T11:22:35.388023Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,4))  \n",
    "g_pos = np.array([0, 2, 4,6])*2\n",
    "colors = ['lightseagreen','lightseagreen','lightseagreen','lightseagreen']\n",
    "colors2 = ['paleturquoise','paleturquoise','paleturquoise','paleturquoise']\n",
    "BoxPlots(SBI_Errors[:,:,-3],g_pos,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "POSITIONS = g_pos+0.5\n",
    "colors = ['cadetblue','cadetblue','cadetblue','cadetblue']\n",
    "colors2 = ['darkturquoise','darkturquoise','darkturquoise','darkturquoise']\n",
    "BoxPlots(SBI_Errors_Min[:,:,-3],POSITIONS,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "POSITIONS = g_pos+1.5\n",
    "colors = ['sandybrown','sandybrown','sandybrown','sandybrown']\n",
    "colors2 = ['peachpuff','peachpuff','peachpuff','peachpuff']\n",
    "BoxPlots(LS_Errors[:,:,-3],POSITIONS,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "POSITIONS = g_pos+2\n",
    "colors = ['darkorange','darkorange','darkorange','darkorange']\n",
    "colors2 = ['orange','orange','orange','orange']\n",
    "BoxPlots(LS_Errors_Min[:,:,-3],POSITIONS,colors,colors2,ax,widths=0.5,scatter=True)\n",
    "\n",
    "ax.set_xticks([1,5,9,13],['2','10','20','30'],fontsize=24)\n",
    "ax.set_xlabel('SNR',fontsize=32)\n",
    "ax.tick_params(axis='x', labelsize=24)\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Reduced NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Reduced SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767acbec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T09:31:55.029689Z",
     "start_time": "2025-12-09T09:31:54.975146Z"
    }
   },
   "source": [
    "## f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b31e40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:36:52.620037Z",
     "start_time": "2025-12-15T10:36:39.126089Z"
    }
   },
   "outputs": [],
   "source": [
    "Dir = MSDir+'/Ctrl055_R01_28/'\n",
    "dat = pmt.read_mat(Dir+'data_loaded.mat')\n",
    "bvecs = dat['direction']\n",
    "bvals = dat['bval']\n",
    "FixedParams = {\n",
    "    'bvals':bvals,\n",
    "    'bvecs':bvecs,\n",
    "    'Delta':[0.017,0.035,0.061],\n",
    "    'delta':0.007,\n",
    "}\n",
    "Delta = FixedParams['Delta']\n",
    "delta = FixedParams['delta']\n",
    "n_pts = 90\n",
    "\n",
    "Delta = [0.017,0.035,0.061] # We know this \n",
    "delta = 0.007 # We know this \n",
    "\n",
    "\n",
    "data = dat['data']\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(0, 10), median_radius=5,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "\n",
    "S_mask, _, _ = load_nifti(Dir+'mask_055.nii.gz', return_img=True)\n",
    "\n",
    "\n",
    "mask1 = np.ones_like(S_mask[:,54,:])\n",
    "mask1[S_mask[:,54,:]==0] = 0\n",
    "structure = np.ones((3, 3), dtype=bool)\n",
    "\n",
    "floor = np.clip(maskdata.min(axis=-1),-np.inf,0)\n",
    "maskdata_2 = np.copy(maskdata)\n",
    "maskdata_2[floor <=0 ] = maskdata[floor <= 0] + abs(floor)[floor <=0 ,None] + 1e-5\n",
    "\n",
    "# Apply dilation. Increase 'iterations' to make the mask even fatter.\n",
    "fat_mask = binary_dilation(mask1, structure=structure, iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb1ab1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:38:01.854681Z",
     "start_time": "2025-12-15T10:36:52.621212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "def optimize_chunk(pixels):\n",
    "    results = []\n",
    "    for i, j in pixels:\n",
    "        samples = Network.sample((1000,), x=AxcaliberFeatures(bvecs[:],bvals[:],Deltas[:],maskdata_2[i, j,axial_middle, :]),show_progress_bars=False)        \n",
    "        results.append((i, j, samples.mean(axis=0)))\n",
    "    return results\n",
    "\n",
    "chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    ")\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for chunk in results:\n",
    "    for i, j, x in chunk:\n",
    "        NoiseEst[i, j] = x\n",
    "        NoiseEst[i, j,-2] = np.clip(NoiseEst[i, j,-2],0,100)\n",
    "        NoiseEst[i, j,-3] = np.clip(NoiseEst[i, j,-3],0,1)\n",
    "NoiseEst2 = np.copy(NoiseEst)\n",
    "\n",
    "for i in range(13):\n",
    "    NoiseEst2[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2[(1-NoiseEst2[...,-3])<0.3,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d7d10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:38:23.271820Z",
     "start_time": "2025-12-15T10:38:23.154966Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "im = plt.imshow(1-NoiseEst2[...,-3],vmin=0.1,vmax=1,cmap='hot')\n",
    "plt.axis('off')\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94856e7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:54:58.695393Z",
     "start_time": "2025-12-10T21:54:58.639014Z"
    }
   },
   "outputs": [],
   "source": [
    "Save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc706b70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T08:42:13.057980Z",
     "start_time": "2025-12-27T08:42:13.009589Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(133)\n",
    "S0 = 2000\n",
    "MD_prior = np.random.rand(1)*0.005\n",
    "FA_prior = np.random.rand(1)*0.999\n",
    "DHind_guess = [mat_to_vals(random_diffusion_tensor(m, f)) for m,f in zip(MD_prior,FA_prior)]\n",
    "\n",
    "Dpar_guess = np.random.rand()*1e-3            # mm^2/s\n",
    "Dperp_guess = np.random.rand()*1e-3             # mm^2/s\n",
    "phi = 0#np.random.rand()*pi\n",
    "cos_theta = 0#np.random.rand()  # uniform in [0,1]\n",
    "theta = np.arccos(cos_theta)         # in [0, pi/2]\n",
    "Angs_guess = np.vstack([theta,phi]).T\n",
    "S0_guess =np.random.rand()*2475+25\n",
    "\n",
    "frac_guess = np.random.rand()\n",
    "guess = np.column_stack([Angs_guess,Dpar_guess,Dperp_guess,DHind_guess,frac_guess,mean_guess,S0_guess]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265414c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T08:42:59.588461Z",
     "start_time": "2025-12-27T08:42:59.536035Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(133)\n",
    "S0 = 2000\n",
    "MD_prior = np.random.rand(1)*0.005\n",
    "FA_prior = np.random.rand(1)*0.999\n",
    "DHind_guess = [mat_to_vals(random_diffusion_tensor(m, f)) for m,f in zip(MD_prior,FA_prior)]\n",
    "\n",
    "Dpar_guess = np.random.rand()*1e-3            # mm^2/s\n",
    "Dperp_guess = np.random.rand()*1e-3             # mm^2/s\n",
    "phi = 0#np.random.rand()*pi\n",
    "cos_theta = 0#np.random.rand()  # uniform in [0,1]\n",
    "theta = np.arccos(cos_theta)         # in [0, pi/2]\n",
    "Angs_guess = np.vstack([theta,phi]).T\n",
    "S0_guess =np.random.rand()*2475+25\n",
    "\n",
    "frac_guess = np.random.rand()\n",
    "guess = np.column_stack([Angs_guess,Dpar_guess,Dperp_guess,DHind_guess,frac_guess,mean_guess,S0_guess]).squeeze()\n",
    "bounds = np.array([[-np.inf,np.inf]]*13).T\n",
    "bounds[:,0] = [0,np.pi/2]\n",
    "bounds[:,1] = [-np.pi,np.pi]\n",
    "bounds[:,2] = [0,5e-3]\n",
    "bounds[:,3] = [0,5e-3]\n",
    "bounds[:,4] = [-5e-3,5e-3]\n",
    "bounds[:,5] = [-5e-3,5e-3]\n",
    "bounds[:,6] = [-5e-3,5e-3]\n",
    "bounds[:,7] = [-5e-3,5e-3]\n",
    "bounds[:,8] = [-5e-3,5e-3]\n",
    "bounds[:,9] = [-5e-3,5e-3]\n",
    "bounds[:,10] = [0,1]\n",
    "bounds[:,11] = [1e-4,0.005+1e-4]\n",
    "bounds[:,12] = [25,2500]\n",
    "\n",
    "bve_split = [bvecs[:(n_pts+1)],bvecs[(n_pts+1):2*(n_pts+1)],bvecs[2*(n_pts+1):]]\n",
    "bva_split = [bvals[:(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],bvals[2*(n_pts+1):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daad2a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:41:34.559451Z",
     "start_time": "2025-12-15T10:38:36.396575Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "def optimize_pixel_LS(i, j):\n",
    "    result = sp.optimize.least_squares(residuals, guess, args=[maskdata_2[i, j, axial_middle, :],bve_split,bva_split,Delta,True],\n",
    "                              bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "    return i, j, result.x\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices,position=0,leave=True)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_LS[i, j] = x\n",
    "\n",
    "NoiseEst2_LS = np.copy(NoiseEst_LS)\n",
    "for i in range(13):\n",
    "    NoiseEst2_LS[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2_LS[(1-NoiseEst2_LS[...,-3])<0.3,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7cad6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:41:37.186672Z",
     "start_time": "2025-12-15T10:41:37.046704Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "im = plt.imshow(1-NoiseEst2_LS[...,-3],vmin=0.0,vmax=1,cmap='hot')\n",
    "cbar = plt.colorbar(im,fraction=0.035, pad=-0.1)\n",
    "cbar.ax.tick_params(labelsize=32)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c4048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:42:20.414019Z",
     "start_time": "2025-12-15T10:41:45.765920Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "def optimize_chunk(pixels):\n",
    "    results = []\n",
    "    for i, j in pixels:\n",
    "        samples = Network.sample((1000,), x=AxcaliberFeatures(bvecs[DevilIndices],bvals[DevilIndices],Deltas[DevilIndices],maskdata_2[i, j,axial_middle, DevilIndices]),show_progress_bars=False)        \n",
    "        results.append((i, j, samples.mean(axis=0)))\n",
    "    return results\n",
    "\n",
    "chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    ")\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for chunk in results:\n",
    "    for i, j, x in chunk:\n",
    "        NoiseEst[i, j] = x\n",
    "        NoiseEst[i, j,-2] = np.clip(NoiseEst[i, j,-2],0,100)\n",
    "        NoiseEst[i, j,-3] = np.clip(NoiseEst[i, j,-3],0,1)\n",
    "NoiseEst2_min = np.copy(NoiseEst)\n",
    "\n",
    "for i in range(13):\n",
    "    NoiseEst2_min[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2_min[(1-NoiseEst2_min[...,-3])<0.3,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7c177",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:42:23.935908Z",
     "start_time": "2025-12-15T10:42:23.814954Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "temp = gaussian_filter(1-NoiseEst2_min[...,-3], sigma=0.5)\n",
    "im = plt.imshow(temp,vmin=0.1,vmax=1,cmap='hot')\n",
    "plt.axis('off')\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e56a102",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:46:13.940288Z",
     "start_time": "2025-12-15T10:42:41.073306Z"
    }
   },
   "outputs": [],
   "source": [
    "bve_splitD = [bvecs_Dev[:7],bvecs_Dev[7:14],bvecs_Dev[14:]]\n",
    "bva_splitD = [bvals_Dev[:7],bvals_Dev[7:14],bvals_Dev[14:]]\n",
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel_LS(i, j):\n",
    "    result = sp.optimize.least_squares(residuals, guess, args=[maskdata[i, j,axial_middle, DevilIndices,True],bve_splitD,bva_splitD,Delta],\n",
    "                          bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "    return i, j, result.x\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices,position=0,leave = True)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_LS_Min = np.zeros(list(ArrShape) + [13])\n",
    "bve_splitD\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_LS_Min[i, j] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c994b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:48:45.055496Z",
     "start_time": "2025-12-15T10:48:44.916081Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "temp = gaussian_filter(1-NoiseEst_LS_Min[...,-3], sigma=0.5)\n",
    "im = plt.imshow(temp,vmin=0.0,vmax=1,cmap='hot')\n",
    "cbar = plt.colorbar(im,fraction=0.035, pad=-0.1)\n",
    "cbar.ax.tick_params(labelsize=32)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c454948",
   "metadata": {},
   "source": [
    "## g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53070b82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T10:15:29.049167Z",
     "start_time": "2025-12-09T10:15:01.349107Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, 54, :, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "def optimize_chunk(pixels):\n",
    "    results = []\n",
    "    for i, j in pixels:\n",
    "        posterior_samples_1 = Network.sample((1000,), x=AxcaliberFeatures(bvecs[:],bvals[:],Deltas[:],maskdata[i, 54,j, :]),show_progress_bars=False)        \n",
    "        results.append((i, j, posterior_samples_1.mean(axis=0)))\n",
    "    return results\n",
    "\n",
    "chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    ")\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "NoiseEst_CC = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for chunk in results:\n",
    "    for i, j, x in chunk:\n",
    "        NoiseEst_CC[i, j] = x\n",
    "        NoiseEst_CC[i, j,-2] = np.clip(NoiseEst_CC[i, j,-2],0,100)\n",
    "        NoiseEst_CC[i, j,-3] = np.clip(NoiseEst_CC[i, j,-3],0,1)\n",
    "NoiseEst2_CC = np.copy(NoiseEst_CC)\n",
    "\n",
    "comb_mask = mask1.astype(bool) * ((1-NoiseEst2_CC[...,-3])>0.1)\n",
    "\n",
    "mask_CC = (1-NoiseEst2_CC[...,-3])<0.3\n",
    "for i in range(13):\n",
    "    NoiseEst2_CC[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2_CC[~comb_mask,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4729be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T10:16:04.718034Z",
     "start_time": "2025-12-09T10:15:47.618512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, 54, :, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "def optimize_chunk(pixels):\n",
    "    results = []\n",
    "    for i, j in pixels:\n",
    "        posterior_samples_1 = Network.sample((1000,), x=AxcaliberFeatures(bvecs[DevilIndices],bvals[DevilIndices],Deltas[DevilIndices],maskdata[i, 54,j, DevilIndices]),show_progress_bars=False)        \n",
    "        results.append((i, j, posterior_samples_1.mean(axis=0)))\n",
    "    return results\n",
    "\n",
    "chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    ")\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "NoiseEst_Min_CC = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for chunk in results:\n",
    "    for i, j, x in chunk:\n",
    "        NoiseEst_Min_CC[i, j] = x\n",
    "        NoiseEst_Min_CC[i, j,-2] = np.clip(NoiseEst_Min_CC[i, j,-2],0,100)\n",
    "        NoiseEst_Min_CC[i, j,-3] = np.clip(NoiseEst_Min_CC[i, j,-3],0,1)\n",
    "NoiseEst_Min_CC = np.copy(NoiseEst_Min_CC)\n",
    "\n",
    "for i in range(13):\n",
    "    NoiseEst_Min_CC[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst_Min_CC[~comb_mask,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76d303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:43:41.209975Z",
     "start_time": "2025-12-10T10:43:41.028548Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "plt.imshow(np.flipud(NoiseEst2_CC[...,-1].T),cmap='gray')\n",
    "im = plt.imshow(np.flipud(NoiseEst2_CC[...,-2].T),cmap='hot',vmin=0.001,vmax=0.006)\n",
    "#cbar = plt.colorbar(im,fraction=0.035, pad=0.01,format=ticker.FormatStrFormatter('%.e'))\n",
    "#cbar.ax.tick_params(labelsize=14)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ca748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:43:42.096980Z",
     "start_time": "2025-12-10T10:43:41.912577Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "plt.imshow(np.flipud(NoiseEst_Min_CC[...,-1].T),cmap='gray')\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=0.0042, vmax=0.006)\n",
    "im = plt.imshow(np.flipud(NoiseEst_Min_CC[...,-2].T),cmap='hot',norm=norm)\n",
    "#cbar = plt.colorbar(im,fraction=0.035, pad=0.01,format=ticker.FormatStrFormatter('%.e'))\n",
    "#cbar.ax.tick_params(labelsize=14)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e3cf18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T10:21:51.969731Z",
     "start_time": "2025-12-09T10:16:29.744597Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, 54, :, :], axis=-1) != 0\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel_LS(i, j):\n",
    "    result = sp.optimize.least_squares(residuals, guess, args=[maskdata[i, 54, j, :],bve_split,bva_split,Delta,True],\n",
    "                              bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "    return i, j, result.x\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices,position=0,leave=True)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_LS_CC = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_LS_CC[i, j] = x\n",
    "\n",
    "NoiseEst2_LS_CC = np.copy(NoiseEst_LS_CC)\n",
    "\n",
    "for i in range(13):\n",
    "    NoiseEst2_LS_CC[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2_LS_CC[~comb_mask,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e48f4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T10:28:17.086059Z",
     "start_time": "2025-12-09T10:22:52.692370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, 54, :, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "def optimize_pixel_LS(i, j):\n",
    "    result = sp.optimize.least_squares(residuals, guess, args=[maskdata[i,54,j, DevilIndices,True],bve_splitD,bva_splitD,Delta],\n",
    "                          bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "    return i, j, result.x\n",
    "\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "\n",
    "\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices,position=0,leave=True)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_LS_Min_CC = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_LS_Min_CC[i, j] = x\n",
    "NoiseEst2_LS_Min_CC = np.copy(NoiseEst_LS_Min_CC)\n",
    "\n",
    "for i in range(13):\n",
    "    NoiseEst2_LS_Min_CC[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2_LS_Min_CC[~comb_mask,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a6159",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:45:01.308043Z",
     "start_time": "2025-12-10T10:45:01.113369Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "plt.imshow(np.flipud(NoiseEst2_LS_CC[...,-1].T),cmap='gray')\n",
    "im = plt.imshow(np.flipud(NoiseEst2_LS_CC[...,-2].T)*1000,cmap='hot',vmin=0,vmax=6)\n",
    "cbar = plt.colorbar(im,fraction=0.03, pad=0.01)\n",
    "cbar.ax.tick_params(labelsize=32)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe35c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:44:52.816355Z",
     "start_time": "2025-12-10T10:44:52.604776Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "plt.imshow(np.flipud(NoiseEst2_LS_Min_CC[...,-1].T),cmap='gray')\n",
    "im = plt.imshow(np.flipud(NoiseEst2_LS_Min_CC[...,-2].T)*1000,cmap='hot',vmin=0,vmax=6)\n",
    "cbar = plt.colorbar(im,fraction=0.03, pad=0.01)\n",
    "cbar.ax.tick_params(labelsize=32)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd0b5be",
   "metadata": {},
   "source": [
    "## h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea12d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T10:39:27.537864Z",
     "start_time": "2025-12-09T10:38:47.710536Z"
    }
   },
   "outputs": [],
   "source": [
    "Dirs = ['Ctrl055_R01_28','Ctrl056_R01_29','Ctrl057_R01_30']\n",
    "Masks = ['mask_055.nii.gz','mask_056.nii.gz','mask_057.nii.gz']\n",
    "BVecs = []\n",
    "BVals = []\n",
    "S_masks = []\n",
    "Datas = []\n",
    "Outlines = []\n",
    "for D,M in tqdm(zip(Dirs,Masks)):\n",
    "    dat = pmt.read_mat(MSDir+D+'/data_loaded.mat')\n",
    "    BVecs.append(dat['direction'])\n",
    "    BVals.append(dat['bval'])\n",
    "    \n",
    "    m, _, _ = load_nifti(MSDir+D+'/'+M, return_img=True)\n",
    "    S_masks.append(m)\n",
    "\n",
    "    data = dat['data']\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    md, mk = median_otsu(data, vol_idx=range(0, 10), median_radius=5,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    Datas.append(md)\n",
    "    Outlines.append(mk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7c0ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T10:35:14.930127Z",
     "start_time": "2025-12-09T10:35:14.875254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "IndxArr  = []\n",
    "BVecsDev = []\n",
    "BValsDev = []\n",
    "for bve,bva in zip(BVecs,BVals): \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[:91][bva[:91]==2000]\n",
    "    distance_matrix = squareform(pdist(bvecs2000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[:91][bva[:91]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[:91][bva[:91]==4000]\n",
    "    distance_matrix = squareform(pdist(bvecs4000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[:91][bva[:91]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices1 = true_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[91:182][bva[91:182]==2000]\n",
    "    distance_matrix = squareform(pdist(bvecs2000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[91:182][bva[91:182]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[91:182][bva[91:182]==4000]\n",
    "    distance_matrix = squareform(pdist(bvecs4000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[91:182][bva[91:182]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices2 = true_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[182:][bva[182:]==2000]\n",
    "    distance_matrix = squareform(pdist(bvecs2000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[182:][bva[182:]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[182:][bva[182:]==4000]\n",
    "    distance_matrix = squareform(pdist(bvecs4000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[182:][bva[182:]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices3 = true_indices\n",
    "    \n",
    "    DevIndices = [0] + true_indices1 + [n_pts] +  true_indices2 + [n_pts+1] + true_indices3\n",
    "    bvecs_Dev = bve[DevIndices]\n",
    "    bvals_Dev = bva[DevIndices]\n",
    "\n",
    "    IndxArr.append(DevIndices)\n",
    "    BVecsDev.append(bvecs_Dev)\n",
    "    BValsDev.append(bvals_Dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c865d1e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T10:41:30.289842Z",
     "start_time": "2025-12-09T10:41:12.766504Z"
    }
   },
   "outputs": [],
   "source": [
    "Full_SBI = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[54,52,54],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = sma[:,sl,:]\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = Network.sample((1000,), x=AxcaliberFeatures(BVecs[kk],BVals[kk],Deltas,D[i, sl, j, :]),show_progress_bars=False)\n",
    "        return i, j, posterior_samples_1.mean(axis=0)\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "\n",
    "    Full_SBI.append(NoiseEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af6b29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T10:42:42.210226Z",
     "start_time": "2025-12-09T10:42:24.668877Z"
    }
   },
   "outputs": [],
   "source": [
    "Min_SBI = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[54,52,54],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = sma[:,sl,:]\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = Network.sample((1000,), x=AxcaliberFeatures(BVecs[kk][IndxArr[kk]],BVals[kk][IndxArr[kk]],Deltas[IndxArr[kk]],D[i, sl, j, IndxArr[kk]]),show_progress_bars=False)\n",
    "        return i, j, posterior_samples_1.mean(axis=0)\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices,position=0,leave=True)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "\n",
    "    Min_SBI.append(NoiseEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bea23f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T10:44:51.528712Z",
     "start_time": "2025-12-09T10:43:28.093710Z"
    }
   },
   "outputs": [],
   "source": [
    "Full_LS = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[54,52,54],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = sma[:,sl,:]\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    bve_split_kk = [BVecs[kk][:(n_pts+1)],BVecs[kk][(n_pts+1):2*(n_pts+1)],BVecs[kk][2*(n_pts+1):]]\n",
    "    bva_split_kk = [BVals[kk][:(n_pts+1)],BVals[kk][(n_pts+1):2*(n_pts+1)],BVals[kk][2*(n_pts+1):]]\n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel_LS(i, j):\n",
    "        result = sp.optimize.least_squares(residuals, guess, args=[D[i, sl, j, :],bve_split_kk,bva_split_kk,Delta,True],\n",
    "                                  bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        return i, j, result.x\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices,position=0,leave=True)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst_LS[i, j] = x\n",
    "\n",
    "    Full_LS.append(NoiseEst_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd082b31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T10:49:17.000331Z",
     "start_time": "2025-12-09T10:44:51.530278Z"
    }
   },
   "outputs": [],
   "source": [
    "Min_LS = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[54,52,54],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = sma[:,sl,:]\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    bve_splitd_kk = [BVecsDev[kk][:7],BVecsDev[kk][7:13],BVecsDev[kk][13:]]\n",
    "    bva_splitd_kk = [BValsDev[kk][:7],BValsDev[kk][7:13],BValsDev[kk][13:]]\n",
    "\n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel_LS(i, j):\n",
    "        result = sp.optimize.least_squares(residuals, guess, args=[D[i, sl, j, IndxArr[kk]],bve_splitd_kk,bva_splitd_kk,Delta,True],\n",
    "                                  bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        return i, j, result.x\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices,position=0,leave=True)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst_LS[i, j] = x\n",
    "\n",
    "    Min_LS.append(NoiseEst_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f0ac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T10:54:41.238594Z",
     "start_time": "2025-12-09T10:54:41.181345Z"
    }
   },
   "outputs": [],
   "source": [
    "CMasks = []\n",
    "kk = 0\n",
    "d  = 54\n",
    "temp = np.copy(Full_SBI[kk])\n",
    "for i in range(13):\n",
    "    temp[~Outlines[kk][:,d,:],i] = math.nan\n",
    "    \n",
    "mask1 = np.ones_like(S_masks[kk][:,d,:])\n",
    "mask1[S_masks[kk][:,d,:]==0] = 0\n",
    "\n",
    "# Apply dilation. Increase 'iterations' to make the mask even fatter.\n",
    "fat_mask = binary_dilation(mask1, structure=structure, iterations=1)\n",
    "\n",
    "CMasks.append(fat_mask * ((1-temp[...,-4])>0.1) * (temp[...,-4]>0))\n",
    "\n",
    "kk = 1\n",
    "d  = 52\n",
    "temp = np.copy(Full_SBI[kk])\n",
    "for i in range(13):\n",
    "    temp[~Outlines[kk][:,d,:],i] = math.nan\n",
    "    \n",
    "mask1 = np.ones_like(S_masks[kk][:,d,:])\n",
    "mask1[S_masks[kk][:,d,:]==0] = 0\n",
    "\n",
    "# Apply dilation. Increase 'iterations' to make the mask even fatter.\n",
    "fat_mask = binary_dilation(mask1, structure=structure, iterations=1)\n",
    "\n",
    "CMasks.append(fat_mask * ((1-temp[...,-4])>0) * (temp[...,-4]>0))\n",
    "\n",
    "kk = 2\n",
    "d  = 54\n",
    "temp = np.copy(Full_SBI[kk])\n",
    "for i in range(13):\n",
    "    temp[~Outlines[kk][:,d,:],i] = math.nan\n",
    "    \n",
    "mask1 = np.ones_like(S_masks[kk][:,d,:])\n",
    "mask1[S_masks[kk][:,d,:]==0] = 0\n",
    "# Apply dilation. Increase 'iterations' to make the mask even fatter.\n",
    "fat_mask = binary_dilation(mask1, structure=structure, iterations=1)\n",
    "CMasks.append(fat_mask * ((1-temp[...,-4])>0.3) * (temp[...,-4]>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14351f54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T11:30:17.705431Z",
     "start_time": "2025-12-10T11:30:17.161596Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "g_pos = np.array([0,0.25,0.5])\n",
    "colors = ['lightseagreen','lightseagreen','lightseagreen']\n",
    "colors2 = ['paleturquoise','paleturquoise','paleturquoise']\n",
    "for i in range(3):\n",
    "    y_dat = 1000*abs(Min_SBI[i][CMasks[i]][:,-2]-Full_SBI[i][CMasks[i]][:,-2]).squeeze()\n",
    "    BoxPlots(y_dat,[g_pos[i]],[colors[i]],[colors2[i]],ax,scatter=True)\n",
    "\n",
    "g_pos = np.array([2,2.25,2.5])\n",
    "colors = ['darkorange','darkorange','darkorange']\n",
    "colors2 = ['peachpuff','peachpuff','peachpuff']\n",
    "y_data = [1000*abs(Full_LS[i][CMasks[i]][:,-2]-Min_LS[i][CMasks[i]][:,-2]) for i in range(3)]\n",
    "for i in range(3):\n",
    "    y_dat = y_data[i]\n",
    "    BoxPlots(y_dat,[g_pos[i]],[colors[i]],[colors2[i]],ax,scatter=True)\n",
    "\n",
    "ax.set_xticks([0.25,2.25],['SBI Comp','NLLS Comp'],fontsize =36)\n",
    "\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e94ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5dde02f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T11:13:32.267809Z",
     "start_time": "2025-12-09T11:13:32.221128Z"
    }
   },
   "source": [
    "## i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc3498",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T08:14:19.251502Z",
     "start_time": "2025-12-27T08:12:30.533428Z"
    }
   },
   "outputs": [],
   "source": [
    "Dirs = ['NMSS_11_1year','NMSS_15','NMSS_16','NMSS_18','NMSS_19','Ctrl055_R01_28','Ctrl056_R01_29','Ctrl057_R01_30']\n",
    "BVecs = []\n",
    "BVals = []\n",
    "S_masks = []\n",
    "Datas = []\n",
    "Outlines = []\n",
    "axial_middles = []\n",
    "for D in tqdm(Dirs,position=0,leave=True):\n",
    "    F = pmt.read_mat(MSDir+D+'/data_loaded.mat')\n",
    "    affine = np.ones((4,4))\n",
    "    BVecs.append(F['direction'])\n",
    "    BVals.append(F['bval'])\n",
    "\n",
    "\n",
    "    \n",
    "    data, affine = reslice(F['data'], affine, (2,2,2), (2.5,2.5,2.5))\n",
    "\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    md, mk = median_otsu(data, vol_idx=range(0, 10), median_radius=5,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "\n",
    "    floor = np.clip(md.min(axis=-1),-np.inf,0)\n",
    "    maskdata_2 = np.copy(md)\n",
    "    maskdata_2[floor <=0 ] = md[floor <= 0] + abs(floor)[floor <=0 ,None] + 1e-5\n",
    "    Datas.append(maskdata_2)\n",
    "    axial_middles.append(axial_middle)\n",
    "    Outlines.append(mk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6fc19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T08:14:19.334850Z",
     "start_time": "2025-12-27T08:14:19.252336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "IndxArr  = []\n",
    "BVecsDev = []\n",
    "BValsDev = []\n",
    "for bve,bva in zip(BVecs,BVals): \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[:91][bva[:91]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[:91][bva[:91]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[:91][bva[:91]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[:91][bva[:91]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices1 = true_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[91:182][bva[91:182]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[91:182][bva[91:182]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[91:182][bva[91:182]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[91:182][bva[91:182]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices2 = true_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[182:][bva[182:]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[182:][bva[182:]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[182:][bva[182:]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[182:][bva[182:]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices3 = true_indices\n",
    "    \n",
    "    DevIndices = [0] + true_indices1 + [n_pts] +  true_indices2 + [n_pts+1] + true_indices3\n",
    "    bvecs_Dev = bve[DevIndices]\n",
    "    bvals_Dev = bva[DevIndices]\n",
    "\n",
    "    IndxArr.append(DevIndices)\n",
    "    BVecsDev.append(bvecs_Dev)\n",
    "    BValsDev.append(bvals_Dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24cec0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T08:24:28.249377Z",
     "start_time": "2025-12-27T08:14:19.335481Z"
    }
   },
   "outputs": [],
   "source": [
    "Full_SBI_Extra = []\n",
    "for kk,(D,sl) in enumerate(zip(Datas,[48]*8)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    def optimize_chunk(pixels):\n",
    "        results = []\n",
    "        for i, j in pixels:\n",
    "            posterior_samples_1 = Network.sample((1000,), x=AxcaliberFeatures(BVecs[kk],BVals[kk],Deltas,D[i,j,sl, :]),show_progress_bars=False)\n",
    "            results.append((i, j, posterior_samples_1.mean(axis=0)))\n",
    "        return results\n",
    "    \n",
    "    chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    "    )\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for chunk in results:\n",
    "        for i, j, x in chunk:\n",
    "            NoiseEst[i, j] = x\n",
    "\n",
    "    Full_SBI_Extra.append(NoiseEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192afc15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T08:33:46.988978Z",
     "start_time": "2025-12-27T08:24:28.250738Z"
    }
   },
   "outputs": [],
   "source": [
    "Min_SBI_Extra = []\n",
    "for kk,(D,sl) in enumerate(zip(Datas,[48]*8)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    Arr = D[:,:,sl, IndxArr[kk]]\n",
    "    def optimize_chunk(pixels):\n",
    "        results = []\n",
    "        for i, j in pixels:\n",
    "            posterior_samples_1 = Network.sample((1000,), x=AxcaliberFeatures(BVecs[kk][IndxArr[kk]],BVals[kk][IndxArr[kk]],Deltas[IndxArr[kk]],Arr[i,j,:]),show_progress_bars=False)\n",
    "            results.append((i, j, posterior_samples_1.mean(axis=0)))\n",
    "        return results\n",
    "    \n",
    "    chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices)\n",
    "    )\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for chunk in results:\n",
    "        for i, j, x in chunk:\n",
    "            NoiseEst[i, j] = x\n",
    "\n",
    "    Min_SBI_Extra.append(NoiseEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f3966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T09:41:25.230308Z",
     "start_time": "2025-12-27T08:43:12.348430Z"
    }
   },
   "outputs": [],
   "source": [
    "Full_LS_extra = []\n",
    "for kk,(D,sl) in enumerate(zip(Datas,[48]*8)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "\n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "\n",
    "    bve_split_kk = [BVecs[kk][:(n_pts+1)],BVecs[kk][(n_pts+1):2*(n_pts+1)],BVecs[kk][2*(n_pts+1):]]\n",
    "    bva_split_kk = [BVals[kk][:(n_pts+1)],BVals[kk][(n_pts+1):2*(n_pts+1)],BVals[kk][2*(n_pts+1):]]\n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel_LS(i, j):\n",
    "        result = sp.optimize.least_squares(residuals, guess, args=[D[i,j,sl, :],bve_split_kk,bva_split_kk,Delta,True],\n",
    "                                  bounds=bounds,verbose=0,jac='3-point')\n",
    "        return i, j, result.x\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "\n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices,position=0,leave=True)\n",
    "    )\n",
    "\n",
    "\n",
    "    NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst_LS[i, j] = x\n",
    "\n",
    "    Full_LS_extra.append(NoiseEst_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4068c3e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:51:10.406858Z",
     "start_time": "2025-12-27T09:41:25.232681Z"
    }
   },
   "outputs": [],
   "source": [
    "Min_LS_extra = []\n",
    "for kk,(D,sl) in enumerate(zip(Datas,[48]*8)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "\n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "\n",
    "    bve_splitd_kk = [BVecsDev[kk][:7],BVecsDev[kk][7:13],BVecsDev[kk][13:]]\n",
    "    bva_splitd_kk = [BValsDev[kk][:7],BValsDev[kk][7:13],BValsDev[kk][13:]]\n",
    "\n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel_LS(i, j):\n",
    "        result = sp.optimize.least_squares(residuals, guess, args=[D[i, j,sl, IndxArr[kk]],bve_splitd_kk,bva_splitd_kk,Delta,True],\n",
    "                                  bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        return i, j, result.x\n",
    "\n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "\n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices,position=0,leave=True)\n",
    "    )\n",
    "\n",
    "\n",
    "    NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst_LS[i, j] = x\n",
    "\n",
    "    Min_LS_extra.append(NoiseEst_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cce310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:52:21.374586Z",
     "start_time": "2025-12-27T11:52:21.080409Z"
    }
   },
   "outputs": [],
   "source": [
    "WMDir = '../../MS_data/WM_masks/'\n",
    "WMs = []\n",
    "for i,Name in tqdm(enumerate(['NMSS_11_1year','NMSS_15','NMSS_16','NMSS_18','NMSS_19','Ctrl055_R01_28','Ctrl056_R01_29','Ctrl057_R01_30'])):\n",
    "    \n",
    "    for k,x in enumerate(os.listdir(WMDir)):\n",
    "        if Name in x:\n",
    "            WM, affine, img = load_nifti(WMDir+x, return_img=True)\n",
    "            #WM, affine = reslice(WM, affine, (2,2,2), (2.5,2.5,2.5))\n",
    "            if(i<5):\n",
    "                WM_t = np.fliplr(np.swapaxes(WM,0,1))\n",
    "            else:\n",
    "                WM_t = np.fliplr(np.flipud(np.swapaxes(WM,0,1)))\n",
    "            WM_t,_ = reslice(WM_t, affine, (2,2,2), (2.5,2.5,2.5))\n",
    "            WMs.append(WM_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ae372",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(133)\n",
    "S0 = 2000\n",
    "MD_prior = np.random.rand(1)*0.005\n",
    "FA_prior = np.random.rand(1)*0.999\n",
    "DHind_guess = [mat_to_vals(random_diffusion_tensor(m, f)) for m,f in zip(MD_prior,FA_prior)]\n",
    "\n",
    "Dpar_guess = np.random.rand()*1e-3            # mm^2/s\n",
    "Dperp_guess = np.random.rand()*1e-3             # mm^2/s\n",
    "phi = 0#np.random.rand()*pi\n",
    "cos_theta = 0#np.random.rand()  # uniform in [0,1]\n",
    "theta = np.arccos(cos_theta)         # in [0, pi/2]\n",
    "Angs_guess = np.vstack([theta,phi]).T\n",
    "S0_guess =np.random.rand()*2475+25\n",
    "\n",
    "mean_guess = np.random.rand()*0.005 + 1e-4\n",
    "\n",
    "frac_guess = np.random.rand()\n",
    "guess = np.column_stack([Angs_guess,Dpar_guess,Dperp_guess,DHind_guess,frac_guess,mean_guess,S0_guess]).squeeze()\n",
    "bounds = np.array([[-np.inf,np.inf]]*13).T\n",
    "bounds[:,0] = [0,np.pi/2]\n",
    "bounds[:,1] = [-np.pi,np.pi]\n",
    "bounds[:,2] = [0,5e-3]\n",
    "bounds[:,3] = [0,5e-3]\n",
    "bounds[:,4] = [-5e-3,5e-3]\n",
    "bounds[:,5] = [-5e-3,5e-3]\n",
    "bounds[:,6] = [-5e-3,5e-3]\n",
    "bounds[:,7] = [-5e-3,5e-3]\n",
    "bounds[:,8] = [-5e-3,5e-3]\n",
    "bounds[:,9] = [-5e-3,5e-3]\n",
    "bounds[:,10] = [0,1]\n",
    "bounds[:,11] = [1e-4,0.005+1e-4]\n",
    "bounds[:,12] = [25,2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "FullSBI = []\n",
    "FullNLLS = []\n",
    "MinSBI = []\n",
    "MinNLLS = []\n",
    "for jj,N in enumerate(Names):\n",
    "    Subfiles = []\n",
    "    for k,x in enumerate(os.listdir(RetestDir)):\n",
    "        if N in x:\n",
    "            print(x)\n",
    "            Subfiles.append(x)\n",
    "    Subfiles = sorted(Subfiles)\n",
    "\n",
    "    S = Subfiles[0]\n",
    "    MatDir = RetestDir+S\n",
    "    F = pmt.read_mat(MatDir+'/data_loaded.mat')\n",
    "    \n",
    "    bvecs = F['direction']\n",
    "    bvals = F['bval']\n",
    "    Deltas = F['deltas']\n",
    "    data = F['data']\n",
    "\n",
    "    bvecs = bvecs[Deltas != 42]\n",
    "    bvals = bvals[Deltas != 42]\n",
    "    data = data[..., Deltas !=42]\n",
    "    Deltas = Deltas[Deltas != 42]*0.001\n",
    "    n_pts = np.sum(Deltas == Deltas[0])\n",
    "    bve = np.copy(bvecs)\n",
    "    bva = np.copy(bvals)\n",
    "    affine1 = np.eye(4)\n",
    "\n",
    "    data, affine1 = reslice(data, affine1, (2,2,2), (2.5,2.5,2.5))\n",
    "    _, maskCut = median_otsu(data, vol_idx=range(10, 80), autocrop=False)\n",
    "    true_indices = np.argwhere(maskCut)\n",
    "\n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    AM = (max_coords[-1]+min_coords[-1])//2\n",
    "    bvecs = (bvecs.T/np.linalg.norm(bvecs,axis=1)).T\n",
    "    bvecs[np.isnan(bvecs)] = 0\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[:n_pts][bva[:n_pts]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[:n_pts][bva[:n_pts]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[:n_pts][bva[:n_pts]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[:91][bva[:91]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices1 = true_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[n_pts:2*n_pts][bva[n_pts:2*n_pts]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[n_pts:2*n_pts][bva[n_pts:2*n_pts]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[n_pts:2*n_pts][bva[n_pts:2*n_pts]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[n_pts:2*n_pts][bva[n_pts:2*n_pts]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices2 = true_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[2*n_pts:][bva[2*n_pts:]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[2*n_pts:][bva[2*n_pts:]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[2*n_pts:][bva[2*n_pts:]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[2*n_pts:][bva[2*n_pts:]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices3 = true_indices\n",
    "    \n",
    "    DevIndices = [0] + true_indices1 + [n_pts] +  true_indices2 + [2*n_pts] + true_indices3\n",
    "\n",
    "    IndxArr = [DevIndices]\n",
    "    \n",
    "    gtabs = [gradient_table(bvals = bvals,bvecs = bvecs)]\n",
    "    Delts = [Deltas]\n",
    "    Dats = []\n",
    "    for i,S in enumerate(Subfiles[1:]):\n",
    "        MatDir = RetestDir+S\n",
    "        F = pmt.read_mat(MatDir+'/data_loaded.mat')\n",
    "\n",
    "        bvecs = F['direction']\n",
    "        bvals = F['bval']\n",
    "        Deltas = F['deltas']\n",
    "        data1 = F['data']\n",
    "\n",
    "        bvecs = bvecs[Deltas != 42]\n",
    "        bvals = bvals[Deltas != 42]\n",
    "        data1 = data1[..., Deltas !=42]\n",
    "        Deltas = Deltas[Deltas != 42]*0.001\n",
    "        Delts.append(Deltas)\n",
    "        n_pts = np.sum(Deltas == Deltas[0])\n",
    "        bve = np.copy(bvecs)\n",
    "        bva = np.copy(bvals)\n",
    "    \n",
    "        data1, affine1 = reslice(data1, affine1, (2,2,2), (2.5,2.5,2.5))\n",
    "        bvecs = (bvecs.T/np.linalg.norm(bvecs,axis=1)).T\n",
    "        bvecs[np.isnan(bvecs)] = 0\n",
    "        if(jj == 0):\n",
    "            if(i < 2):\n",
    "                data1 = data1[:,::-1]\n",
    "        elif(jj == 1 or jj == 2 or jj == 3 or jj == 4):\n",
    "            if(i<3):\n",
    "                data1 = data1[:,::-1]\n",
    "        elif(jj==5):\n",
    "            if(i>0 and i < 3):\n",
    "                data1 = data1[:,::-1]\n",
    "        Dats.append(data1)\n",
    "        gtabs.append(gradient_table(bvals = bvals,bvecs = bvecs))\n",
    "        \n",
    "        selected_indices = [0]\n",
    "        bvecs2000 = bve[:n_pts][bva[:n_pts]==2000]\n",
    "        distance_matrix = squareform(pdist(bve))\n",
    "        # Iteratively select the point furthest from the current selection\n",
    "        for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "            remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "\n",
    "            # Calculate the minimum distance to the selected points for each remaining point\n",
    "            min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "\n",
    "            # Select the point with the maximum minimum distance\n",
    "            next_index = remaining_indices[np.argmax(min_distances)]\n",
    "            selected_indices.append(next_index)\n",
    "\n",
    "        selected_indices = selected_indices\n",
    "        bvecs2000_selected = bve[:n_pts][bva[:n_pts]==2000][selected_indices]\n",
    "        true_indices = []\n",
    "        for b in bvecs2000_selected:\n",
    "            true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "\n",
    "        # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "        selected_indices = [0]\n",
    "        bvecs4000 = bve[:n_pts][bva[:n_pts]==4000]\n",
    "        distance_matrix = squareform(pdist(bve))\n",
    "        # Iteratively select the point furthest from the current selection\n",
    "        for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "            remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "\n",
    "            # Calculate the minimum distance to the selected points for each remaining point\n",
    "            min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "\n",
    "            # Select the point with the maximum minimum distance\n",
    "            next_index = remaining_indices[np.argmax(min_distances)]\n",
    "            selected_indices.append(next_index)\n",
    "\n",
    "        selected_indices = selected_indices\n",
    "        bvecs4000_selected = bve[:91][bva[:91]==4000][selected_indices]\n",
    "        for b in bvecs4000_selected:\n",
    "            true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "        true_indices1 = true_indices\n",
    "\n",
    "        # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "        selected_indices = [0]\n",
    "        bvecs2000 = bve[n_pts:2*n_pts][bva[n_pts:2*n_pts]==2000]\n",
    "        distance_matrix = squareform(pdist(bve))\n",
    "        # Iteratively select the point furthest from the current selection\n",
    "        for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "            remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "\n",
    "            # Calculate the minimum distance to the selected points for each remaining point\n",
    "            min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "\n",
    "            # Select the point with the maximum minimum distance\n",
    "            next_index = remaining_indices[np.argmax(min_distances)]\n",
    "            selected_indices.append(next_index)\n",
    "\n",
    "        selected_indices = selected_indices\n",
    "        bvecs2000_selected = bve[n_pts:2*n_pts][bva[n_pts:2*n_pts]==2000][selected_indices]\n",
    "        true_indices = []\n",
    "        for b in bvecs2000_selected:\n",
    "            true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "\n",
    "        # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "        selected_indices = [0]\n",
    "        bvecs4000 = bve[n_pts:2*n_pts][bva[n_pts:2*n_pts]==4000]\n",
    "        distance_matrix = squareform(pdist(bve))\n",
    "        # Iteratively select the point furthest from the current selection\n",
    "        for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "            remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "\n",
    "            # Calculate the minimum distance to the selected points for each remaining point\n",
    "            min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "\n",
    "            # Select the point with the maximum minimum distance\n",
    "            next_index = remaining_indices[np.argmax(min_distances)]\n",
    "            selected_indices.append(next_index)\n",
    "\n",
    "        selected_indices = selected_indices\n",
    "        bvecs4000_selected = bve[n_pts:2*n_pts][bva[n_pts:2*n_pts]==4000][selected_indices]\n",
    "        for b in bvecs4000_selected:\n",
    "            true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "        true_indices2 = true_indices\n",
    "\n",
    "        # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "        selected_indices = [0]\n",
    "        bvecs2000 = bve[2*n_pts:][bva[2*n_pts:]==2000]\n",
    "        distance_matrix = squareform(pdist(bve))\n",
    "        # Iteratively select the point furthest from the current selection\n",
    "        for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "            remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "\n",
    "            # Calculate the minimum distance to the selected points for each remaining point\n",
    "            min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "\n",
    "            # Select the point with the maximum minimum distance\n",
    "            next_index = remaining_indices[np.argmax(min_distances)]\n",
    "            selected_indices.append(next_index)\n",
    "\n",
    "        selected_indices = selected_indices\n",
    "        bvecs2000_selected = bve[2*n_pts:][bva[2*n_pts:]==2000][selected_indices]\n",
    "        true_indices = []\n",
    "        for b in bvecs2000_selected:\n",
    "            true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "\n",
    "        # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "        selected_indices = [0]\n",
    "        bvecs4000 = bve[2*n_pts:][bva[2*n_pts:]==4000]\n",
    "        distance_matrix = squareform(pdist(bve))\n",
    "        # Iteratively select the point furthest from the current selection\n",
    "        for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "            remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "\n",
    "            # Calculate the minimum distance to the selected points for each remaining point\n",
    "            min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "\n",
    "            # Select the point with the maximum minimum distance\n",
    "            next_index = remaining_indices[np.argmax(min_distances)]\n",
    "            selected_indices.append(next_index)\n",
    "\n",
    "        selected_indices = selected_indices\n",
    "        bvecs4000_selected = bve[2*n_pts:][bva[2*n_pts:]==4000][selected_indices]\n",
    "        for b in bvecs4000_selected:\n",
    "            true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "        true_indices3 = true_indices\n",
    "\n",
    "        DevIndices = [0] + true_indices1 + [n_pts] +  true_indices2 + [2*n_pts] + true_indices3\n",
    "\n",
    "        IndxArr.append(DevIndices)\n",
    "        \n",
    "    NewDats = [data]\n",
    "    for d,gt in zip(Dats,gtabs[1:]):\n",
    "        affine_map = rigid_register(data[...,gtabs[0].bvals==0].mean(axis=-1),d[...,gt.bvals==0].mean(axis=-1),affine1,affine1)\n",
    "        data2_warp = np.array([affine_map.transform(d[:,:,:,i], interpolation=\"linear\") for i in range(len(gt.bvals))])\n",
    "        data2_warp = np.rollaxis(data2_warp, 0, data2_warp.ndim)\n",
    "        NewDats.append(data2_warp)\n",
    "    \n",
    "    NewDats_masked = [ND*maskCut[...,None] for ND in NewDats]\n",
    "    \n",
    "    Full_SBI_Arr = []\n",
    "    for ND,gt,Del in tqdm(zip(NewDats_masked,gtabs,Delts),position=0,leave=True):\n",
    "        mask = np.sum(ND[:, :, AM, :], axis=-1) != 0\n",
    "        # Get the indices where mask is True\n",
    "        indices = np.argwhere(mask)\n",
    "        floor = np.clip(ND.min(axis=-1),-np.inf,0)\n",
    "        dat = ND + abs(floor)[:,:,:,None] + 1e-5\n",
    "        # Define the function for optimization\n",
    "        def optimize_chunk(pixels):\n",
    "            results = []\n",
    "            for i, j in pixels:\n",
    "                samples = Network.sample((1000,), x=AxcaliberFeatures(gt.bvecs,gt.bvals,Del,dat[i, j,AM, :]),show_progress_bars=False)        \n",
    "                results.append((i, j, samples.mean(axis=0)))\n",
    "            return results\n",
    "\n",
    "        chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "        results = Parallel(n_jobs=6)(\n",
    "            delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices,position=0,leave=True)\n",
    "        )\n",
    "\n",
    "        ArrShape = maskCut[...,AM].shape\n",
    "\n",
    "        NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "        # Assign the optimization results to NoiseEst\n",
    "        for chunk in results:\n",
    "            for i, j, x in chunk:\n",
    "                NoiseEst[i, j] = x\n",
    "                NoiseEst[i, j,-2] = np.clip(NoiseEst[i, j,-2],0,100)\n",
    "                NoiseEst[i, j,-3] = np.clip(NoiseEst[i, j,-3],0,1)\n",
    "        NoiseEst2 = np.copy(NoiseEst)\n",
    "\n",
    "        for i in range(13):\n",
    "            NoiseEst2[~maskCut[...,AM],i] = math.nan\n",
    "\n",
    "        NoiseEst2[(1-NoiseEst2[...,-3])<0.3,-2] = math.nan\n",
    "        Full_SBI_Arr.append(NoiseEst2)\n",
    "    Full_SBI_Arr2.append(Full_SBI_Arr)\n",
    "    \n",
    "    Full_SBI_Arr = []\n",
    "    for ND,gt,Del,idxs in tqdm(zip(NewDats_masked,gtabs,Delts,IndxArr),position=0,leave=True):\n",
    "        mask = np.sum(ND[:, :, AM, :], axis=-1) != 0\n",
    "        # Get the indices where mask is True\n",
    "        indices = np.argwhere(mask)\n",
    "        floor = np.clip(ND.min(axis=-1),-np.inf,0)\n",
    "        dat = ND + abs(floor)[:,:,:,None] + 1e-5\n",
    "        # Define the function for optimization\n",
    "        def optimize_chunk(pixels):\n",
    "            results = []\n",
    "            for i, j in pixels:\n",
    "                samples = Network.sample((1000,), x=AxcaliberFeatures(gt.bvecs[idxs],gt.bvals[idxs],Del[idxs],dat[i, j,AM, idxs]),show_progress_bars=False)        \n",
    "                results.append((i, j, samples.mean(axis=0)))\n",
    "            return results\n",
    "\n",
    "        chunked_indices = [indices[i:i+ChunkSize] for i in range(0, len(indices), ChunkSize)]\n",
    "        results = Parallel(n_jobs=6)(\n",
    "            delayed(optimize_chunk)(chunk) for chunk in tqdm(chunked_indices,position=0,leave=True)\n",
    "        )\n",
    "\n",
    "        ArrShape = maskCut[...,AM].shape\n",
    "\n",
    "        NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "        # Assign the optimization results to NoiseEst\n",
    "        for chunk in results:\n",
    "            for i, j, x in chunk:\n",
    "                NoiseEst[i, j] = x\n",
    "                NoiseEst[i, j,-2] = np.clip(NoiseEst[i, j,-2],0,100)\n",
    "                NoiseEst[i, j,-3] = np.clip(NoiseEst[i, j,-3],0,1)\n",
    "        NoiseEst2 = np.copy(NoiseEst)\n",
    "\n",
    "        for i in range(13):\n",
    "            NoiseEst2[~maskCut[...,AM],i] = math.nan\n",
    "\n",
    "        NoiseEst2[(1-NoiseEst2[...,-3])<0.3,-2] = math.nan\n",
    "        Full_SBI_Arr.append(NoiseEst2)\n",
    "    Full_SBI_min_Arr2.append(Full_SBI_Arr)\n",
    "    \n",
    "        # Get the indices where mask is True\n",
    "    Full_NLLS_Arr = []\n",
    "    for ND,gt,Del in tqdm(zip(NewDats_masked,gtabs,Delts),position=0,leave=True):\n",
    "        mask = np.sum(ND[:, :, AM, :], axis=-1) != 0\n",
    "        # Get the indices where mask is True\n",
    "        indices = np.argwhere(mask)\n",
    "        floor = np.clip(ND.min(axis=-1),-np.inf,0)\n",
    "        dat = ND + abs(floor)[:,:,:,None] + 1e-5\n",
    "        \n",
    "        bve_split_kk = [gt.bvecs[:(n_pts)],gt.bvecs[(n_pts):2*(n_pts)],gt.bvecs[2*(n_pts):]]\n",
    "        bva_split_kk = [gt.bvals[:(n_pts)],gt.bvals[(n_pts):2*(n_pts)],gt.bvals[2*(n_pts):]]\n",
    "        # Define the function for optimization\n",
    "        def optimize_pixel_LS(i, j):\n",
    "            result = sp.optimize.least_squares(residuals, guess, args=[dat[i,j,AM, :],bve_split_kk,bva_split_kk,Del,True],\n",
    "                                      bounds=bounds,verbose=0,jac='3-point')\n",
    "            return i, j, result.x\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize NoiseEst with the appropriate shape\n",
    "        ArrShape = mask.shape\n",
    "\n",
    "        # Use joblib to parallelize the optimization tasks\n",
    "        results = Parallel(n_jobs=6)(\n",
    "            delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices,position=0,leave=True)\n",
    "        )\n",
    "\n",
    "\n",
    "        NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "        # Assign the optimization results to NoiseEst\n",
    "        for i, j, x in results:\n",
    "            NoiseEst_LS[i, j] = x\n",
    "\n",
    "        Full_NLLS_Arr.append(NoiseEst_LS)\n",
    "    Full_NLLS_Arr2.append(Full_NLLS_Arr)\n",
    "    \n",
    "            # Get the indices where mask is True\n",
    "    Full_NLLS_Arr = []\n",
    "    for ND,gt,Del,idxs in tqdm(zip(NewDats_masked,gtabs,Delts,IndxArr),position=0,leave=True):\n",
    "        mask = np.sum(ND[:, :, AM, :], axis=-1) != 0\n",
    "        # Get the indices where mask is True\n",
    "        indices = np.argwhere(mask)\n",
    "        floor = np.clip(ND.min(axis=-1),-np.inf,0)\n",
    "        dat = ND + abs(floor)[:,:,:,None] + 1e-5\n",
    "        \n",
    "        bve_split_kk = [gt.bvecs[idxs[:7]],gt.bvecs[idxs[7:14]],gt.bvecs[idxs[14:]]]\n",
    "        bva_split_kk = [gt.bvals[idxs[:7]],gt.bvals[idxs[7:14]],gt.bvals[idxs[14:]]]\n",
    "        bve_split_kk \n",
    "        # Define the function for optimization\n",
    "        def optimize_pixel_LS(i, j):\n",
    "            result = sp.optimize.least_squares(residuals, guess, args=[dat[i,j,AM, idxs],bve_split_kk,bva_split_kk,Delta,True],\n",
    "                                      bounds=bounds,verbose=0,jac='3-point')\n",
    "            return i, j, result.x\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize NoiseEst with the appropriate shape\n",
    "        ArrShape = mask.shape\n",
    "\n",
    "        # Use joblib to parallelize the optimization tasks\n",
    "        results = Parallel(n_jobs=6)(\n",
    "            delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices,position=0,leave=True)\n",
    "        )\n",
    "\n",
    "\n",
    "        NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "        # Assign the optimization results to NoiseEst\n",
    "        for i, j, x in results:\n",
    "            NoiseEst_LS[i, j] = x\n",
    "\n",
    "        Full_NLLS.append(NoiseEst_LS)\n",
    "    MinNLLS.append(Full_NLLS_Arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e14482",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:00:13.784851Z",
     "start_time": "2025-12-27T12:00:13.727044Z"
    }
   },
   "outputs": [],
   "source": [
    "FullNLLS_list = []\n",
    "MinNLLS_list = []\n",
    "FullSBI_list = []\n",
    "MinSBI_list = []\n",
    "\n",
    "\n",
    "for kk in range(len(FullNLLS)):\n",
    "    for i in range(len(FullNLLS[kk])):\n",
    "        FullNLLS_list.append(FullNLLS[kk][i])\n",
    "        MinNLLS_list.append(MinNLLS[kk][i])\n",
    "        FullSBI_list.append(FullSBI[kk][i])\n",
    "        MinSBI_list.append(MinSBI[kk][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149a951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:03:43.536700Z",
     "start_time": "2025-12-27T12:03:27.772351Z"
    }
   },
   "outputs": [],
   "source": [
    "jj = -4\n",
    "SBI_comp_Frac_RT = []\n",
    "for i in range(29):\n",
    "    NS1 = np.copy(FullSBI_list[i][...,jj])\n",
    "    NS2 = np.copy(MinSBI_list[i][...,jj])\n",
    "    Ma = ~np.isnan(NS1)\n",
    "    NS1[~Ma] = 0\n",
    "    NS2[~Ma] = 0\n",
    "    SBI_comp_Frac_RT.append(masked_local_ssim(NS1, NS2, Ma, win_size=7))\n",
    "\n",
    "LS_comp_Frac_RT = []\n",
    "for i in range(29):\n",
    "    NS1 = np.copy(FullNLLS_list[i][...,jj])\n",
    "    NS2 = np.copy(MinNLLS_list[i][...,jj])\n",
    "    Ma = ~np.isnan(NS1)\n",
    "    NS1[~Ma] = 0\n",
    "    NS2[~Ma] = 0\n",
    "    LS_comp_Frac_RT.append(masked_local_ssim(NS1, NS2, Ma, win_size=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9bd60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:17:57.981888Z",
     "start_time": "2025-12-27T13:17:57.875613Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "y_data = np.array(SBI_comp_Frac+SBI_comp_Frac_RT)\n",
    "g_pos = np.array([1.3])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=False,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(SBI_comp_Frac)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "ax.scatter(x_data,y_data,marker='o',color=colors2,s=100,alpha=0.8)\n",
    "\n",
    "y_data = np.array(SBI_comp_Frac_RT)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "ax.scatter(x_data,y_data,marker='s',color='darkcyan',s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(LS_comp_Frac+LS_comp_Frac_RT)\n",
    "g_pos = np.array([1.9])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=False)\n",
    "\n",
    "y_data = np.array(LS_comp_Frac)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "ax.scatter(x_data,y_data,marker='o',color=colors2,s=100,alpha=0.8)\n",
    "\n",
    "y_data = np.array(LS_comp_Frac_RT)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "ax.scatter(x_data,y_data,marker='s',color='chocolate',s=100,alpha=0.5)\n",
    "\n",
    "plt.xticks([1.3,1.9],['SBI','NNLS'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_SSIM_Frac.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4bbf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:17:55.303746Z",
     "start_time": "2025-12-27T13:17:55.191737Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(1,1,figsize=(3.2,4.8))\n",
    "\n",
    "y_data = np.array(PrecFull_SBI_Frac)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_Frac)\n",
    "g_pos = np.array([1.1])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(PrecFull_NLLS_Frac)\n",
    "g_pos = np.array([1.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_Frac)\n",
    "g_pos = np.array([2.15])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([0.65,1.1,1.8,2.15],['Full','Red.','Full','Red.'],fontsize=32,rotation=90)\n",
    "\n",
    "x = np.arange(1.7,2.3,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_Frac)[~np.isnan(PrecFull_NLLS_Frac)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_Frac)[~np.isnan(PrecFull_NLLS_Frac)], 77)\n",
    "plt.fill_between(x,y1,y2,color='sandybrown',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.25,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_Frac)[~np.isnan(PrecFull_SBI_Frac)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_Frac)[~np.isnan(PrecFull_SBI_Frac)], 77)\n",
    "plt.fill_between(x,y1,y2,color='mediumturquoise',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "#ax1.set_xlim(0.3,2.8)\n",
    "ax1.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "#ax1.set_yticks([0,0.1,0.2])\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_Prec_Frac.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf4947",
   "metadata": {},
   "source": [
    "## j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775a913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:10:55.652368Z",
     "start_time": "2025-12-27T12:09:17.639092Z"
    }
   },
   "outputs": [],
   "source": [
    "KK = [48]*8\n",
    "FA_Full_SBI = []\n",
    "MD_Full_SBI = []\n",
    "for jj in range(8):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(Par_frac)(i, j,Full_SBI_Extra[jj][...,4:10]) for i, j in tqdm(indices,position=0,leave=True)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, k in results:\n",
    "        temp1[i, j] = k[0]\n",
    "        temp2[i, j] = k[1]\n",
    "\n",
    "    FA_Full_SBI.append(temp1)\n",
    "    MD_Full_SBI.append(temp2)\n",
    "KK = [48]*8\n",
    "FA_Min_SBI = []\n",
    "MD_Min_SBI = []\n",
    "for jj in range(8):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "\n",
    "    Arr = Min_SBI_Extra[jj][...,4:10]\n",
    "         \n",
    "    results = Parallel(n_jobs=8,)(\n",
    "        delayed(Par_frac)(i, j,Min_SBI_Extra[jj][...,4:10]) for i, j in tqdm(indices,position=0,leave=True)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, k in results:\n",
    "        temp1[i, j] = k[0]\n",
    "        temp2[i, j] = k[1]\n",
    "\n",
    "    FA_Min_SBI.append(temp1)\n",
    "    MD_Min_SBI.append(temp2)\n",
    "KK = [48]*8\n",
    "FA_Full_LS = []\n",
    "MD_Full_LS = []\n",
    "for jj in range(8):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    results = Parallel(n_jobs=8,)(\n",
    "        delayed(Par_frac)(i, j,Full_LS_extra[jj][...,4:10]) for i, j in tqdm(indices,position=0,leave=True)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, k in results:\n",
    "        temp1[i, j] = k[0]\n",
    "        temp2[i, j] = k[1]\n",
    "\n",
    "    FA_Full_LS.append(temp1)\n",
    "    MD_Full_LS.append(temp2)\n",
    "\n",
    "KK = [48]*8\n",
    "FA_Min_LS = []\n",
    "MD_Min_LS = []\n",
    "for jj in range(8):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(Par_frac)(i, j,Min_LS_extra[jj][...,4:10]) for i, j in tqdm(indices,position=0,leave=True)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, k in results:\n",
    "        temp1[i, j] = k[0]\n",
    "        temp2[i, j] = k[1]\n",
    "\n",
    "    FA_Min_LS.append(temp1)\n",
    "    MD_Min_LS.append(temp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d7620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:16:24.700834Z",
     "start_time": "2025-12-27T12:16:15.366456Z"
    }
   },
   "outputs": [],
   "source": [
    "MD_Full_SBI_RT = []\n",
    "for kk in tqdm(range(29),position=0,leave=True):\n",
    "    temp = np.zeros_like(FullSBI_list[kk][:,:,0])*math.nan\n",
    "    mask = ~np.isnan(FullSBI_list[kk][:,:,0])\n",
    "    indices = np.argwhere(mask)\n",
    "    for i1,i2 in indices:\n",
    "        temp[i1,i2] = MD_FA(vals_to_mat(FullSBI_list[kk][i1,i2,4:10]))[0]\n",
    "    MD_Full_SBI_RT.append(temp)\n",
    "\n",
    "MD_Full_NLLS_RT = []\n",
    "for kk in tqdm(range(29),position=0,leave=True):\n",
    "    temp = np.zeros_like(FullNLLS_list[kk][:,:,0])*math.nan\n",
    "    mask = ~np.isnan(FullNLLS_list[kk][:,:,0])\n",
    "    indices = np.argwhere(mask)\n",
    "    for i1,i2 in indices:\n",
    "        temp[i1,i2] = MD_FA(vals_to_mat(FullNLLS_list[kk][i1,i2,4:10]))[0]\n",
    "    MD_Full_NLLS_RT.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a961fd4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:16:12.880434Z",
     "start_time": "2025-12-27T12:16:03.569537Z"
    }
   },
   "outputs": [],
   "source": [
    "MD_Min_SBI_RT = []\n",
    "for kk in tqdm(range(29),position=0,leave=True):\n",
    "    temp = np.zeros_like(MinSBI_list[kk][:,:,0])*math.nan\n",
    "    mask = ~np.isnan(MinSBI_list[kk][:,:,0])\n",
    "    indices = np.argwhere(mask)\n",
    "    for i1,i2 in indices:\n",
    "        temp[i1,i2] = MD_FA(vals_to_mat(MinSBI_list[kk][i1,i2,4:10]))[0]\n",
    "    MD_Min_SBI_RT.append(temp)\n",
    "\n",
    "MD_Min_NLLS_RT = []\n",
    "for kk in tqdm(range(29),position=0,leave=True):\n",
    "    temp = np.zeros_like(MinNLLS_list[kk][:,:,0])*math.nan\n",
    "    mask = ~np.isnan(MinNLLS_list[kk][:,:,0])\n",
    "    indices = np.argwhere(mask)\n",
    "    for i1,i2 in indices:\n",
    "        temp[i1,i2] = MD_FA(vals_to_mat(MinNLLS_list[kk][i1,i2,4:10]))[0]\n",
    "    MD_Min_NLLS_RT.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610a88d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:11:31.325231Z",
     "start_time": "2025-12-27T12:11:31.241857Z"
    }
   },
   "outputs": [],
   "source": [
    "SBI_comp_MD = []\n",
    "KK = [48]*8\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(MD_Min_SBI[i])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(MD_Full_SBI[i])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=7)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_comp_MD.append(masked_ssim.mean())\n",
    "\n",
    "LS_comp_MD = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(MD_Min_LS[i])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(MD_Full_LS[i])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=7)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    LS_comp_MD.append(masked_ssim.mean())\n",
    "\n",
    "SBI_LS_comp_MD = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(MD_Full_SBI[i])\n",
    "    NS2 = np.copy(MD_Full_LS[i])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=7)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_LS_comp_MD.append(masked_ssim.mean())\n",
    "Prec7_SBI_MD = []\n",
    "PrecFull_SBI_MD = []\n",
    "\n",
    "Prec7_NLLS_MD = []\n",
    "PrecFull_NLLS_MD = []\n",
    "for i in range(8):\n",
    "    Prec7_SBI_MD.append(np.std(MD_Min_SBI[i][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_SBI_MD.append(np.std(MD_Full_SBI[i][WMs[i].astype(bool)[:,:,48]]))\n",
    "\n",
    "    Prec7_NLLS_MD.append(np.std(MD_Min_LS[i][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_NLLS_MD.append(np.std(MD_Full_LS[i][WMs[i].astype(bool)[:,:,48]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b7dbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:17:10.223370Z",
     "start_time": "2025-12-27T12:16:54.401781Z"
    }
   },
   "outputs": [],
   "source": [
    "jj = -4\n",
    "SBI_comp_MD_RT = []\n",
    "for i in range(29):\n",
    "    NS1 = np.copy(MD_Full_SBI_RT[i])\n",
    "    NS2 = np.copy(MD_Min_SBI_RT[i])\n",
    "    Ma = ~np.isnan(NS1)\n",
    "    NS1[~Ma] = 0\n",
    "    NS2[~Ma] = 0\n",
    "    SBI_comp_MD_RT.append(masked_local_ssim(NS1, NS2, Ma, win_size=7))\n",
    "\n",
    "LS_comp_MD_RT = []\n",
    "for i in range(29):\n",
    "    NS1 = np.copy(MD_Full_NLLS_RT[i])\n",
    "    NS2 = np.copy(MD_Min_NLLS_RT[i])\n",
    "    Ma = ~np.isnan(NS1)\n",
    "    NS1[~Ma] = 0\n",
    "    NS2[~Ma] = 0\n",
    "    LS_comp_MD_RT.append(masked_local_ssim(NS1, NS2, Ma, win_size=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4c6f86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:17:50.227841Z",
     "start_time": "2025-12-27T13:17:50.116680Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SBI_comp_MD+SBI_comp_MD_RT)\n",
    "g_pos = np.array([1.3])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=False,scatter_alpha=0.5)\n",
    "y_data = np.array(SBI_comp_MD)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "ax.scatter(x_data,y_data,marker='o',color=colors2,s=100,alpha=0.8)\n",
    "\n",
    "y_data = np.array(SBI_comp_MD_RT)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "ax.scatter(x_data,y_data,marker='s',color='darkcyan',s=100,alpha=0.5)\n",
    "\n",
    "\n",
    "y_data = np.array(LS_comp_MD+LS_comp_MD_RT)\n",
    "g_pos = np.array([1.9])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=False)\n",
    "\n",
    "y_data = np.array(LS_comp_MD)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "ax.scatter(x_data,y_data,marker='o',color=colors2,s=100,alpha=0.8)\n",
    "\n",
    "y_data = np.array(LS_comp_MD_RT)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "ax.scatter(x_data,y_data,marker='s',color='chocolate',s=100,alpha=0.5)\n",
    "\n",
    "plt.xticks([1.3,1.9],['SBI','NNLS'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_SSIM_MD.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a350f2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:17:41.177761Z",
     "start_time": "2025-12-27T13:17:41.057514Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(1,1,figsize=(3.2,4.8))\n",
    "\n",
    "y_data = np.array(PrecFull_SBI_MD)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_MD)\n",
    "g_pos = np.array([1.1])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(PrecFull_NLLS_MD)\n",
    "g_pos = np.array([1.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_MD)\n",
    "g_pos = np.array([2.15])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([0.65,1.1,1.8,2.15],['Full','Red.','Full','Red.'],fontsize=32,rotation=90)\n",
    "\n",
    "x = np.arange(1.7,2.3,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_MD)[~np.isnan(PrecFull_NLLS_MD)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_MD)[~np.isnan(PrecFull_NLLS_MD)], 77)\n",
    "plt.fill_between(x,y1,y2,color='sandybrown',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.25,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_MD)[~np.isnan(PrecFull_SBI_MD)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_MD)[~np.isnan(PrecFull_SBI_MD)], 77)\n",
    "plt.fill_between(x,y1,y2,color='mediumturquoise',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "#ax1.set_xlim(0.3,2.8)\n",
    "ax1.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax1.set_yticks([0.0004,0.0008,0.0012])\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_Prec_MD.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d2ff3",
   "metadata": {},
   "source": [
    "## k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4dd29f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:20:11.231933Z",
     "start_time": "2025-12-27T12:20:11.160721Z"
    }
   },
   "outputs": [],
   "source": [
    "jj = 2\n",
    "SBI_comp_Dp = []\n",
    "KK = [48]*8\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Min_SBI_Extra[i][...,jj])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(Full_SBI_Extra[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=7)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_comp_Dp.append(masked_ssim.mean())\n",
    "\n",
    "LS_comp_Dp = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Min_LS_extra[i][...,jj])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(Full_LS_extra[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=7)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    LS_comp_Dp.append(masked_ssim.mean())\n",
    "\n",
    "SBI_LS_comp_Dp = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Full_SBI_Extra[i][...,jj])\n",
    "    NS2 = np.copy(Full_LS_extra[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=7)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_LS_comp_Dp.append(masked_ssim.mean())\n",
    "Prec7_SBI_Dp = []\n",
    "PrecFull_SBI_Dp = []\n",
    "\n",
    "Prec7_NLLS_Dp = []\n",
    "PrecFull_NLLS_Dp = []\n",
    "for i in range(8):\n",
    "    Prec7_SBI_Dp.append(np.std(Min_SBI_Extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_SBI_Dp.append(np.std(Full_SBI_Extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "\n",
    "    Prec7_NLLS_Dp.append(np.std(Min_LS_extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_NLLS_Dp.append(np.std(Full_LS_extra[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68821085",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:21:24.214608Z",
     "start_time": "2025-12-27T12:21:08.137949Z"
    }
   },
   "outputs": [],
   "source": [
    "jj = 2\n",
    "SBI_comp_Dp_RT = []\n",
    "for i in range(29):\n",
    "    NS1 = np.copy(FullSBI_list[i][...,jj])\n",
    "    NS2 = np.copy(MinSBI_list[i][...,jj])\n",
    "    Ma = ~np.isnan(NS1)\n",
    "    NS1[~Ma] = 0\n",
    "    NS2[~Ma] = 0\n",
    "    SBI_comp_Dp_RT.append(masked_local_ssim(NS1, NS2, Ma, win_size=7))\n",
    "\n",
    "LS_comp_Dp_RT = []\n",
    "for i in range(29):\n",
    "    NS1 = np.copy(FullNLLS_list[i][...,jj])\n",
    "    NS2 = np.copy(MinNLLS_list[i][...,jj])\n",
    "    Ma = ~np.isnan(NS1)\n",
    "    NS1[~Ma] = 0\n",
    "    NS2[~Ma] = 0\n",
    "    LS_comp_Dp_RT.append(masked_local_ssim(NS1, NS2, Ma, win_size=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e498d6a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:17:38.453834Z",
     "start_time": "2025-12-27T13:17:38.341892Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SBI_comp_Dp+SBI_comp_Dp_RT)\n",
    "g_pos = np.array([1.3])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=False,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(SBI_comp_Dp)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "ax.scatter(x_data,y_data,marker='o',color=colors2,s=100,alpha=0.8)\n",
    "\n",
    "y_data = np.array(SBI_comp_Dp_RT)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "ax.scatter(x_data,y_data,marker='s',color='darkcyan',s=100,alpha=0.5)\n",
    "\n",
    "y_data = np.array(LS_comp_Dp+LS_comp_Dp_RT)\n",
    "g_pos = np.array([1.9])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(LS_comp_Dp)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "ax.scatter(x_data,y_data,marker='o',color=colors2,s=100,alpha=0.8)\n",
    "\n",
    "y_data = np.array(LS_comp_Dp_RT)\n",
    "x_data = g_pos*np.ones_like(y_data)\n",
    "x_data += stats.t(df=6, scale=0.02).rvs(len(x_data))\n",
    "ax.scatter(x_data,y_data,marker='s',color='chocolate',s=100,alpha=0.5)\n",
    "\n",
    "plt.xticks([1.3,1.9],['SBI','NNLS'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_SSIM_Dpar.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad739f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:17:37.092143Z",
     "start_time": "2025-12-27T13:17:36.828916Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(1,1,figsize=(3.2,4.8))\n",
    "\n",
    "y_data = np.array(PrecFull_SBI_Dp)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI_Dp)\n",
    "g_pos = np.array([1.1])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(PrecFull_NLLS_Dp)\n",
    "g_pos = np.array([1.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "\n",
    "y_data = np.array(Prec7_NLLS_Dp)\n",
    "g_pos = np.array([2.15])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([0.65,1.1,1.8,2.15],['Full','Red.','Full','Red.'],fontsize=32,rotation=90)\n",
    "\n",
    "x = np.arange(1.7,2.3,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_Dp)[~np.isnan(PrecFull_NLLS_Dp)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS_Dp)[~np.isnan(PrecFull_NLLS_Dp)], 77)\n",
    "plt.fill_between(x,y1,y2,color='sandybrown',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.25,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_Dp)[~np.isnan(PrecFull_SBI_Dp)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI_Dp)[~np.isnan(PrecFull_SBI_Dp)], 77)\n",
    "plt.fill_between(x,y1,y2,color='mediumturquoise',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "#ax1.set_xlim(0.3,2.8)\n",
    "ax1.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_Prec_Dpar.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "460.938px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
