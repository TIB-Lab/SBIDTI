{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2453dfd-dc43-4326-acdf-dc648fe404cf",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac9ca0-d8ca-4228-b490-6f425ffbef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from scipy.ndimage import binary_dilation\n",
    "# Define font properties\n",
    "import matplotlib.ticker as ticker\n",
    "font = {\n",
    "    'family': 'sans-serif',  # Use sans-serif family\n",
    "    'sans-serif': ['Helvetica'],  # Specify Helvetica as the sans-serif font\n",
    "    'size': 14  # Set the default font size\n",
    "}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# Set tick label sizes\n",
    "plt.rc('ytick', labelsize=24)\n",
    "plt.rc('xtick', labelsize=24)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n",
    "# Customize axes spines and legend appearance\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from dwMRI_BasicFuncs import *\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f6c7e-27ff-46bb-897c-51878700028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymatreader as pmt\n",
    "\n",
    "from dipy.io.image import load_nifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a89b3-cf37-4a40-9277-6b1c7c17006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSDir = './MS_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea3315-31d9-496a-b4f5-c2f41136cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dir = MSDir+'/Ctrl055_R01_28/'\n",
    "dat = pmt.read_mat(Dir+'data_loaded.mat')\n",
    "bvecs = dat['direction']\n",
    "bvals = dat['bval']\n",
    "FixedParams = {\n",
    "    'bvals':bvals,\n",
    "    'bvecs':bvecs,\n",
    "    'Delta':[0.017,0.035,0.061],\n",
    "    'delta':0.007,\n",
    "}\n",
    "Delta = FixedParams['Delta']\n",
    "delta = FixedParams['delta']\n",
    "n_pts = 90\n",
    "\n",
    "Delta = [0.017,0.035,0.061] # We know this \n",
    "delta = 0.007 # We know this \n",
    "\n",
    "\n",
    "data = dat['data']\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(0, 10), median_radius=5,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "\n",
    "plt.imshow(maskdata[:,:,41,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb94b74-8c42-4880-a5d2-dcf76426c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_path = './Networks/'\n",
    "image_path   = '../Figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7efaf-05ea-4a96-a225-47f2130a4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_5/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bfdbb5-c2d9-4170-8d26-e1ff997db1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{network_path}/Full_Dat_50_200k.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa5172-870e-4fd3-9020-8bcdaca9729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/Full_Dat_50_100k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/Full_Dat_50_100k_poisson.pickle\", \"rb\") as handle:\n",
    "        posterior = pickle.load(handle)\n",
    "else:\n",
    "\n",
    "    \n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        s0 = S0Rand[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "    \n",
    "        TrainSig1 = CombSignal_poisson(bvecs[:(n_pts+1)],bvals[:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(bvecs[(n_pts+1):2*(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(bvecs[2*(n_pts+1):],bvals[2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(AddNoise(TrainSig[-1],s0,Noise))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "\n",
    "\n",
    "\n",
    "    Obs = torch.tensor(NoisyTrainSig).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    with open(f\"{network_path}/Full_Dat_50_100k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02345b9b-5556-4bdc-9f85-f27a805e6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, 54, :, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel(i, j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posterior.sample((1000,), x=maskdata[i, 54, j, :],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    NoiseEst[i, j,-2] = np.clip(NoiseEst[i, j,-2],0,100)\n",
    "    NoiseEst[i, j,-3] = np.clip(NoiseEst[i, j,-3],0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e713f175-ab15-4646-aae4-23b6d8f37e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 = np.copy(NoiseEst)\n",
    "\n",
    "mask1 = np.ones_like(S_mask[:,54,:])\n",
    "mask1[S_mask[:,54,:]==0] = 0\n",
    "structure = np.ones((3, 3), dtype=bool)\n",
    "\n",
    "# Apply dilation. Increase 'iterations' to make the mask even fatter.\n",
    "fat_mask = binary_dilation(mask1, structure=structure, iterations=1)\n",
    "\n",
    "comb_mask = fat_mask * ((1-NoiseEst2[...,-3])>0.1)\n",
    "\n",
    "mask_CC = (1-NoiseEst2[...,-3])<0.3\n",
    "for i in range(13):\n",
    "    NoiseEst2[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2[~comb_mask,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb4c911-dde6-4bbf-b2a1-1efc88f7af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 = np.copy(NoiseEst)\n",
    "mask_CC = (1-NoiseEst2[...,-3])<0.3\n",
    "for i in range(13):\n",
    "    NoiseEst2[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2[~comb_mask,-2] = math.nan\n",
    "plt.subplots(figsize=(12,12))\n",
    "plt.imshow(np.flipud(NoiseEst2[...,-1].T),cmap='gray')\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=0.0035, vmax=0.007)\n",
    "im = plt.imshow(np.flipud(NoiseEst2[...,-2].T),cmap='hot',norm=norm)\n",
    "#cbar = plt.colorbar(im,fraction=0.035, pad=0.01,format=ticker.FormatStrFormatter('%.e'))\n",
    "#cbar.ax.tick_params(labelsize=14)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.savefig(FigLoc+'/FullSize_CC.pdf',bbox_inches='tight',format='pdf',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc2b66d-f64b-4f97-b539-b7e7f2e798a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals_S0(params,TrueSig,bvecs,bvals,Delta):\n",
    "    Signal = Simulator_new(params,bvecs,bvals,Delta,S0=params[-1])\n",
    "    return TrueSig - Signal\n",
    "def Simulator_new(params,bvecs,bvals,Delta,S0=1):\n",
    "    new_params = [np.array([params[:2]]),params[2],params[3],params[4:10],[params[10],1-params[10]],params[11],S0]\n",
    "    Sig = []\n",
    "    for bve,bva,d in zip(bvecs,bvals,Delta):\n",
    "        Sig.append(CombSignal_poisson(bve,bva,d,delta,new_params))\n",
    "    return np.hstack(Sig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f036005-511c-4f97-9358-381583b524f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombSignal_poisson(bvecs, bvals, Delta, delta, params):\n",
    "    \"\"\"\n",
    "    Compute the combined diffusion signal in a fast, vectorized way.\n",
    "    \n",
    "    Parameters:\n",
    "      bvecs  : (M,3) array of b-vectors.\n",
    "      bvals  : (M,) array of b-values.\n",
    "      Delta, delta : acquisition parameters (scalars)\n",
    "      params : list/tuple of parameters:\n",
    "          params[0] : fiber directions as an (N,2) array of spherical angles (theta, phi)\n",
    "          params[1] : Dpar (scalar)\n",
    "          params[2] : Dperp (scalar)\n",
    "          params[3] : D (for hindered compartment; passed to vals_to_mat)\n",
    "          params[4] : fiber fractions as an (N+1,) array \n",
    "                      (first element for hindered compartment, then one per fiber)\n",
    "          params[5] : mean (scalar, for gamma distribution)\n",
    "          params[6] : sig2 (scalar, for gamma distribution)\n",
    "          params[7] : S0 (scalar)\n",
    "    \n",
    "    Returns:\n",
    "      Signal : (M,) array of simulated signal values.\n",
    "    \"\"\"\n",
    "    # Unpack parameters\n",
    "    V_angles, Dpar, Dperp, D, fracs, mean, S0 = params\n",
    "\n",
    "    # --- 1. Compute fiber unit vectors from spherical angles ---\n",
    "    # Assume V_angles is an (N,2) array: each row is (theta, phi).\n",
    "    theta_fibers = V_angles[:, 0]\n",
    "    phi_fibers   = V_angles[:, 1]\n",
    "    V_unit = np.column_stack((np.sin(theta_fibers) * np.cos(phi_fibers),\n",
    "                              np.sin(theta_fibers) * np.sin(phi_fibers),\n",
    "                              np.cos(theta_fibers)))  # shape: (N, 3)\n",
    "\n",
    "    # --- 2. Compute angles between each fiber and each b-vector ---\n",
    "    # Make sure bvecs is an array.\n",
    "    bvecs = np.asarray(bvecs)  # shape: (M,3)\n",
    "    M = bvecs.shape[0]\n",
    "    N = V_unit.shape[0]\n",
    "\n",
    "    # Precompute norms of bvecs (we assume fibers are unit length so no extra norm is needed)\n",
    "    bvec_norms = np.linalg.norm(bvecs, axis=1)\n",
    "    # Avoid division by zero:\n",
    "    safe_bvec_norms = np.where(bvec_norms == 0, 1, bvec_norms)\n",
    "\n",
    "    # Compute the dot products for each fiber with all bvecs:\n",
    "    # This gives a (N, M) array where the (i,j) element = v_i dot bvec_j.\n",
    "    dots = V_unit @ bvecs.T  # shape: (N, M)\n",
    "\n",
    "    # Divide each column j by the norm of bvec j (broadcasting over fibers)\n",
    "    cos_angles = dots / safe_bvec_norms  # shape: (N, M)\n",
    "    cos_angles = np.clip(cos_angles, -1, 1)\n",
    "    # Get the angles in [0,pi]\n",
    "    Angs = np.arccos(cos_angles)\n",
    "    # For bvecs that are zero (norm==0), force the angle to zero.\n",
    "    if np.any(bvec_norms == 0):\n",
    "        Angs[:, bvec_norms == 0] = 0\n",
    "    # If an angle is greater than pi/2, use pi - angle.\n",
    "    Angs = np.where(Angs > np.pi/2, np.pi - Angs, Angs)\n",
    "    # In the original code the first measurement was forced to zero (presumably b = 0)\n",
    "    Angs[:, 0] = 0\n",
    "\n",
    "    # --- 3. Precompute the gamma-distributed weights for the integration over R ---\n",
    "    # Gamma distribution parameters:\n",
    "    lam = mean*10000\n",
    "    # Define R values (50 points between 0.0001 and 0.005)\n",
    "    R_vals = np.arange(0.0001, 0.01, 0.0001)  # \n",
    "    transR = (R_vals * 10000).astype(int)\n",
    "\n",
    "    weights = (lam**transR) * np.exp(-lam) / np.array([math.factorial(r) for r in transR.astype(int)]).astype(np.double)\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    # --- 4. Precompute the \"sumterm\" that appears in the restricted compartment ---\n",
    "    # Here we use m=10 terms and assume that a global array Bessel_roots is available.\n",
    "    m = 10\n",
    "    br = Bessel_roots[:m]  # shape: (m,)\n",
    "    br2 = br**2\n",
    "    br6 = br**6\n",
    "    # For each R in R_vals, compute the sumterm.\n",
    "    # We need to broadcast over R and over the m terms.\n",
    "    R2 = R_vals**2  # shape: (50,)\n",
    "    # numerator: shape (50, m)\n",
    "    num = (2 * Dperp * br2 * delta / R2[:, None] - 2 +\n",
    "           2 * np.exp(-Dperp * br2 * delta / R2[:, None]) +\n",
    "           2 * np.exp(-Dperp * br2 * Delta / R2[:, None]) -\n",
    "           np.exp(-Dperp * br2 * (Delta - delta) / R2[:, None]) -\n",
    "           np.exp(-Dperp * br2 * (Delta + delta) / R2[:, None]))\n",
    "    # denominator: shape (50, m)\n",
    "    den = (Dperp**2) * br6 * (br2 - 1) / (R_vals[:, None]**6)\n",
    "    sumterm_R = np.sum(num / den, axis=1)  # shape: (50,)\n",
    "\n",
    "    # --- 5. Compute the restricted compartment signal ---\n",
    "    # For each fiber orientation i (i = 0...N-1) and for each measurement j (j = 0...M-1)\n",
    "    # we need to compute:\n",
    "    #   Restricted(b, theta, R) = exp(-b * (cos(theta)**2) * Dpar) *\n",
    "    #                             exp(-2 * b * (sin(theta)**2) / ((Delta-delta/3)*delta**2) * sumterm)\n",
    "    #\n",
    "    # Notice that only the second exponential depends on R (via sumterm_R) and we need to integrate\n",
    "    # over R with weights.\n",
    "    #\n",
    "    # Compute the part independent of R (base) and the factor x that multiplies sumterm_R.\n",
    "    #\n",
    "    # Angs has shape (N, M) (one row per fiber) and bvals is (M,).\n",
    "    # (We assume that bvals is a 1D array; if not, cast it with np.asarray(bvals).)\n",
    "    bvals = np.asarray(bvals)  # shape: (M,)\n",
    "    base = np.exp(-bvals * (np.cos(Angs)**2) * Dpar)  # shape: (N, M)\n",
    "    # Factor multiplying sumterm_R inside the second exponential.\n",
    "    x = -2 * bvals * (np.sin(Angs)**2) / ((Delta - delta/3) * delta**2)  # shape: (N, M)\n",
    "    # For each fiber orientation and measurement, we want to compute:\n",
    "    #    f(i,j) = sum_{r=0}^{49} weights[r] * exp( x(i,j) * sumterm_R[r] )\n",
    "    # We can compute the 3D array exp(x * sumterm_R) with shape (N, M, 50) and then contract out the last axis.\n",
    "    exp_term = np.exp(x[..., None] * sumterm_R)  # shape: (N, M, 50)\n",
    "    # Now take the weighted sum over the last axis (the R axis):\n",
    "    restricted_integral = np.tensordot(exp_term, weights, axes=([2], [0]))  # shape: (N, M)\n",
    "    # The restricted compartment signal for each fiber and measurement is then:\n",
    "    Res = base * restricted_integral  # shape: (N, M)\n",
    "    #\n",
    "    # Finally, combine the fibers by weighting each fiber's contribution by its fraction.\n",
    "    # The original code did: np.sum([f * R for f,R in zip(fracs[1:],Res)], axis=0)\n",
    "    # That is equivalent to a dot product: (fracs[1:]) dot (each row of Res).\n",
    "    restricted_signal = np.dot(fracs[1:], Res)  # shape: (M,)\n",
    "\n",
    "    # --- 6. Compute the hindered compartment signal ---\n",
    "    # Compute the diffusion tensor from D (using your vals_to_mat function).\n",
    "    dh = vals_to_mat(D)\n",
    "    # The hindered signal is given by:\n",
    "    #    Hi = exp(-b * s)\n",
    "    # where s = sum((bvec @ dh)*bvec, axis=1). Here bvecs is (M,3).\n",
    "    s = np.sum((bvecs @ dh) * bvecs, axis=1)  # shape: (M,)\n",
    "    hindered_signal = np.exp(-bvals * s)  # shape: (M,)\n",
    "\n",
    "    # --- 7. Combine compartments and scale by S0 ---\n",
    "    Signal = fracs[0] * hindered_signal + restricted_signal\n",
    "    return S0 * Signal\n",
    "\n",
    "def SpherAng(v_in):\n",
    "\n",
    "    if v_in[2] < 0:\n",
    "        v_in = -v_in  # Flip the vector to the top hemisphere\n",
    "\n",
    "    x, y, z = v_in\n",
    "    r = np.linalg.norm(v_in)\n",
    "    if r == 0:\n",
    "        # Degenerate vector, define angles however you like:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    # Polar angle in [0, pi]\n",
    "    theta = np.arccos(z / r)\n",
    "    \n",
    "    # Azimuthal angle in (-pi, pi]\n",
    "    phi = np.arctan2(y, x)\n",
    "        \n",
    "    return theta,phi\n",
    "\n",
    "def j1_derivative(x):\n",
    "    \"\"\"Derivative of J1(x) using the identity: J1'(x) = 0.5 * (J0(x) - J2(x)).\"\"\"\n",
    "    return 0.5 * (j0(x) - j2(x))\n",
    "\n",
    "def j2(x):\n",
    "    \"\"\"Bessel function J_2(x).\"\"\"\n",
    "    return jv(2, x)\n",
    "\n",
    "def j1prime_zeros(n, x_max=100, step=0.1):\n",
    "    \"\"\"\n",
    "    Find the first n positive roots of J1'(x) by scanning from x=0 to x_max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n     : int\n",
    "        Number of roots to find\n",
    "    x_max : float\n",
    "        Maximum x to search\n",
    "    step  : float\n",
    "        Step size for scanning sign changes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    zeros : list of float\n",
    "        List of the first n roots (x > 0) of J1'(x).\n",
    "    \"\"\"\n",
    "    zeros = []\n",
    "    x_vals = np.arange(0.0, x_max, step)\n",
    "    \n",
    "    f_prev = j1_derivative(x_vals[0])\n",
    "    for i in range(1, len(x_vals)):\n",
    "        f_curr = j1_derivative(x_vals[i])\n",
    "        # Check for a sign change in [x_vals[i-1], x_vals[i]]\n",
    "        if f_prev * f_curr < 0:\n",
    "            root = bisect(j1_derivative, x_vals[i-1], x_vals[i])\n",
    "            zeros.append(root)\n",
    "            if len(zeros) == n:\n",
    "                break\n",
    "        f_prev = f_curr\n",
    "    \n",
    "    return zeros\n",
    "\n",
    "n_roots = 100\n",
    "Bessel_roots = np.array(j1prime_zeros(n_roots, x_max=10e6, step=0.01))\n",
    "Bessel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d12fc-bbf9-4b11-a3f1-c3b905332c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(133)\n",
    "S0 = 2000\n",
    "mean_guess = np.random.rand()*0.005+1e-4\n",
    "Params_abc =  np.random.rand(1,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(1,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind_guess = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind_guess = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind_guess])\n",
    "\n",
    "Dpar_guess = np.random.rand()*1e-3            # mm^2/s\n",
    "Dperp_guess = np.random.rand()*1e-3             # mm^2/s\n",
    "phi = 0#np.random.rand()*pi\n",
    "cos_theta = 0#np.random.rand()  # uniform in [0,1]\n",
    "theta = np.arccos(cos_theta)         # in [0, pi/2]\n",
    "Angs_guess = np.vstack([theta,phi]).T\n",
    "S0_guess =np.random.rand()*2475+25\n",
    "\n",
    "frac_guess = np.random.rand()\n",
    "guess = np.column_stack([Angs_guess,Dpar_guess,Dperp_guess,DHind_guess,frac_guess,mean_guess,S0_guess]).squeeze()\n",
    "bounds = np.array([[-np.inf,np.inf]]*13).T\n",
    "bounds[:,0] = [0,np.pi/2]\n",
    "bounds[:,1] = [-np.pi,np.pi]\n",
    "bounds[:,2] = [0,5e-3]\n",
    "bounds[:,3] = [0,5e-3]\n",
    "bounds[:,4] = [-5e-3,5e-3]\n",
    "bounds[:,5] = [-5e-3,5e-3]\n",
    "bounds[:,6] = [-5e-3,5e-3]\n",
    "bounds[:,7] = [-5e-3,5e-3]\n",
    "bounds[:,8] = [-5e-3,5e-3]\n",
    "bounds[:,9] = [-5e-3,5e-3]\n",
    "bounds[:,10] = [0,1]\n",
    "bounds[:,11] = [1e-4,0.005+1e-4]\n",
    "bounds[:,12] = [25,2500]\n",
    "\n",
    "bve_split = [bvecs[:(n_pts+1)],bvecs[(n_pts+1):2*(n_pts+1)],bvecs[2*(n_pts+1):]]\n",
    "bva_split = [bvals[:(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],bvals[2*(n_pts+1):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bf01c-b2a8-4767-b5cf-a0b3e1704753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, 54, :, :], axis=-1) != 0\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel_LS(i, j):\n",
    "    result = sp.optimize.least_squares(residuals_S0, guess, args=[maskdata[i, 54, j, :],bve_split,bva_split,Delta],\n",
    "                              bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "    return i, j, result.x\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_LS[i, j] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e89062-407b-4af0-aa27-2960a9052e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoxPlots(y_data, positions, colors, colors2, ax,hatch = False,scatter=False,scatter_alpha=0.5, **kwargs):\n",
    "\n",
    "    GREY_DARK = \"#747473\"\n",
    "    jitter = 0.02\n",
    "    # Clean data to remove NaNs column-wise\n",
    "    if(np.ndim(y_data) == 1):\n",
    "        cleaned_data = y_data[~np.isnan(y_data)]\n",
    "    else:\n",
    "        cleaned_data = [d[~np.isnan(d)] for d in y_data]\n",
    "    \n",
    "    # Define properties for the boxes (patch objects)\n",
    "    boxprops = dict(\n",
    "        linewidth=2, \n",
    "        facecolor='none',       # use facecolor for filling (set to 'none' if you want no fill)\n",
    "        edgecolor='turquoise'   # edgecolor for the outline\n",
    "    )\n",
    "\n",
    "    # Define properties for the medians (Line2D objects)\n",
    "    # Ensure GREY_DARK is defined (or replace it with a color string)\n",
    "    medianprops = dict(\n",
    "        linewidth=2, \n",
    "        color=GREY_DARK,\n",
    "        solid_capstyle=\"butt\"\n",
    "    )\n",
    "\n",
    "    # For whiskers, since they are Line2D objects, use 'color'\n",
    "    whiskerprops = dict(\n",
    "        linewidth=2, \n",
    "        color='turquoise'\n",
    "    )\n",
    "\n",
    "    bplot = ax.boxplot(\n",
    "        cleaned_data,\n",
    "        positions=positions, \n",
    "        showfliers=False,\n",
    "        showcaps = False,\n",
    "        medianprops=medianprops,\n",
    "        whiskerprops=whiskerprops,\n",
    "        boxprops=boxprops,\n",
    "        patch_artist=True,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Update the color of each box (these are patch objects)\n",
    "    for i, box in enumerate(bplot['boxes']):\n",
    "        box.set_edgecolor(colors[i])\n",
    "        if(hatch):\n",
    "            box.set_hatch('/')\n",
    "    \n",
    "    \n",
    "    # Update the color of the whiskers (each box has 2 whiskers)\n",
    "    for i in range(len(positions)):\n",
    "        bplot['whiskers'][2*i].set_color(colors[i])\n",
    "        bplot['whiskers'][2*i+1].set_color(colors[i])\n",
    "    \n",
    "    # If caps are enabled, update their color (Line2D objects)\n",
    "    if 'caps' in bplot:\n",
    "        for i, cap in enumerate(bplot['caps']):\n",
    "            cap.set_color(colors[i//2])  # two caps per box\n",
    "\n",
    "    if(scatter):\n",
    "        if(np.ndim(cleaned_data) == 1):\n",
    "            x_data = np.array([positions] * len(cleaned_data))\n",
    "            x_jittered = x_data + stats.t(df=6, scale=jitter).rvs(len(x_data))\n",
    "            ax.scatter(x_data, cleaned_data, s=100, color=colors2, alpha=scatter_alpha)\n",
    "        else:\n",
    "            x_data = [np.array([positions[i]] * len(d)) for i, d in enumerate(cleaned_data)]\n",
    "            x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "            # Plot the scatter points with jitter (using colors2)\n",
    "            for x, y, c in zip(x_jittered, cleaned_data, colors2):\n",
    "                ax.scatter(x, y, s=100, color=c, alpha=scatter_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae295b19-748d-4b13-9a2a-5c749bab0c56",
   "metadata": {},
   "source": [
    "## Multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a8c78-ca6b-473d-8e8f-8851b496af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_5/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf0c1e-f9f4-4451-9d2a-e5dc53d2e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dirs = ['NMSS_11_1year','NMSS_15','NMSS_16','NMSS_18','NMSS_19','Ctrl055_R01_28','Ctrl056_R01_29','Ctrl057_R01_30']\n",
    "BVecs = []\n",
    "BVals = []\n",
    "Deltas = []\n",
    "deltas = []\n",
    "S_masks = []\n",
    "Datas = []\n",
    "Outlines = []\n",
    "axial_middles = []\n",
    "for D in tqdm(Dirs):\n",
    "    F = pmt.read_mat(MSDir+D+'/data_loaded.mat')\n",
    "    affine = np.ones((4,4))\n",
    "    BVecs.append(F['direction'])\n",
    "    BVals.append(F['bval'])\n",
    "    Deltas.append(FixedParams['Delta'])\n",
    "    deltas.append(FixedParams['delta'])\n",
    "\n",
    "\n",
    "    \n",
    "    data, affine = reslice(F['data'], affine, (2,2,2), (2.5,2.5,2.5))\n",
    "\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    md, mk = median_otsu(data, vol_idx=range(0, 10), median_radius=5,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    Datas.append(md)\n",
    "    axial_middles.append(axial_middle)\n",
    "    Outlines.append(mk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849ec52-bcd9-4f1f-8c88-3bd031314a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "NumSamps = 10000\n",
    "\n",
    "# Directions\n",
    "x1  = np.random.randn(NumSamps)\n",
    "y1  = np.random.randn(NumSamps)\n",
    "z1  =  np.random.randn(NumSamps)\n",
    "V = np.vstack([x1,y1,z1])\n",
    "V = (V/np.linalg.norm(V,axis=0)).T\n",
    "Angs = np.array([SpherAng(v) for v in V])\n",
    "\n",
    "#Diffusion of restricted\n",
    "Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "Dperp = np.random.rand(NumSamps)*5e-3\n",
    "\n",
    "#Diffusion of hindered\n",
    "Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "\n",
    "#Fraction of hindered\n",
    "frac  = np.random.rand(NumSamps)\n",
    "\n",
    "mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "\n",
    "S0Rand =np.random.rand(NumSamps)*2475+25\n",
    "\n",
    "Choice = np.random.choice(np.arange(8),NumSamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11c7cd-ae6a-4cb7-b004-7166a2bc97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainParams = np.column_stack([V,Angs,Dpar,Dperp,DHind,frac,mean,S0Rand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1587984-1f75-4e81-9139-c06ea419a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/AxMultiMSFull_300.pickle\"):\n",
    "    with open(f\"{network_path}/AxMultiMSFull_300.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        s0 = S0Rand[i]\n",
    "        c = Choice[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "    \n",
    "        TrainSig1 = CombSignal_poisson(BVecs[c][:(n_pts+1)],BVals[c][:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(BVecs[c][(n_pts+1):2*(n_pts+1)],BVals[c][(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(BVecs[c][2*(n_pts+1):],BVals[c][2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(AddNoise(TrainSig[-1],s0,Noise))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Obs = np.hstack([NoisyTrainSig,np.expand_dims(Choice, axis=-1)])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    with open(f\"{network_path}/AxMultiMSFull_300.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34abe34-a5db-43c6-b300-c6a172c1ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bve,bva = BVecs[0],BVals[0]\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs2000 = bve[:91][bva[:91]==2000]\n",
    "distance_matrix = squareform(pdist(bvecs2000))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(9):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs2000_selected = bve[:91][bva[:91]==2000][selected_indices]\n",
    "true_indices = []\n",
    "for b in bvecs2000_selected:\n",
    "    true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs4000 = bve[:91][bva[:91]==4000]\n",
    "distance_matrix = squareform(pdist(bvecs4000))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(9):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs4000_selected = bve[:91][bva[:91]==4000][selected_indices]\n",
    "for b in bvecs4000_selected:\n",
    "    true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "true_indices1 = true_indices\n",
    "\n",
    "DevIndices = [0] + true_indices1[:3]+true_indices1[9:12] + [91+t for t in true_indices1[3:6]]+[91+t for t in true_indices1[12:15]]\\\n",
    "+ [182+t for t in true_indices1[6:9]]+[182+t for t in true_indices1[15:18]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5138d-c64d-463d-894d-3821863c073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "NumSamps = 30000\n",
    "\n",
    "# Directions\n",
    "x1  = np.random.randn(NumSamps)\n",
    "y1  = np.random.randn(NumSamps)\n",
    "z1  =  np.random.randn(NumSamps)\n",
    "V = np.vstack([x1,y1,z1])\n",
    "V = (V/np.linalg.norm(V,axis=0)).T\n",
    "Angs = np.array([SpherAng(v) for v in V])\n",
    "\n",
    "#Diffusion of restricted\n",
    "Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "Dperp = np.random.rand(NumSamps)*5e-3\n",
    "\n",
    "#Diffusion of hindered\n",
    "Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "\n",
    "#Fraction of hindered\n",
    "frac  = np.random.rand(NumSamps)\n",
    "\n",
    "mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "\n",
    "S0Rand =np.random.rand(NumSamps)*2475+25\n",
    "\n",
    "Choice = np.random.choice(np.arange(8),NumSamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342d584-f0ee-45bd-8202-7c6d726fd0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainParams = np.column_stack([V,Angs,Dpar,Dperp,DHind,frac,mean,S0Rand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62b309-0302-450c-959a-a13b891e9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "BVecs[c][DevIndices][7:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda1bc97-50b5-45ee-b256-074af0f00bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BVals[c][DevIndices[7:13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6bdb76-ad30-4f69-b5b9-a07bdffe79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DevIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5929f9c-82da-47ed-ab6a-a70a9edda4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(D[30, 30, sl, :])\n",
    "plt.plot(DevIndices,np.ones(19)*400,'xr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301e139-42ce-4ab8-be57-fb7d0c464e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/AxMultiMSDev_30.pickle\"):\n",
    "    with open(f\"{network_path}/AxMultiMSDev_30.pickle\", \"rb\") as handle:\n",
    "        posteriorDev = pickle.load(handle)\n",
    "else:\n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        s0 = S0Rand[i]\n",
    "        c = Choice[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "    \n",
    "        TrainSig1 = CombSignal_poisson(BVecs[c][DevIndices][:7],BVals[c][DevIndices][:7],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(BVecs[c][DevIndices][7:13],BVals[c][DevIndices][7:13],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(BVecs[c][DevIndices][13:],BVals[c][DevIndices][13:],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(AddNoise(TrainSig[-1],s0,Noise))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "    \n",
    "        TrainSig1 = CombSignal_poisson(BVecs[c][:(n_pts+1)],BVals[c][:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(BVecs[c][(n_pts+1):2*(n_pts+1)],BVals[c][(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(BVecs[c][2*(n_pts+1):],BVals[c][2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(AddNoise(TrainSig[-1],s0,Noise))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "    \n",
    "    Obs = np.hstack([NoisyTrainSig,np.expand_dims(Choice, axis=-1)])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    with open(f\"{network_path}/AxMultiMSDev_30.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913a4fe-850c-4b9c-98ef-d8b6ed59272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_SBI = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,axial_middles,Outlines)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = sma[:,:,sl]\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = posteriorFull.sample((1000,), x=np.append(D[i, j, sl, :],kk),show_progress_bars=False)\n",
    "        return i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "\n",
    "    Full_SBI.append(NoiseEst)\n",
    "    fig,ax = plt.subplots()\n",
    "    plt.imshow(1-NoiseEst[...,-3].T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e570f-9542-4a47-9c2b-5f37be22a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dev_SBI = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,axial_middles,Outlines)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = sma[:,:,sl]\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = posteriorDev.sample((1000,), x=np.append(D[i, j, sl, DevIndices],kk),show_progress_bars=False)\n",
    "        return i, j, np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "\n",
    "    Dev_SBI.append(NoiseEst)\n",
    "    fig,ax = plt.subplots()\n",
    "    plt.imshow(1-NoiseEst[...,-3].T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462de03e-16b7-4ea9-966a-d4e019ad38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBI_comp = []\n",
    "i = 5\n",
    "NS1 = np.copy(Dev_SBI[i][...,0])\n",
    "NS2 = np.copy(Full_SBI[i][...,0])\n",
    "Ma  = Outlines[i][:,:,axial_middles[i]]\n",
    "core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=7)\n",
    "masked_ssim = ssim_map[Outlines[i][:,:,axial_middles[i]]].mean()\n",
    "SBI_comp.append(masked_ssim.mean())\n",
    "SBI_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87447386-77a0-47e4-8836-0d288ba18cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(NS2)\n",
    "plt.show()\n",
    "plt.imshow(NS1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e3b84-41c5-4f61-8aae-f4954af82b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a287a0-4988-4e1b-beb9-8a8105b4c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_ssim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e8fe42-029f-472c-8b55-7c8eab5ea92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ssim_map)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b131348-a05d-42fa-826f-89f91de1a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBI_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97aae61-3ebc-428c-afc1-16e041798020",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(NS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58fbc97-64e6-4c83-bce5-0889781dfcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(NS1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4f944-4103-4ede-9954-65df3940eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBI_comp = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Dev_SBI[i][...,3])\n",
    "    NS2 = np.copy(Full_SBI[i][...,3])\n",
    "    Ma  = Outlines[i]\n",
    "\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SBI_comp.append(result)\n",
    "print(SBI_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a2dec-f84a-40f0-a810-c8bd005c67b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "\n",
    "    Full_SBI.append(NoiseEst)\n",
    "    fig,ax = plt.subplots()\n",
    "    plt.imshow(1-NoiseEst[...,-3].T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d38fc-6b0c-4cb4-898b-c1877db89f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4da2e-7c49-4d8f-b6a2-608ce4694138",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dirs = ['NMSS_11_1year','NMSS_15','NMSS_16','NMSS_18','NMSS_19','Ctrl055_R01_28','Ctrl056_R01_29','Ctrl057_R01_30']\n",
    "BVecs = []\n",
    "BVals = []\n",
    "Deltas = []\n",
    "deltas = []\n",
    "S_masks = []\n",
    "Datas = []\n",
    "Outlines = []\n",
    "for D in tqdm(Dirs):\n",
    "    dat = pmt.read_mat(MSDir+D+'/data_loaded.mat')\n",
    "    BVecs.append(dat['direction'])\n",
    "    BVals.append(dat['bval'])\n",
    "    Deltas.append(FixedParams['Delta'])\n",
    "    deltas.append(FixedParams['delta'])\n",
    "\n",
    "    data = dat['data']\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    md, mk = median_otsu(data, vol_idx=range(0, 10), median_radius=5,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    Datas.append(md)\n",
    "    Outlines.append(mk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19844b0-f21a-408d-a0ec-aa44262027c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "NumSamps = 300000\n",
    "\n",
    "# Directions\n",
    "x1  = np.random.randn(NumSamps)\n",
    "y1  = np.random.randn(NumSamps)\n",
    "z1  =  np.random.randn(NumSamps)\n",
    "V = np.vstack([x1,y1,z1])\n",
    "V = (V/np.linalg.norm(V,axis=0)).T\n",
    "Angs = np.array([SpherAng(v) for v in V])\n",
    "\n",
    "#Diffusion of restricted\n",
    "Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "Dperp = np.random.rand(NumSamps)*5e-3\n",
    "\n",
    "#Diffusion of hindered\n",
    "Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "\n",
    "#Fraction of hindered\n",
    "frac  = np.random.rand(NumSamps)\n",
    "\n",
    "mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "\n",
    "S0Rand =np.random.rand(NumSamps)*2475+25\n",
    "\n",
    "Choice = np.random.choice([1,2,3,4,5,6,7,8],NumSamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02021253-c434-4615-9392-c221f06817db",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainParams = np.column_stack([V,Angs,Dpar,Dperp,DHind,frac,mean,S0Rand,Choice*100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c63b9c-3f49-40c9-ae85-89ef3fb7cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/8Indv_50_300k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/8Indv_50_300k_poisson.pickle\", \"rb\") as handle:\n",
    "        posterior = pickle.load(handle)\n",
    "else:\n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        s0 = S0Rand[i]\n",
    "        c = Choice[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "    \n",
    "        TrainSig1 = CombSignal_poisson(BVecs[c-1][:(n_pts+1)],bvals[:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(BVecs[c-1][(n_pts+1):2*(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(BVecs[c-1][2*(n_pts+1):],bvals[2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(np.append(AddNoise(TrainSig[-1],s0,Noise),c*100))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Obs = torch.tensor(NoisyTrainSig).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs=100)\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    with open(f\"{network_path}/8Indv_50_300k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efe940-ea23-40a9-9f96-25a4b77e2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "IndxArr  = []\n",
    "BVecsDev = []\n",
    "BValsDev = []\n",
    "for bve,bva in zip(BVecs,BVals): \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[:91][bva[:91]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[:91][bva[:91]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[:91][bva[:91]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[:91][bva[:91]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices1 = true_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[91:182][bva[91:182]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[91:182][bva[91:182]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[91:182][bva[91:182]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[91:182][bva[91:182]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices2 = true_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[182:][bva[182:]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[182:][bva[182:]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[182:][bva[182:]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[182:][bva[182:]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices3 = true_indices\n",
    "    \n",
    "    DevIndices = [0] + true_indices1 + true_indices2 + true_indices3\n",
    "    bvecs_Dev = bve[DevIndices]\n",
    "    bvals_Dev = bva[DevIndices]\n",
    "\n",
    "    IndxArr.append(DevIndices)\n",
    "    BVecsDev.append(bvecs_Dev)\n",
    "    BValsDev.append(bvals_Dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f84a9-368d-4003-94ca-f069cd579dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "NumSamps = 300000\n",
    "\n",
    "# Directions\n",
    "x1  = np.random.randn(NumSamps)\n",
    "y1  = np.random.randn(NumSamps)\n",
    "z1  =  np.random.randn(NumSamps)\n",
    "V = np.vstack([x1,y1,z1])\n",
    "V = (V/np.linalg.norm(V,axis=0)).T\n",
    "Angs = np.array([SpherAng(v) for v in V])\n",
    "\n",
    "#Diffusion of restricted\n",
    "Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "Dperp = np.random.rand(NumSamps)*5e-3\n",
    "\n",
    "#Diffusion of hindered\n",
    "Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "\n",
    "#Fraction of hindered\n",
    "frac  = np.random.rand(NumSamps)\n",
    "\n",
    "mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "\n",
    "S0Rand =np.random.rand(NumSamps)*2475+25\n",
    "\n",
    "Choice = np.random.choice([1,2,3,4,5,6,7,8],NumSamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9c2af-1cf2-4678-b4d9-09ad9389c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainParams = np.column_stack([V,Angs,Dpar,Dperp,DHind,frac,mean,S0Rand,Choice*100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6125b8-b592-42fd-9ac4-1eb51d0146ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/Dev_8Indv_50_300k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/Dev_8Indv_50_300k_poisson.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        #s = sig2[i]\n",
    "        s0 = S0Rand[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "        c = Choice[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "\n",
    "        TrainSig1 = CombSignal_poisson(BVecsDev[c-1][:7],BValsDev[c-1][:7],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(BVecsDev[c-1][7:13],BValsDev[c-1][7:13],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(BVecsDev[c-1][13:],BValsDev[c-1][13:],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(np.append(AddNoise(TrainSig[-1],s0,Noise),c*100))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "    \n",
    "    \n",
    "    Obs = torch.tensor(NoisyTrainSig).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs=100)\n",
    "    posteriorMin = inference.build_posterior(density_estimator)\n",
    "    with open(f\"{network_path}/Dev_8Indv_50_300k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posteriorMin, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad9188-a62e-4b98-affa-dfbbcefb1992",
   "metadata": {},
   "outputs": [],
   "source": [
    "[48,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525db2aa-f168-4fd5-afef-9cc4100d1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Datas[8][:,:,48,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a02337-6ddf-4c7c-9262-2623effaa323",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_SBI = []\n",
    "for kk,(D,sl) in enumerate(zip(Datas,[48]*8)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = posterior.sample((1000,), x=np.append(D[i,j,sl, :],100*(kk+1)),show_progress_bars=False)\n",
    "        return i, j, posterior_samples_1.mean(axis=0)\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [14])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "\n",
    "    Full_SBI.append(NoiseEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd8c8b-ea30-4f0a-9f00-9409a26c8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_SBI = []\n",
    "for kk,(D,sl) in enumerate(zip(Datas,[48]*8)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = posteriorMin.sample((1000,), x=np.append(D[i,j,sl, IndxArr[kk]],100*(kk+1)),show_progress_bars=False)\n",
    "        return i, j, posterior_samples_1.mean(axis=0)\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [14])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "\n",
    "    Min_SBI.append(NoiseEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aefca6-e803-4663-a274-1a7c2f3a631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_LS = []\n",
    "for kk,(D,sl) in enumerate(zip(Datas,[48]*8)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "\n",
    "    bve_split = [BVecs[kk][:(n_pts+1)],BVecs[kk][(n_pts+1):2*(n_pts+1)],BVecs[kk][2*(n_pts+1):]]\n",
    "    bva_split = [BVals[kk][:(n_pts+1)],BVals[kk][(n_pts+1):2*(n_pts+1)],BVals[kk][2*(n_pts+1):]]\n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel_LS(i, j):\n",
    "        result = sp.optimize.least_squares(residuals_S0, guess, args=[D[i,j,sl, :],bve_split,bva_split,Delta],\n",
    "                                  bounds=bounds,verbose=0,jac='3-point')\n",
    "        return i, j, result.x\n",
    "    \n",
    "\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst_LS[i, j] = x\n",
    "\n",
    "    Full_LS.append(NoiseEst_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b2fd3-d82f-42e2-b4f8-cc567f8414aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_LS = []\n",
    "for kk,(D,sl) in enumerate(zip(Datas,[48]*8)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    bve_splitd = [BVecsDev[kk][:7],BVecsDev[kk][7:13],BVecsDev[kk][13:]]\n",
    "    bva_splitd = [BValsDev[kk][:7],BValsDev[kk][7:13],BValsDev[kk][13:]]\n",
    "\n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel_LS(i, j):\n",
    "        result = sp.optimize.least_squares(residuals_S0, guess, args=[D[i, j,sl, IndxArr[kk]],bve_splitd,bva_splitd,Delta],\n",
    "                                  bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        return i, j, result.x\n",
    "        \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst_LS[i, j] = x\n",
    "\n",
    "    Min_LS.append(NoiseEst_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f5eaa-45d1-4777-8988-0bf44f7ccb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('temp_Full_LS',Full_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e6fac-8b4b-4dc4-abbd-f0d81cfd19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('temp_Min_LS',Min_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b75ae-6862-4d78-a73c-81e06be349ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"say 'AxCaliber Fit done'\") # or '\\7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6acb8f5-d5c6-4013-a89c-74311a231653",
   "metadata": {},
   "outputs": [],
   "source": [
    "WRKir = '/Users/maximilianeggl/Dropbox/PostDoc/Silvia/SBIDTIPaper2/Code/MS_data/WM_masks/'\n",
    "WMs = []\n",
    "for i,Name in tqdm(enumerate(['NMSS_11_1year','NMSS_15','NMSS_16','NMSS_18','NMSS_19','Ctrl055_R01_28','Ctrl056_R01_29','Ctrl057_R01_30'])):\n",
    "    \n",
    "    for k,x in enumerate(os.listdir(WRKir)):\n",
    "        if Name in x:\n",
    "            print(Name)\n",
    "            WM, affine, img = load_nifti(WRKir+x, return_img=True)\n",
    "            #WM, affine = reslice(WM, affine, (2,2,2), (2.5,2.5,2.5))\n",
    "            if(i<5):\n",
    "                WM_t = np.fliplr(np.swapaxes(WM,0,1))\n",
    "            else:\n",
    "                WM_t = np.fliplr(np.flipud(np.swapaxes(WM,0,1)))\n",
    "            WMs.append(WM_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b99587c-d845-4939-a6f2-a9a0fce9d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBI_comp = []\n",
    "KK = [48]*8\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Min_SBI[i][...,3])\n",
    "    NS2 = np.copy(Full_SBI[i][...,3])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_comp.append(masked_ssim.mean())\n",
    "print(SBI_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984afcf8-8195-43ca-b54c-c7167c1582dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Par_frac(i,j,Mat):\n",
    "    MD = np.linalg.eigh(vals_to_mat(Mat[i,j]))[0].mean()\n",
    "\n",
    "    FA = FracAni(np.linalg.eigh(vals_to_mat(Mat[i,j]))[0],MD)\n",
    "    return i, j, [FA,MD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a203d4d-f02e-47aa-8297-eae2045b6198",
   "metadata": {},
   "outputs": [],
   "source": [
    "KK = [48]*8\n",
    "FA_Full_SBI = []\n",
    "MD_Full_SBI = []\n",
    "for jj in range(8):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(Par_frac)(i, j,Full_SBI[jj][...,4:10]) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, k in results:\n",
    "        temp1[i, j] = k[0]\n",
    "        temp2[i, j] = k[1]\n",
    "\n",
    "    FA_Full_SBI.append(temp1)\n",
    "    MD_Full_SBI.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e56f0-397b-421f-831e-b12b82bdcd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "KK = [48]*8\n",
    "FA_Min_SBI = []\n",
    "MD_Min_SBI = []\n",
    "for jj in range(8):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(Par_frac)(i, j,Min_SBI[jj][...,4:10]) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, k in results:\n",
    "        temp1[i, j] = k[0]\n",
    "        temp2[i, j] = k[1]\n",
    "\n",
    "    FA_Min_SBI.append(temp1)\n",
    "    MD_Min_SBI.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d2f63-da3c-4ed7-88d1-a2a96fb2890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "KK = [48]*8\n",
    "FA_Full_LS = []\n",
    "MD_Full_LS = []\n",
    "for jj in range(8):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(Par_frac)(i, j,Full_LS[jj][...,4:10]) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, k in results:\n",
    "        temp1[i, j] = k[0]\n",
    "        temp2[i, j] = k[1]\n",
    "\n",
    "    FA_Full_LS.append(temp1)\n",
    "    MD_Full_LS.append(temp2)\n",
    "\n",
    "KK = [48]*8\n",
    "FA_Min_LS = []\n",
    "MD_Min_LS = []\n",
    "for jj in range(8):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(Par_frac)(i, j,Min_LS[jj][...,4:10]) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, k in results:\n",
    "        temp1[i, j] = k[0]\n",
    "        temp2[i, j] = k[1]\n",
    "\n",
    "    FA_Min_LS.append(temp1)\n",
    "    MD_Min_LS.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3faa3dc-f364-48bf-b227-14eb01ee1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = 0\n",
    "SBI_comp = []\n",
    "KK = [48]*8\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Min_SBI[i][...,3])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(Full_SBI[i][...,3])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_comp.append(masked_ssim.mean())\n",
    "\n",
    "LS_comp = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Min_LS[i][...,3])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(Full_LS[i][...,3])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    LS_comp.append(masked_ssim.mean())\n",
    "\n",
    "SBI_LS_comp = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Full_SBI[i][...,3])\n",
    "    NS2 = np.copy(Full_LS[i][...,3])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_LS_comp.append(masked_ssim.mean())\n",
    "Prec7_SBI = []\n",
    "PrecFull_SBI = []\n",
    "\n",
    "Prec7_NLLS = []\n",
    "PrecFull_NLLS = []\n",
    "for i in range(8):\n",
    "    Prec7_SBI.append(np.std(Min_SBI[i][...,3][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_SBI.append(np.std(Full_SBI[i][...,3][WMs[i].astype(bool)[:,:,48]]))\n",
    "\n",
    "    Prec7_NLLS.append(np.std(Min_LS[i][...,3][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_NLLS.append(np.std(Full_LS[i][...,3][WMs[i].astype(bool)[:,:,48]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef452ed-ece8-4a4c-bc60-a4058cdb9573",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SBI_comp)\n",
    "g_pos = np.array([1.3])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(LS_comp)\n",
    "g_pos = np.array([1.9])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.xticks([1.3,1.9],['SBI','NNLS'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_SSIM_Dperp.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00b9c6-7a15-4fab-afd8-2a33777d31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(1,1,figsize=(3.2,4.8))\n",
    "\n",
    "y_data = np.array(PrecFull_SBI)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI)\n",
    "g_pos = np.array([1.1])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(PrecFull_NLLS)\n",
    "g_pos = np.array([1.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "\n",
    "y_data = np.array(Prec7_NLLS)\n",
    "g_pos = np.array([2.15])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([0.65,1.1,1.8,2.15],['Full','Red.','Full','Red.'],fontsize=32,rotation=90)\n",
    "\n",
    "x = np.arange(1.7,2.3,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS)[~np.isnan(PrecFull_NLLS)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS)[~np.isnan(PrecFull_NLLS)], 77)\n",
    "plt.fill_between(x,y1,y2,color='sandybrown',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.25,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI)[~np.isnan(PrecFull_SBI)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI)[~np.isnan(PrecFull_SBI)], 77)\n",
    "plt.fill_between(x,y1,y2,color='mediumturquoise',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "#ax1.set_xlim(0.3,2.8)\n",
    "ax1.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_Prec_Dperp.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8409b87-19ea-44d4-9d33-3a9d848cecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = 0\n",
    "SBI_comp = []\n",
    "KK = [48]*8\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(MD_Min_SBI[i])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(MD_Full_SBI[i])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_comp.append(masked_ssim.mean())\n",
    "\n",
    "LS_comp = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(MD_Min_LS[i])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(MD_Full_LS[i])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    LS_comp.append(masked_ssim.mean())\n",
    "\n",
    "SBI_LS_comp = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(MD_Full_SBI[i])\n",
    "    NS2 = np.copy(MD_Full_LS[i])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_LS_comp.append(masked_ssim.mean())\n",
    "Prec7_SBI = []\n",
    "PrecFull_SBI = []\n",
    "\n",
    "Prec7_NLLS = []\n",
    "PrecFull_NLLS = []\n",
    "for i in range(8):\n",
    "    Prec7_SBI.append(np.std(MD_Min_SBI[i][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_SBI.append(np.std(MD_Full_SBI[i][WMs[i].astype(bool)[:,:,48]]))\n",
    "\n",
    "    Prec7_NLLS.append(np.std(MD_Min_LS[i][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_NLLS.append(np.std(MD_Full_LS[i][WMs[i].astype(bool)[:,:,48]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4289a4-f11c-467b-8f69-c687838f44a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SBI_comp)\n",
    "g_pos = np.array([1.3])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(LS_comp)\n",
    "g_pos = np.array([1.9])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.xticks([1.3,1.9],['SBI','NNLS'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_SSIM_MD.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb8faa-fd38-4604-b59f-f277a0c1564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(1,1,figsize=(3.2,4.8))\n",
    "\n",
    "y_data = np.array(PrecFull_SBI)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI)\n",
    "g_pos = np.array([1.1])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(PrecFull_NLLS)\n",
    "g_pos = np.array([1.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "\n",
    "y_data = np.array(Prec7_NLLS)\n",
    "g_pos = np.array([2.15])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([0.65,1.1,1.8,2.15],['Full','Red.','Full','Red.'],fontsize=32,rotation=90)\n",
    "\n",
    "x = np.arange(1.7,2.3,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS)[~np.isnan(PrecFull_NLLS)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS)[~np.isnan(PrecFull_NLLS)], 77)\n",
    "plt.fill_between(x,y1,y2,color='sandybrown',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.25,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI)[~np.isnan(PrecFull_SBI)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI)[~np.isnan(PrecFull_SBI)], 77)\n",
    "plt.fill_between(x,y1,y2,color='mediumturquoise',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "#ax1.set_xlim(0.3,2.8)\n",
    "ax1.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_Prec_MD.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290057f-bfa9-45f3-bb1f-cf12a7e74a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = -4\n",
    "SBI_comp = []\n",
    "KK = [48]*8\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Min_SBI[i][...,jj])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(Full_SBI[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_comp.append(masked_ssim.mean())\n",
    "\n",
    "LS_comp = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Min_LS[i][...,jj])\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = np.copy(Full_LS[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    LS_comp.append(masked_ssim.mean())\n",
    "\n",
    "SBI_LS_comp = []\n",
    "for i in range(8):\n",
    "    NS1 = np.copy(Full_SBI[i][...,jj])\n",
    "    NS2 = np.copy(Full_LS[i][...,jj])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_LS_comp.append(masked_ssim.mean())\n",
    "Prec7_SBI = []\n",
    "PrecFull_SBI = []\n",
    "\n",
    "Prec7_NLLS = []\n",
    "PrecFull_NLLS = []\n",
    "for i in range(8):\n",
    "    Prec7_SBI.append(np.std(Min_SBI[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_SBI.append(np.std(Full_SBI[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "\n",
    "    Prec7_NLLS.append(np.std(Min_LS[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n",
    "    PrecFull_NLLS.append(np.std(Full_LS[i][...,jj][WMs[i].astype(bool)[:,:,48]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321680f-7c6d-4b2d-a658-c4cdb803fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax)\n",
    "y_data = np.array(SBI_comp)\n",
    "g_pos = np.array([1.3])\n",
    "colors = ['lightseagreen']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True,scatter_alpha=0.5)\n",
    "\n",
    "y_data = np.array(LS_comp)\n",
    "g_pos = np.array([1.9])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.xticks([1.3,1.9],['SBI','NNLS'],fontsize=32,rotation=90)\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "ax.set_ylim(-0.1,1)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_SSIM_Frac.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a079089-2fa8-4be9-8fe5-b5660797155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(1,1,figsize=(3.2,4.8))\n",
    "\n",
    "y_data = np.array(PrecFull_SBI)\n",
    "g_pos = np.array([0.65])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(Prec7_SBI)\n",
    "g_pos = np.array([1.1])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(PrecFull_NLLS)\n",
    "g_pos = np.array([1.8])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "\n",
    "\n",
    "y_data = np.array(Prec7_NLLS)\n",
    "g_pos = np.array([2.15])\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax1,widths=0.2,scatter=True)\n",
    "plt.xticks([0.65,1.1,1.8,2.15],['Full','Red.','Full','Red.'],fontsize=32,rotation=90)\n",
    "\n",
    "x = np.arange(1.7,2.3,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS)[~np.isnan(PrecFull_NLLS)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_NLLS)[~np.isnan(PrecFull_NLLS)], 77)\n",
    "plt.fill_between(x,y1,y2,color='sandybrown',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "x = np.arange(0.55,1.25,0.05)\n",
    "y1 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI)[~np.isnan(PrecFull_SBI)], 25)\n",
    "y2 = np.ones_like(x)*np.percentile(np.array(PrecFull_SBI)[~np.isnan(PrecFull_SBI)], 77)\n",
    "plt.fill_between(x,y1,y2,color='mediumturquoise',zorder=10,alpha=0.2,hatch='//')\n",
    "\n",
    "#ax1.set_xlim(0.3,2.8)\n",
    "ax1.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax1.set_yticks([0,0.1,0.2])\n",
    "if Save: plt.savefig(FigLoc+'MS_Ax_Prec_Frac.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718aaab-a818-4181-a986-df5af0cbf546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf5c92-2834-46c4-a7d8-5b7bc15aa732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
