{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080f0173-04fa-42e4-bd77-e911bc2c8486",
   "metadata": {},
   "source": [
    "# Frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630fda2-22c4-4565-ab02-6a64b311e8cc",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f15b22-2eea-41ce-afaf-3cd1197b1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dill as pickle\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import lognorm,norm\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from scipy.special import i0e\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.optimize import least_squares\n",
    "# Define font properties\n",
    "font = {\n",
    "    'family': 'sans-serif',  # Use sans-serif family\n",
    "    'sans-serif': ['Helvetica'],  # Specify Helvetica as the sans-serif font\n",
    "    'size': 14  # Set the default font size\n",
    "}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# Set tick label sizes\n",
    "plt.rc('ytick', labelsize=24)\n",
    "plt.rc('xtick', labelsize=24)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n",
    "# Customize axes spines and legend appearance\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "\n",
    "from sbi import analysis as analysis\n",
    "from sbi import utils as utils\n",
    "from sbi.inference import SNPE, simulate_for_sbi\n",
    "from sbi.inference.potentials.posterior_based_potential import posterior_estimator_based_potential\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")\n",
    "from sbi.utils import process_prior,BoxUniform\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Categorical,Normal\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "from dipy.sims.voxel import single_tensor\n",
    "from dipy.data import get_fnames\n",
    "from dipy.io.gradients import read_bvals_bvecs\n",
    "from dipy.core.gradients import gradient_table\n",
    "from dipy.reconst.dti import (decompose_tensor, from_lower_triangular)\n",
    "from dipy.io.image import load_nifti\n",
    "from dipy.segment.mask import median_otsu\n",
    "import dipy.reconst.dti as dti\n",
    "import dipy.reconst.dki as dki\n",
    "from dipy.align.reslice import reslice\n",
    "from dipy.core.sphere import disperse_charges, Sphere, HemiSphere\n",
    "\n",
    "import pymatreader as pmt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import i0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d079e5a0-650c-4f63-a403-c986222d877f",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee99bb0-d86e-46a7-ad4e-f0a645c55775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vals_to_mat(dt):\n",
    "    DTI = np.zeros((3,3))\n",
    "    DTI[0,0] = dt[0]\n",
    "    DTI[0,1],DTI[1,0] =  dt[1],dt[1]\n",
    "    DTI[1,1] =  dt[2]\n",
    "    DTI[0,2],DTI[2,0] =  dt[3],dt[3]\n",
    "    DTI[1,2],DTI[2,1] =  dt[4],dt[4]\n",
    "    DTI[2,2] =  dt[5]\n",
    "    return DTI\n",
    "\n",
    "def mat_to_vals(DTI):\n",
    "    dt = np.zeros(6)\n",
    "    dt[0] = DTI[0,0]\n",
    "    dt[1] = DTI[0,1]\n",
    "    dt[2] = DTI[1,1]\n",
    "    dt[3] = DTI[0,2]\n",
    "    dt[4] = DTI[1,2]\n",
    "    dt[5] = DTI[2,2]\n",
    "    return dt\n",
    "\n",
    "def fill_lower_diag(a):\n",
    "    b = [a[0],a[3],a[1],a[4],a[5],a[2]]\n",
    "    n = 3\n",
    "    mask = np.tri(n,dtype=bool) \n",
    "    out = np.zeros((n,n),dtype=float)\n",
    "    out[mask] = b\n",
    "    return out\n",
    "\n",
    "def ComputeDTI(params):\n",
    "    L = fill_lower_diag(params)\n",
    "    \n",
    "    np.fill_diagonal(L, np.abs(np.diagonal(L)))\n",
    "\n",
    "    A = L @ L.T\n",
    "    return A\n",
    "    \n",
    "def ComputeDTI(params):\n",
    "    L = fill_lower_diag(params)\n",
    "    \n",
    "    np.fill_diagonal(L, np.abs(np.diagonal(L)))\n",
    "\n",
    "    A = L @ L.T\n",
    "    return A\n",
    "\n",
    "def ForceLowFA(dt):\n",
    "    # Modify the matrix to ensure low FA (more isotropic)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(dt)\n",
    "    \n",
    "    # Make the eigenvalues more similar to enforce low FA\n",
    "    mean_eigenvalue = np.mean(eigenvalues)\n",
    "\n",
    "    adjusted_eigenvalues = np.clip(eigenvalues, mean_eigenvalue * np.random.rand(), mean_eigenvalue * 1.0)\n",
    "    \n",
    "    # Reconstruct the matrix with the adjusted eigenvalues\n",
    "    dt_low_fa = eigenvectors @ np.diag(adjusted_eigenvalues) @ eigenvectors.T\n",
    "    \n",
    "    return dt_low_fa\n",
    "    \n",
    "def FracAni(evals,MD):\n",
    "    numerator = np.sqrt(3 * np.sum((evals - MD) ** 2))\n",
    "    denominator = np.sqrt(2) * np.sqrt(np.sum(evals ** 2))\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "def clip_negative_eigenvalues(matrix):\n",
    "    # Perform eigenvalue decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "    \n",
    "    # Clip negative eigenvalues to 0\n",
    "    clipped_eigenvalues = np.maximum(eigenvalues, 1e-5)\n",
    "    \n",
    "    # Reconstruct the matrix with the clipped eigenvalues\n",
    "    clipped_matrix = eigenvectors @ np.diag(clipped_eigenvalues) @ np.linalg.inv(eigenvectors)\n",
    "    \n",
    "    return clipped_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b35170-5f7a-410b-835d-c542141918e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FitDT(Dat,seed=1):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    # DT_abc\n",
    "    data = Dat[:,0]\n",
    "    shape,loc,scale = lognorm.fit(data)\n",
    "    \n",
    "    dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "\n",
    "    #DT_rest\n",
    "    data = Dat[:,1]\n",
    "    loc,scale = norm.fit(data)\n",
    "    \n",
    "    # Compute the fitted PDF\n",
    "    dti2_fitted = stats.norm(loc=loc, scale=scale)\n",
    "\n",
    "    return dti1_fitted,dti2_fitted\n",
    "\n",
    "def FitKT(Dat,seed=1):\n",
    "    np.random.seed(seed)    \n",
    "    # Fitting x4\n",
    "    data = Dat[:,0]\n",
    "    shape,loc,scale = lognorm.fit(data)\n",
    "    x4_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "    \n",
    "    # Fitting R1\n",
    "    data = Dat[:,3]\n",
    "    loc,scale = norm.fit(data)\n",
    "    R1_fitted = norm(loc,scale)\n",
    "    \n",
    "    # Fitting x2\n",
    "    data = Dat[:,9]\n",
    "    shape,loc,scale = lognorm.fit(data)\n",
    "    x2_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "\n",
    "    # Fitting R2\n",
    "    data = Dat[:,12]\n",
    "    loc,scale = norm.fit(data)\n",
    "    R2_fitted = norm(loc,scale)\n",
    "\n",
    "\n",
    "    return x4_fitted,R1_fitted,x2_fitted,R2_fitted\n",
    "\n",
    "def GenDTKT(DT_Fits,KT_Fits,seed,size):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    DT = np.zeros([size,6])\n",
    "    KT = np.zeros([size,15])\n",
    "\n",
    "    DT[:,0] = DT_Fits[0].rvs(size)\n",
    "    DT[:,2] = DT_Fits[0].rvs(size)\n",
    "    DT[:,5] = DT_Fits[0].rvs(size)\n",
    "\n",
    "    DT[:,1] = DT_Fits[1].rvs(size)\n",
    "    DT[:,3] = DT_Fits[1].rvs(size)\n",
    "    DT[:,4] = DT_Fits[1].rvs(size)\n",
    "\n",
    "    for k in range(3):\n",
    "        KT[:,k] = KT_Fits[0].rvs(size)\n",
    "    for k in range(3,9):\n",
    "        KT[:,k] = KT_Fits[1].rvs(size)\n",
    "    for k in range(9,12):\n",
    "        KT[:,k] = KT_Fits[2].rvs(size)\n",
    "    for k in range(12,15):\n",
    "        KT[:,k] = KT_Fits[3].rvs(size)\n",
    "\n",
    "    return DT,KT\n",
    "    \n",
    "def DKIMetrics(dt,kt,analytical=True):\n",
    "    if(dt.ndim == 1):\n",
    "        dt = vals_to_mat(dt)\n",
    "    evals,evecs = np.linalg.eigh(dt)\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evals = evals[idx]\n",
    "    evecs = evecs[:, idx]\n",
    "    \n",
    "    params = np.concatenate([evals,np.hstack(evecs),kt])\n",
    "    params2 = np.concatenate([evals,np.hstack(evecs),-kt])\n",
    "\n",
    "    mk = dki.mean_kurtosis(params,analytical=analytical,min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    ak = dki.axial_kurtosis(params,analytical=analytical,min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    rk = dki.radial_kurtosis(params,analytical=analytical,min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    mkt = dki.mean_kurtosis_tensor(params, min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    kfa = kurtosis_fractional_anisotropy_test(params)\n",
    "\n",
    "    return mk,ak,rk,mkt,kfa\n",
    "    \n",
    "def kurtosis_fractional_anisotropy_test(dki_params):\n",
    "    r\"\"\"Compute the anisotropy of the kurtosis tensor (KFA).\n",
    "\n",
    "    See :footcite:p:`Glenn2015` and :footcite:p:`NetoHenriques2021` for further\n",
    "    details about the method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dki_params : ndarray (x, y, z, 27) or (n, 27)\n",
    "        All parameters estimated from the diffusion kurtosis model.\n",
    "        Parameters are ordered as follows:\n",
    "            1) Three diffusion tensor's eigenvalues\n",
    "            2) Three lines of the eigenvector matrix each containing the first,\n",
    "                second and third coordinates of the eigenvector\n",
    "            3) Fifteen elements of the kurtosis tensor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kfa : array\n",
    "        Calculated mean kurtosis tensor.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The KFA is defined as :footcite:p:`Glenn2015`:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "         KFA \\equiv\n",
    "         \\frac{||\\mathbf{W} - MKT \\mathbf{I}^{(4)}||_F}{||\\mathbf{W}||_F}\n",
    "\n",
    "    where $W$ is the kurtosis tensor, MKT the kurtosis tensor mean, $I^{(4)}$ is\n",
    "    the Miny symmetric rank 2 isotropic tensor and $||...||_F$ is the tensor's\n",
    "    Frobenius norm :footcite:p:`Glenn2015`.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. footbibliography::\n",
    "\n",
    "    \"\"\"\n",
    "    Wxxxx = dki_params[..., 12]\n",
    "    Wyyyy = dki_params[..., 13]\n",
    "    Wzzzz = dki_params[..., 14]\n",
    "    Wxxxy = dki_params[..., 15]\n",
    "    Wxxxz = dki_params[..., 16]\n",
    "    Wxyyy = dki_params[..., 17]\n",
    "    Wyyyz = dki_params[..., 18]\n",
    "    Wxzzz = dki_params[..., 19]\n",
    "    Wyzzz = dki_params[..., 20]\n",
    "    Wxxyy = dki_params[..., 21]\n",
    "    Wxxzz = dki_params[..., 22]\n",
    "    Wyyzz = dki_params[..., 23]\n",
    "    Wxxyz = dki_params[..., 24]\n",
    "    Wxyyz = dki_params[..., 25]\n",
    "    Wxyzz = dki_params[..., 26]\n",
    "\n",
    "\n",
    "    W = 1.0 / 5.0 * (Wxxxx + Wyyyy + Wzzzz + 2 * Wxxyy + 2 * Wxxzz + 2 * Wyyzz)\n",
    "    # Compute's equation numerator\n",
    "    A = (\n",
    "        (Wxxxx - W) ** 2\n",
    "        + (Wyyyy - W) ** 2\n",
    "        + (Wzzzz - W) ** 2\n",
    "        + 4 * (Wxxxy**2 + Wxxxz**2 + Wxyyy**2 + Wyyyz**2 + Wxzzz**2 + Wyzzz**2)\n",
    "        + 6 * ((Wxxyy - W / 3) ** 2 + (Wxxzz - W / 3) ** 2 + (Wyyzz - W / 3) ** 2)\n",
    "        + 12 * (Wxxyz**2 + Wxyyz**2 + Wxyzz**2)\n",
    "    )\n",
    "    # Compute's equation denominator\n",
    "    B = (\n",
    "        Wxxxx**2\n",
    "        + Wyyyy**2\n",
    "        + Wzzzz**2\n",
    "        + 4 * (Wxxxy**2 + Wxxxz**2 + Wxyyy**2 + Wyyyz**2 + Wxzzz**2 + Wyzzz**2)\n",
    "        + 6 * (Wxxyy**2 + Wxxzz**2 + Wyyzz**2)\n",
    "        + 12 * (Wxxyz**2 + Wxyyz**2 + Wxyzz**2)\n",
    "    )\n",
    "\n",
    "    # Compute KFA\n",
    "    KFA = np.zeros(A.shape)\n",
    "    KFA = np.sqrt(A / B)\n",
    "\n",
    "    return KFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a434193-abea-45ec-a91a-edb095873b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomSimulator(Mat,gtab,S0,snr=None):\n",
    "    evals,evecs = np.linalg.eigh(Mat)\n",
    "    signal = single_tensor(gtab, S0=S0, evals=evals, evecs=evecs)\n",
    "    if(snr is None):\n",
    "        return signal\n",
    "    else:\n",
    "        return AddNoise(signal,S0,snr)\n",
    "\n",
    "def Simulator(gtab,S0,params,SNR):\n",
    "\n",
    "    dt = ComputeDTI(params)\n",
    "    signal_dti = CustomSimulator(dt,gtab,S0,SNR)\n",
    "    \n",
    "    return signal_dti\n",
    "\n",
    "\n",
    "def GenRicciNoise(signal,S0,snr):\n",
    "\n",
    "    size = signal.shape\n",
    "    sigma = S0 / snr\n",
    "    noise1 = np.random.normal(0, sigma, size=size)\n",
    "    noise2 = np.random.normal(0, sigma, size=size)\n",
    "\n",
    "    return np.sqrt((signal+noise1) ** 2 + noise2 ** 2)\n",
    "\n",
    "\n",
    "def AddNoise(signal,S0,snr):\n",
    "    \n",
    "    return GenRicciNoise(signal,S0,snr)\n",
    "\n",
    "def CustomDKISimulator(dt,kt,gtab,S0,snr=None):\n",
    "    if(dt.ndim == 1):\n",
    "        dt = vals_to_mat(dt)\n",
    "    evals,evecs = np.linalg.eigh(dt)\n",
    "    params = np.concatenate([evals,np.hstack(evecs),kt])\n",
    "    signal = dki.dki_prediction(params,gtab,S0)\n",
    "    if(snr is None):\n",
    "        return signal\n",
    "    else:\n",
    "        return AddNoise(signal,S0,snr)\n",
    "\n",
    "def CustomDKISimulator2(params,kt,gtab,S0,snr=None):\n",
    "    dt = ComputeDTI(params)\n",
    "    evals,evecs = np.linalg.eigh(dt)\n",
    "    combined_set = np.concatenate([evals,np.hstack(evecs),kt])\n",
    "    signal = dki.dki_prediction(combined_set,gtab,S0)\n",
    "    if(np.isnan(signal).any() or np.isinf(signal).any() or (signal>1e15).any()):\n",
    "        pass#import pdb;pdb.set_trace()\n",
    "    if(snr is None):\n",
    "        return signal\n",
    "    else:\n",
    "        return AddNoise(signal,S0,snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7c4d1-00e9-43b2-8dd7-8c3593ab0398",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIPrior:\n",
    "    def __init__(self, lower_abs : Tensor, upper_abs : Tensor, \n",
    "                       lower_rest: Tensor, upper_rest: Tensor,\n",
    "                        return_numpy: bool = False):\n",
    "\n",
    "        self.dist_abs = BoxUniform(low= lower_abs* torch.ones(3), high=upper_abs * torch.ones(3))\n",
    "        self.dist_rest = BoxUniform(low=lower_rest * torch.ones(3), high=upper_rest *torch.ones(3))\n",
    "        self.return_numpy = return_numpy\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        \n",
    "        abc  = self.dist_abs.sample(sample_shape)\n",
    "        rest = self.dist_rest.sample(sample_shape)\n",
    "        \n",
    "        if self.return_numpy:   \n",
    "            params = np.hstack([abc,rest]) \n",
    "        else:\n",
    "            params = torch.hstack([abc,rest])\n",
    "\n",
    "        return params\n",
    "        \n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        \n",
    "        abc  = values[:,:3]\n",
    "        rest = values[:,3:]\n",
    "\n",
    "        log_prob_abc  = self.dist_abs.log_prob(abc)\n",
    "        log_prob_rest = self.dist_rest.log_prob(rest)\n",
    "        return log_prob_abc+log_prob_rest\n",
    "\n",
    "class DTIPriorS0:\n",
    "    def __init__(self, lower_abs : Tensor, upper_abs : Tensor, \n",
    "                       lower_rest: Tensor, upper_rest: Tensor,\n",
    "                       lower_S0: Tensor, upper_S0: Tensor,\n",
    "                        return_numpy: bool = False):\n",
    "\n",
    "        self.dist_abs = BoxUniform(low= lower_abs* torch.ones(3), high=upper_abs * torch.ones(3))\n",
    "        self.dist_rest = BoxUniform(low=lower_rest * torch.ones(3), high=upper_rest *torch.ones(3))\n",
    "        self.dist_S0 = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "        self.return_numpy = return_numpy\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        \n",
    "        abc  = self.dist_abs.sample(sample_shape)\n",
    "        rest = self.dist_rest.sample(sample_shape)\n",
    "        S0   = self.dist_S0.sample(sample_shape)\n",
    "        \n",
    "        if self.return_numpy:   \n",
    "            params = np.hstack([abc,rest,S0]) \n",
    "        else:\n",
    "            params = torch.hstack([abc,rest,S0])\n",
    "\n",
    "        return params\n",
    "        \n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        \n",
    "        abc  = values[:,:3]\n",
    "        rest = values[:,3:-1]\n",
    "        S0   = values[:,-1]\n",
    "\n",
    "        log_prob_abc  = self.dist_abs.log_prob(abc)\n",
    "        log_prob_rest = self.dist_rest.log_prob(rest)\n",
    "        log_prob_S0 = self.dist_S0.log_prob(S0)\n",
    "        return log_prob_abc+log_prob_rest+log_prob_S0\n",
    "\n",
    "class DTIPriorS0Direc:\n",
    "    def __init__(self, lower_abs : Tensor, upper_abs : Tensor, \n",
    "                       lower_rest: Tensor, upper_rest: Tensor,\n",
    "                       lower_S0: Tensor, upper_S0: Tensor,\n",
    "                        return_numpy: bool = False):\n",
    "\n",
    "        self.dist_abs = BoxUniform(low= lower_abs* torch.ones(3), high=upper_abs * torch.ones(3))\n",
    "        self.dist_rest = BoxUniform(low=lower_rest * torch.ones(3), high=upper_rest *torch.ones(3))\n",
    "        self.dist_S0 = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "        self.direction_choice = Categorical(probs=torch.ones(1, 5))\n",
    "        self.return_numpy = return_numpy\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        \n",
    "        abc  = self.dist_abs.sample(sample_shape)\n",
    "        rest = self.dist_rest.sample(sample_shape)\n",
    "        S0   = self.dist_S0.sample(sample_shape)\n",
    "        direc = self.direction_choice.sample(sample_shape)       \n",
    "        \n",
    "        if self.return_numpy:   \n",
    "            params = np.hstack([abc,rest,S0,direc]) \n",
    "        else:\n",
    "            params = torch.hstack([abc,rest,S0,direc])\n",
    "\n",
    "        return params\n",
    "        \n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        \n",
    "        abc  = values[:,:3]\n",
    "        rest = values[:,3:-2]\n",
    "        S0   = values[:,-2]\n",
    "        direc   = values[:,-1]\n",
    "\n",
    "        log_prob_abc   = self.dist_abs.log_prob(abc)\n",
    "        log_prob_rest  = self.dist_rest.log_prob(rest)\n",
    "        log_prob_S0    = self.dist_S0.log_prob(S0)\n",
    "        log_prob_direc =  self.direction_choice.log_prob(direc)\n",
    "        return log_prob_abc+log_prob_rest+log_prob_S0+log_prob_direc\n",
    "\n",
    "class DTIPriorS0Noise:\n",
    "    def __init__(self, lower_abs : Tensor, upper_abs : Tensor, \n",
    "                       lower_rest: Tensor, upper_rest: Tensor,\n",
    "                       lower_S0: Tensor, upper_S0: Tensor,\n",
    "                       lower_noise: Tensor, upper_noise: Tensor,\n",
    "                        return_numpy: bool = False):\n",
    "\n",
    "        self.dist_abs = BoxUniform(low= lower_abs* torch.ones(3), high=upper_abs * torch.ones(3))\n",
    "        self.dist_rest = BoxUniform(low=lower_rest * torch.ones(3), high=upper_rest *torch.ones(3))\n",
    "        self.dist_S0 = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "        self.dist_noise = BoxUniform(low=torch.tensor([lower_noise]), high=torch.tensor([upper_noise]))\n",
    "        self.return_numpy = return_numpy\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        \n",
    "        abc     = self.dist_abs.sample(sample_shape)\n",
    "        rest    = self.dist_rest.sample(sample_shape)\n",
    "        S0      = self.dist_S0.sample(sample_shape)\n",
    "        noise   = self.dist_noise.sample(sample_shape)\n",
    "        \n",
    "        if self.return_numpy:   \n",
    "            params = np.hstack([abc,rest,S0,noise]) \n",
    "        else:\n",
    "            params = torch.hstack([abc,rest,S0,noise])\n",
    "\n",
    "        return params\n",
    "        \n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        \n",
    "        abc     = values[:,:3]\n",
    "        rest    = values[:,3:-2]\n",
    "        S0      = values[:,-2]\n",
    "        noise   = values[:,-1]\n",
    "\n",
    "        log_prob_abc  = self.dist_abs.log_prob(abc)\n",
    "        log_prob_rest = self.dist_rest.log_prob(rest)\n",
    "        log_prob_S0 = self.dist_S0.log_prob(S0)\n",
    "        log_prob_noise = self.dist_noise.log_prob(noise)\n",
    "        return log_prob_abc+log_prob_rest+log_prob_S0+log_prob_noise\n",
    "\n",
    "def histogram_mode(data, bins=50):\n",
    "    # Calculate the histogram\n",
    "    counts, bin_edges = np.histogram(data, bins=bins)\n",
    "    \n",
    "    # Find the bin with the maximum count (highest frequency)\n",
    "    max_bin_index = np.argmax(counts)\n",
    "    \n",
    "    # Calculate the mode as the midpoint of the bin with the highest count\n",
    "    mode = (bin_edges[max_bin_index] + bin_edges[max_bin_index + 1]) / 2\n",
    "    \n",
    "    return mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72ee52-6bf7-41b7-9ed6-43e088bb2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Errors(Guess,Truth,gtab,signal_true,signal_provided,S0Guess=200):\n",
    "    # Eigenvalue error\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess)\n",
    "    evals_guess = np.sort(evals_guess_raw)\n",
    "    evals_true_raw,evecs_true = np.linalg.eigh(Truth)\n",
    "    evals_true = np.sort(evals_true_raw)\n",
    "    \n",
    "    EigError = np.linalg.norm(evals_guess-evals_true)\n",
    "\n",
    "    # Mean diffusivitiy\n",
    "    mean_true = np.mean(evals_true)\n",
    "    mean_guess = np.mean(evals_guess)\n",
    "    MD = abs(mean_true-mean_guess)\n",
    "\n",
    "    # Fractional Anisotropy\n",
    "    FA_true  = FracAni(evals_true,mean_true)\n",
    "    FA_guess = FracAni(evals_guess,mean_guess)\n",
    "    FA = abs(FA_true-FA_guess)                                        \n",
    "\n",
    "    # Frobenius error\n",
    "    Frob =  np.linalg.norm(Guess-Truth, 'fro')\n",
    "\n",
    "    # Signal error\n",
    "    signal_guess = single_tensor(gtab, S0=S0Guess, evals=evals_guess_raw, evecs=evecs_guess)\n",
    "    Err  = np.linalg.norm(signal_true-signal_guess)/len(signal_true)\n",
    "    Corr = np.corrcoef(signal_true,signal_guess)[0,1]\n",
    "    \n",
    "    Err2  = np.linalg.norm(signal_provided-signal_guess[:len(signal_provided)])/len(signal_provided)\n",
    "    Corr2 = np.corrcoef(signal_provided,signal_guess[:len(signal_provided)])[0,1]\n",
    "    \n",
    "    return MD,FA,EigError,Frob,Err,Corr,Err2,Corr2\n",
    "\n",
    "def ErrorsMDFA(Guess,Truth):\n",
    "    # Eigenvalue error\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess)\n",
    "    evals_guess = np.sort(evals_guess_raw)\n",
    "    evals_true_raw,evecs_true = np.linalg.eigh(Truth)\n",
    "    evals_true = np.sort(evals_true_raw)\n",
    "\n",
    "    # Mean diffusivitiy\n",
    "    mean_true = np.mean(evals_true)\n",
    "    mean_guess = np.mean(evals_guess)\n",
    "    if(not mean_true == 0):\n",
    "        MD = abs(mean_true-mean_guess)\n",
    "    else:\n",
    "        MD = abs(mean_true-mean_guess)\n",
    "\n",
    "    # Fractional Anisotropy\n",
    "    FA_true  = FracAni(evals_true,mean_true)\n",
    "    FA_guess = FracAni(evals_guess,mean_guess)\n",
    "    if(not FA_true == 0):\n",
    "        FA = abs(FA_true-FA_guess)\n",
    "    else:\n",
    "        FA = abs(FA_true-FA_guess)\n",
    "                                    \n",
    "    \n",
    "    return MD,FA\n",
    "    \n",
    "def PercsMDFA(Guess,Truth):\n",
    "    # Eigenvalue error\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess)\n",
    "    evals_guess = np.sort(evals_guess_raw)\n",
    "    evals_true_raw,evecs_true = np.linalg.eigh(Truth)\n",
    "    evals_true = np.sort(evals_true_raw)\n",
    "\n",
    "    # Mean diffusivitiy\n",
    "    mean_true = np.mean(evals_true)\n",
    "    mean_guess = np.mean(evals_guess)\n",
    "    if(not mean_true == 0):\n",
    "        MD = abs(mean_true-mean_guess)/mean_true\n",
    "    else:\n",
    "        MD = abs(mean_true-mean_guess)/mean_true\n",
    "\n",
    "    # Fractional Anisotropy\n",
    "    FA_true  = FracAni(evals_true,mean_true)\n",
    "    FA_guess = FracAni(evals_guess,mean_guess)\n",
    "    if(not FA_true == 0):\n",
    "        FA = abs(FA_true-FA_guess)/FA_true\n",
    "    else:\n",
    "        FA = abs(FA_true-FA_guess)/FA_true\n",
    "                                    \n",
    "    \n",
    "    return MD,FA\n",
    "\n",
    "\n",
    "def DKIErrors(GuessDT,GuessKT,TruthDT,TruthKT):\n",
    "    guess = DKIMetrics(GuessDT,GuessKT,False)\n",
    "    truth = DKIMetrics(TruthDT,TruthKT,False)\n",
    "\n",
    "    #mk diff\n",
    "    mk = abs(guess[0]-truth[0])\n",
    "    ak = abs(guess[1]-truth[1])\n",
    "    rk = abs(guess[2]-truth[2])\n",
    "    mkt = abs(guess[3]-truth[3])\n",
    "    kfa = abs(guess[4]-truth[4])\n",
    "\n",
    "    return mk,ak,rk,mkt,kfa\n",
    "\n",
    "def Percs(GuessDT,GuessKT,TruthDT,TruthKT):\n",
    "    guess = DKIMetrics(GuessDT,GuessKT,False)\n",
    "    truth = DKIMetrics(TruthDT,TruthKT,False)\n",
    "    \n",
    "    #mk diff\n",
    "    mk = abs(guess[0]-truth[0])/abs(truth[0])\n",
    "    ak = abs(guess[1]-truth[1])/abs(truth[1])\n",
    "    rk = abs(guess[2]-truth[2])/abs(truth[2])\n",
    "    mkt = abs(guess[3]-truth[3])/abs(truth[3])\n",
    "    kfa = abs(guess[4]-truth[4])/abs(truth[4])\n",
    "    \n",
    "    return mk,ak,rk,mkt,kfa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e0c9e-1f61-4830-88d4-11bbfc9f7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viol_plot(A,col,hatch=False,**kwargs):\n",
    "    A_T = np.transpose(A)\n",
    "    filtered_A = []\n",
    "    for column in A_T:\n",
    "        # Remove NaNs\n",
    "        column = column[~np.isnan(column)]\n",
    "        # Filter data within 3 standard deviations\n",
    "\n",
    "        Q1 = np.percentile(column, 25)\n",
    "        Q3 = np.percentile(column, 75)\n",
    "        if(Q1==Q3):\n",
    "            filtered_A.append(column)\n",
    "        else:\n",
    "            IQR = Q3 - Q1\n",
    "            outlier_step = 15 * IQR\n",
    "            filtered_entries = (column < Q3 + outlier_step)*(column > Q1 - outlier_step)\n",
    "            filtered_column = column[filtered_entries]\n",
    "            filtered_A.append(filtered_column)\n",
    "    \n",
    "    vp = plt.violinplot(filtered_A,showmeans=True,**kwargs)  \n",
    "    for v in vp['bodies']:\n",
    "        v.set_facecolor(col)\n",
    "    vp['cbars'].set_color(col)\n",
    "    vp['cmins'].set_color(col)\n",
    "    vp['cmaxes'].set_color(col)\n",
    "    vp['cmeans'].set_color('black')\n",
    "    if(hatch):\n",
    "        vp['bodies'][0].set_hatch('//')\n",
    "\n",
    "def box_plot(data, edge_color, fill_color, hatch=None, linewidth=1.5, **kwargs):\n",
    "    # Clean data to remove NaNs column-wise\n",
    "    if(np.ndim(data) == 1):\n",
    "        cleaned_data = data[~np.isnan(data)]\n",
    "    else:\n",
    "        cleaned_data = [d[~np.isnan(d)] for d in data]\n",
    "    # Create the box plot with cleaned data\n",
    "    bp = plt.boxplot(cleaned_data, patch_artist=True, **kwargs)\n",
    "    \n",
    "    for element in ['boxes', 'whiskers', 'means', 'medians', 'caps']:\n",
    "        plt.setp(bp[element], color=edge_color, linewidth=linewidth)\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set(facecolor=fill_color, linewidth=linewidth)      \n",
    "        if hatch is not None:\n",
    "            patch.set(hatch=hatch)\n",
    "\n",
    "    return bp\n",
    "\n",
    "def viol_plot2(A,col,hatch=False,**kwargs):\n",
    "    A_T = np.transpose(A)\n",
    "    filtered_A = []\n",
    "    for column in A_T:\n",
    "        # Remove NaNs\n",
    "        column = column[~np.isnan(column)]\n",
    "        # Identify outliers using Z-score\n",
    "        z_scores = stats.zscore(column)\n",
    "        abs_z_scores = np.abs(z_scores)\n",
    "        # Filter data within 3 standard deviations\n",
    "        filtered_entries = (abs_z_scores < 1000)\n",
    "        filtered_column = column[filtered_entries]\n",
    "        filtered_A.append(filtered_column)\n",
    "    \n",
    "    vp = plt.violinplot(filtered_A,showmeans=True,**kwargs)  \n",
    "    for v in vp['bodies']:\n",
    "        v.set_facecolor(col)\n",
    "    vp['cbars'].set_color(col)\n",
    "    vp['cmins'].set_color(col)\n",
    "    vp['cmaxes'].set_color(col)\n",
    "    vp['cmeans'].set_color('black')\n",
    "    if(hatch):\n",
    "        vp['bodies'][0].set_hatch('//')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85666ae8-d1fa-4117-9bbe-f26f7754ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_path = '../Networks/'\n",
    "image_path   = './Images/'\n",
    "if not os.path.exists(image_path):\n",
    "    os.makedirs(image_path)\n",
    "NoiseLevels = [None,20,10,5,2]\n",
    "\n",
    "TrainingSamples = 50000\n",
    "InferSamples    = 500\n",
    "\n",
    "lower_abs,upper_abs = -0.07,0.07\n",
    "lower_rest,upper_rest = -0.015,0.015\n",
    "lower_S0 = 25\n",
    "upper_S0 = 2000\n",
    "Save = True\n",
    "\n",
    "TrueCol  = 'k'\n",
    "NoisyCol = 'k'\n",
    "NLLSFit   = np.array([225,190,106])/255\n",
    "SBIFit   = np.array([64,176,166])/255\n",
    "WLSFit = np.array([140, 100, 200]) / 255  # muted violet\n",
    "MLEFit = np.array([210, 80, 140]) / 255\n",
    "\n",
    "Errors_name = ['MD comparison','FA comparison','eig. comparison','Frobenius','Signal comparison','Correlation','Signal comparison','Correlation2']\n",
    "custom_prior = DTIPriorS0(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0)\n",
    "priorS0, *_ = process_prior(custom_prior) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ad86f-71c9-4fb9-ae2e-edc327c7c686",
   "metadata": {},
   "source": [
    "## DKIFits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58165512-7f46-4df0-b312-eb2d489a7ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 1\n",
    "fdwi = '../HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = '../HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = '../HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = '../HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = '../HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = '../HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "\n",
    "data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "# Get the indices of True values\n",
    "true_indices = np.argwhere(mask)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = true_indices.min(axis=0)\n",
    "max_coords = true_indices.max(axis=0)\n",
    "\n",
    "maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "FlatTD = TestData.reshape(maskdata.shape[0]*maskdata.shape[1],138)\n",
    "FlatTD = FlatTD[FlatTD[:,:69].sum(axis=-1)>0]\n",
    "FlatTD = FlatTD[~np.array(FlatTD<0).any(axis=-1)]\n",
    "\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabExt)\n",
    "tenfit = dkimodel.fit(FlatTD)\n",
    "DKIHCP = tenfit.kt\n",
    "DTIHCP = tenfit.lower_triangular()\n",
    "DKIMin = np.array(DKIHCP)\n",
    "DTIMin = np.array(DTIHCP)\n",
    "\n",
    "\n",
    "DTIFilt1 = DTIMin[(abs(DKIMin)<10).all(axis=1)]\n",
    "DKIFilt1 = DKIMin[(abs(DKIMin)<10).all(axis=1)]\n",
    "DTIFilt = DTIFilt1[(DKIFilt1>-3/7).all(axis=1)]\n",
    "DKIFilt = DKIFilt1[(DKIFilt1>-3/7).all(axis=1)]\n",
    "\n",
    "TrueMets = []\n",
    "FA       = []\n",
    "for (dt,kt) in tqdm(zip(DTIFilt,DKIFilt)):\n",
    "    TrueMets.append(DKIMetrics(dt,kt))\n",
    "    FA.append(FracAni(np.linalg.eigh(vals_to_mat(dt))[0],np.mean(np.linalg.eigh(vals_to_mat(dt))[0])))\n",
    "TrueMets = np.array(TrueMets)\n",
    "TrueFA = np.array(FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbb9c1-966c-4492-a5a9-b0f1169d0409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min fit\n",
    "DT1_Min,DT2_Min = FitDT(DTIFilt,1)\n",
    "x4_Min,R1_Min,x2_Min,R2_Min = FitKT(DKIFilt,1)\n",
    "\n",
    "# LowFA Fit\n",
    "DT1_lfa,DT2_lfa = FitDT(DTIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "x4_lfa,R1_lfa,x2_lfa,R2_lfa = FitKT(DKIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "\n",
    "# HighFA Fit\n",
    "DT1_hfa,DT2_hfa = FitDT(DTIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "x4_hfa,R1_hfa,x2_hfa,R2_hfa = FitKT(DKIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "\n",
    "# UltraLowFA Fit\n",
    "DT1_ulfa,DT2_ulfa = FitDT(DTIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa = FitKT(DKIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "\n",
    "# HigherAK Fit\n",
    "DT1_hak,DT2_hak = FitDT(DTIFilt[TrueMets[:,1]>0.9,:],1)\n",
    "x4_hak,R1_hak,x2_hak,R2_hak = FitKT(DKIFilt[TrueMets[:,1]>0.9,:],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713cacb2-703b-4658-af51-010976107af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invertComputeDTI(A, scale_factor=1e-3):\n",
    "    # 1) ensure perfect symmetry\n",
    "    A = (A + A.T) / 2.0\n",
    "\n",
    "    # 2) Cholesky → L lower‑triangular with positive diagonals\n",
    "    L = np.linalg.cholesky(A)\n",
    "\n",
    "    # 3) recover params:\n",
    "    #    - diagonal params = log(L[ii,ii] / scale_factor)\n",
    "    #    - off‑diagonals = the corresponding L entries\n",
    "    return np.array([\n",
    "        np.log(L[0,0] / scale_factor),  # p0\n",
    "        np.log(L[1,1] / scale_factor),  # p2\n",
    "        np.log(L[2,2] / scale_factor),   # p5\n",
    "        L[1,0],                         # p1\n",
    "        L[2,0],                         # p3\n",
    "        L[2,1]                         # p4\n",
    "\n",
    "    ])\n",
    "def ComputeDTI(params, scale_factor=1e-3):\n",
    "    L = fill_lower_diag(params)\n",
    "    diag_indices = np.diag_indices_from(L)\n",
    "    L[diag_indices] = np.exp(L[diag_indices]) * scale_factor\n",
    "    A = L @ L.T\n",
    "    return A\n",
    "    \n",
    "def rician_nll_DKI(params,S_obs,gtab,S0):\n",
    "    dt        = params[:6]\n",
    "    kt        = params[6:-1]\n",
    "    log_sigma = params[-1]\n",
    "\n",
    "    S_model  = CustomDKISimulator2(dt,kt,gtab,S0)\n",
    "\n",
    "    sigma2 = np.exp(2 * log_sigma)  # ensure positivity\n",
    "    # Avoid log(0) with clipping\n",
    "    S_obs_clipped = np.clip(S_obs, 1e-10, None)\n",
    "    S_model_clipped = np.clip(S_model, 1e-10, 1e10)\n",
    "\n",
    "    bessel_arg = (S_obs_clipped * S_model_clipped) / sigma2\n",
    "\n",
    "    # use the scaled Bessel to avoid overflow:\n",
    "    log_bessel = np.log(i0e(bessel_arg)) + np.abs(bessel_arg)\n",
    "\n",
    "    nll = (\n",
    "        np.log(sigma2)\n",
    "        - np.log(S_obs_clipped)\n",
    "        + (S_obs_clipped**2 + S_model_clipped**2) / (2 * sigma2)\n",
    "        - log_bessel\n",
    "    )\n",
    "    if(np.isnan(nll).any() or np.isinf(nll).any()):\n",
    "        import pdb;pdb.set_trace()\n",
    "    return np.sum(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28e369-4263-40ca-b818-803a5104b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rician_nll_DKI_real(params,S_obs,gtab):\n",
    "    dt        = params[:6]\n",
    "    kt        = params[6:-2]\n",
    "    S0 = np.exp(params[-2])\n",
    "    log_sigma = params[-1]\n",
    "\n",
    "    S_model  = CustomDKISimulator2(dt,kt,gtab,S0)\n",
    "\n",
    "    sigma2 = np.exp(2 * log_sigma)  # ensure positivity\n",
    "    # Avoid log(0) with clipping\n",
    "    S_obs_clipped = np.clip(S_obs, 1e-10, None)\n",
    "    S_model_clipped = np.clip(S_model, 1e-10, 1e10)\n",
    "\n",
    "    bessel_arg = (S_obs_clipped * S_model_clipped) / sigma2\n",
    "\n",
    "    # use the scaled Bessel to avoid overflow:\n",
    "    log_bessel = np.log(i0e(bessel_arg)) + np.abs(bessel_arg)\n",
    "\n",
    "    nll = (\n",
    "        np.log(sigma2)\n",
    "        - np.log(S_obs_clipped)\n",
    "        + (S_obs_clipped**2 + S_model_clipped**2) / (2 * sigma2)\n",
    "        - log_bessel\n",
    "    )\n",
    "\n",
    "    return np.sum(nll)\n",
    "def robust_cholesky(A, tol=1e-12):\n",
    "    \"\"\"\n",
    "    Drop-in replacement for np.linalg.cholesky that tolerates tiny negative eigenvalues.\n",
    "    \n",
    "    Parameters:\n",
    "    - A: symmetric matrix (should be positive semi-definite)\n",
    "    - tol: eigenvalue floor to avoid sqrt of negative numbers\n",
    "    \n",
    "    Returns:\n",
    "    - L: lower-triangular matrix such that A ≈ L @ L.T\n",
    "    \"\"\"\n",
    "    # Symmetry check\n",
    "    if not np.allclose(A, A.T, atol=1e-10):\n",
    "        raise ValueError(\"Matrix must be symmetric.\")\n",
    "\n",
    "    # Eigen-decomposition\n",
    "    eigvals, eigvecs = np.linalg.eigh(A)\n",
    "\n",
    "    # Clip small/negative eigenvalues to ensure stability\n",
    "    eigvals_clipped = np.clip(eigvals, tol, None)\n",
    "    \n",
    "    # Construct the square root of A\n",
    "    A_half = eigvecs @ np.diag(np.sqrt(eigvals_clipped))\n",
    "\n",
    "    # Use QR decomposition on A_half.T to get a lower-triangular L\n",
    "    Q, R = np.linalg.qr(A_half.T)\n",
    "    L = R.T\n",
    "\n",
    "    # Enforce positive diagonals\n",
    "    signs = np.sign(np.diag(L))\n",
    "    signs[signs == 0] = 1  # avoid zero sign\n",
    "    L = L * signs[:, np.newaxis]\n",
    "\n",
    "    # Ensure strictly lower-triangular by zeroing out upper part (optional)\n",
    "    L = np.tril(L)\n",
    "\n",
    "    return L\n",
    "def invertComputeDTI(A, scale_factor=1e-3):\n",
    "    # 1) ensure perfect symmetry\n",
    "    A = (A + A.T) / 2.0\n",
    "\n",
    "    # 2) Cholesky → L lower‑triangular with positive diagonals\n",
    "    L = robust_cholesky(A)\n",
    "\n",
    "    # 3) recover params:\n",
    "    #    - diagonal params = log(L[ii,ii] / scale_factor)\n",
    "    #    - off‑diagonals = the corresponding L entries\n",
    "    return np.array([\n",
    "        np.log(L[0,0] / scale_factor),  # p0\n",
    "        np.log(L[1,1] / scale_factor),  # p2\n",
    "        np.log(L[2,2] / scale_factor),   # p5\n",
    "        L[1,0],                         # p1\n",
    "        L[2,0],                         # p3\n",
    "        L[2,1]                         # p4\n",
    "\n",
    "    ])\n",
    "DatFolder = './SavedDat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e108d429-4fe0-4ee9-a9f3-bc509a10f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLLSFit   = np.array([225,190,106])/255\n",
    "SBIFit   = np.array([64,176,166])/255\n",
    "WLSFit = np.array((192,108,132)) / 255  # muted violet\n",
    "MLEFit = np.array([70,100,150]) / 255\n",
    "BayFit = np.array([140,165,200])/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba527b72-ed43-43ad-a604-a27e59eacde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_local_ssim(img1, img2, mask, win_size=15,dat_range=None):\n",
    "    half_win = win_size // 2\n",
    "    padded_img1 = np.pad(img1, half_win, mode='reflect')\n",
    "    padded_img2 = np.pad(img2, half_win, mode='reflect')\n",
    "    padded_mask = np.pad(mask, half_win, mode='constant', constant_values=0)\n",
    "\n",
    "    ssim_values = []\n",
    "\n",
    "    rows, cols = img1.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            mask_patch = padded_mask[i:i+win_size, j:j+win_size]\n",
    "            if mask_patch.all():  # Only if fully valid\n",
    "                img1_patch = padded_img1[i:i+win_size, j:j+win_size]\n",
    "                img2_patch = padded_img2[i:i+win_size, j:j+win_size]\n",
    "                if(not (np.isnan(img1_patch).any() or np.isnan(img2_patch).any())):\n",
    "                    if(dat_range is None):\n",
    "                        val = ssim(img1_patch, img2_patch,\n",
    "                                   data_range=img1.max() - img1.min(), full=False)\n",
    "                    else:\n",
    "                        val = ssim(img1_patch, img2_patch,\n",
    "                                   data_range=dat_range, full=False)\n",
    "                    ssim_values.append(val)\n",
    "\n",
    "    return np.nanmean(ssim_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b17998-2596-444f-9f39-59a6d0577a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viol_plot(A,col,hatch=False,**kwargs):\n",
    "    A_T = np.transpose(A)\n",
    "    filtered_A = []\n",
    "    for column in A_T:\n",
    "        # Remove NaNs\n",
    "        column = column[~np.isnan(column)]\n",
    "        # Identify outliers using Z-score\n",
    "        z_scores = stats.zscore(column)\n",
    "        abs_z_scores = np.abs(z_scores)\n",
    "        # Filter data within 3 standard deviations\n",
    "        filtered_entries = (abs_z_scores < 1000)\n",
    "        filtered_column = column[filtered_entries]\n",
    "        filtered_A.append(filtered_column)\n",
    "    \n",
    "    vp = plt.violinplot(filtered_A,showmeans=True,**kwargs)  \n",
    "    for v in vp['bodies']:\n",
    "        v.set_facecolor(col)\n",
    "    vp['cbars'].set_color(col)\n",
    "    vp['cmins'].set_color(col)\n",
    "    vp['cmaxes'].set_color(col)\n",
    "    vp['cmeans'].set_color('black')\n",
    "    if(hatch):\n",
    "        vp['bodies'][0].set_hatch('//')\n",
    "def BoxPlots(y_data, positions, colors, colors2, ax,hatch = False,scatter=False,scatter_alpha=0.5, **kwargs):\n",
    "\n",
    "    GREY_DARK = \"#747473\"\n",
    "    jitter = 0.02\n",
    "    # Clean data to remove NaNs column-wise\n",
    "    if(np.ndim(y_data) == 1):\n",
    "        cleaned_data = y_data[~np.isnan(y_data)]\n",
    "    else:\n",
    "        cleaned_data = [d[~np.isnan(d)] for d in y_data]\n",
    "    \n",
    "    # Define properties for the boxes (patch objects)\n",
    "    boxprops = dict(\n",
    "        linewidth=2, \n",
    "        facecolor='none',       # use facecolor for filling (set to 'none' if you want no fill)\n",
    "        edgecolor='turquoise'   # edgecolor for the outline\n",
    "    )\n",
    "\n",
    "    # Define properties for the medians (Line2D objects)\n",
    "    # Ensure GREY_DARK is defined (or replace it with a color string)\n",
    "    medianprops = dict(\n",
    "        linewidth=2, \n",
    "        color=GREY_DARK,\n",
    "        solid_capstyle=\"butt\"\n",
    "    )\n",
    "\n",
    "    # For whiskers, since they are Line2D objects, use 'color'\n",
    "    whiskerprops = dict(\n",
    "        linewidth=2, \n",
    "        color='turquoise'\n",
    "    )\n",
    "\n",
    "    bplot = ax.boxplot(\n",
    "        cleaned_data,\n",
    "        positions=positions, \n",
    "        showfliers=False,\n",
    "        showcaps = False,\n",
    "        medianprops=medianprops,\n",
    "        whiskerprops=whiskerprops,\n",
    "        boxprops=boxprops,\n",
    "        patch_artist=True,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Update the color of each box (these are patch objects)\n",
    "    for i, box in enumerate(bplot['boxes']):\n",
    "        box.set_edgecolor(colors[i])\n",
    "        if(hatch):\n",
    "            box.set_hatch('/')\n",
    "    \n",
    "    \n",
    "    # Update the color of the whiskers (each box has 2 whiskers)\n",
    "    for i in range(len(positions)):\n",
    "        bplot['whiskers'][2*i].set_color(colors[i])\n",
    "        bplot['whiskers'][2*i+1].set_color(colors[i])\n",
    "    \n",
    "    # If caps are enabled, update their color (Line2D objects)\n",
    "    if 'caps' in bplot:\n",
    "        for i, cap in enumerate(bplot['caps']):\n",
    "            cap.set_color(colors[i//2])  # two caps per box\n",
    "\n",
    "    if(scatter):\n",
    "        if(np.ndim(cleaned_data) == 1):\n",
    "            x_data = np.array([positions] * len(cleaned_data))\n",
    "            x_jittered = x_data + stats.t(df=6, scale=jitter).rvs(len(x_data))\n",
    "            ax.scatter(x_data, cleaned_data, s=100, color=colors2, alpha=scatter_alpha)\n",
    "        else:\n",
    "            x_data = [np.array([positions[i]] * len(d)) for i, d in enumerate(cleaned_data)]\n",
    "            x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "            # Plot the scatter points with jitter (using colors2)\n",
    "            for x, y, c in zip(x_jittered, cleaned_data, colors2):\n",
    "                ax.scatter(x, y, s=100, color=c, alpha=scatter_alpha)\n",
    "def BoxPlots2(y_data, positions, colors, colors2, ax,hatch = False):\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "\n",
    "    jitter = 0.02\n",
    "    x_data = [np.array([positions[i]] * len(d)) for i, d in enumerate(y_data)]\n",
    "    x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    # Define properties for the boxes (patch objects)\n",
    "    boxprops = dict(\n",
    "        linewidth=2, \n",
    "        facecolor='none',       # use facecolor for filling (set to 'none' if you want no fill)\n",
    "        edgecolor='turquoise'   # edgecolor for the outline\n",
    "    )\n",
    "\n",
    "    # Define properties for the medians (Line2D objects)\n",
    "    # Ensure GREY_DARK is defined (or replace it with a color string)\n",
    "    medianprops = dict(\n",
    "        linewidth=2, \n",
    "        color='dimgray',  # Replace 'GREY_DARK' with an actual color if needed\n",
    "        solid_capstyle=\"butt\"\n",
    "    )\n",
    "\n",
    "    # For whiskers, since they are Line2D objects, use 'color'\n",
    "    whiskerprops = dict(\n",
    "        linewidth=2, \n",
    "        color='turquoise'\n",
    "    )\n",
    "\n",
    "    bplot = ax.boxplot(\n",
    "        y_data,\n",
    "        positions=positions, \n",
    "        showfliers=False,\n",
    "        showcaps=False,\n",
    "        showmeans=True,\n",
    "        medianprops=medianprops,\n",
    "        whiskerprops=whiskerprops,\n",
    "        boxprops=boxprops,\n",
    "        patch_artist=True\n",
    "    )\n",
    "\n",
    "    # Update the color of each box (these are patch objects)\n",
    "    for i, box in enumerate(bplot['boxes']):\n",
    "        box.set_edgecolor(colors[i])\n",
    "        if(hatch):\n",
    "            box.set_hatch('/')\n",
    "    \n",
    "    # Update the color of the medians (Line2D objects)\n",
    "    for i, median in enumerate(bplot['medians']):\n",
    "        median.set_color(colors[i])\n",
    "    \n",
    "    # Update the color of the whiskers (each box has 2 whiskers)\n",
    "    for i in range(len(positions)):\n",
    "        bplot['whiskers'][2*i].set_color(colors[i])\n",
    "        bplot['whiskers'][2*i+1].set_color(colors[i])\n",
    "    \n",
    "    # If caps are enabled, update their color (Line2D objects)\n",
    "    if 'caps' in bplot:\n",
    "        for i, cap in enumerate(bplot['caps']):\n",
    "            cap.set_color(colors[i//2])  # two caps per box\n",
    "\n",
    "    # Plot the scatter points with jitter (using colors2)\n",
    "    for x, y, c in zip(x_jittered, y_data, colors2):\n",
    "        ax.scatter(x, y, s=100, color=c, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f4eee-37ec-4617-b88a-d26f1f083b31",
   "metadata": {},
   "source": [
    "# In-silico (a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe3e7de-6b71-4fc9-ab47-cb14e25bc4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial = HemiSphere(xyz=bvecs[1:])\n",
    "hsph_updated,_ = disperse_charges(hsph_initial,5000)\n",
    "bvecs = np.vstack([[0,0,0],hsph_updated.vertices])\n",
    "bvalsExt = np.hstack([bvals, 3000*np.ones_like(bvals)])\n",
    "bvecsExt = np.vstack([bvecs, bvecs])\n",
    "bvalsExt[65] = 0\n",
    "gtabSim = gradient_table(bvalsExt, bvecsExt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c4a8c-7c42-40aa-beff-5e6a7b248725",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DKISimFull.pickle\"):\n",
    "    with open(f\"{network_path}/DKISimFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*6000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*2000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*6000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*6000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(2.5*6000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabSim.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabSim,200,np.random.rand()*30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>800).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKISimFull.pickle\"):\n",
    "        with open(f\"{network_path}/DKISimFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba79658-c5dd-4c36-905b-04717d13df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],1,40)\n",
    "DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],1,40)\n",
    "DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],1,40)\n",
    "DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],1,40)\n",
    "DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,40)\n",
    "\n",
    "SampsDT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "SampsKT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "\n",
    "Samples  = []\n",
    "\n",
    "for Sd,Sk in zip(SampsDT,SampsKT):\n",
    "    Samples.append([CustomDKISimulator(Sd,Sk,gtabSim, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "\n",
    "Samples = np.array(Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8813d-edd4-4564-ab59-c6213549b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "se = 14\n",
    "torch.manual_seed(se)\n",
    "np.random.seed(se)\n",
    "j = 1\n",
    "vL = torch.tensor([0.2*j])\n",
    "vS = torch.tensor([0.01*j])  \n",
    "\n",
    "kk = np.random.randint(0,4)\n",
    "DT_guess,KT_guess= GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],se,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cffbde-5070-4dcf-b4ae-d9165ee7ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(params,gtab,y):\n",
    "\n",
    "    dt        = params[:6]\n",
    "    kt        = params[6:]\n",
    "    \n",
    "    Signal = CustomDKISimulator2(dt,kt,gtab,200)\n",
    "    return y - Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2d3d94-4407-48c1-a2d6-982f6837faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invertComputeDTI_exp(A, scale_factor=1e-3):\n",
    "    # 1) ensure perfect symmetry\n",
    "    A = (A + A.T) / 2.0\n",
    "\n",
    "    # 2) Cholesky → L lower‑triangular with positive diagonals\n",
    "    L = np.linalg.cholesky(A)\n",
    "\n",
    "    # 3) recover params:\n",
    "    #    - diagonal params = log(L[ii,ii] / scale_factor)\n",
    "    #    - off‑diagonals = the corresponding L entries\n",
    "    return np.array([\n",
    "        np.log(L[0,0] / scale_factor),  # p0\n",
    "        np.log(L[1,1] / scale_factor),  # p2\n",
    "        np.log(L[2,2] / scale_factor),   # p5\n",
    "        L[1,0],                         # p1\n",
    "        L[2,0],                         # p3\n",
    "        L[2,1]                         # p4\n",
    "\n",
    "    ])\n",
    "def ComputeDTI_exp(params, scale_factor=1e-3):\n",
    "    L = fill_lower_diag(params)\n",
    "    diag_indices = np.diag_indices_from(L)\n",
    "    L[diag_indices] = np.exp(L[diag_indices]) * scale_factor\n",
    "    A = L @ L.T\n",
    "    return A\n",
    "    \n",
    "def rician_nll_DKI(params,S_obs,gtab,S0):\n",
    "    dt        = params[:6]\n",
    "    kt        = params[6:-1]\n",
    "    log_sigma = params[-1]\n",
    "\n",
    "    S_model  = CustomDKISimulator2(dt,kt,gtab,S0)\n",
    "\n",
    "    sigma2 = np.exp(2 * log_sigma)  # ensure positivity\n",
    "    # Avoid log(0) with clipping\n",
    "    S_obs_clipped = np.clip(S_obs, 1e-10, None)\n",
    "    S_model_clipped = np.clip(S_model, 1e-10, 1e10)\n",
    "\n",
    "    bessel_arg = (S_obs_clipped * S_model_clipped) / sigma2\n",
    "\n",
    "    # use the scaled Bessel to avoid overflow:\n",
    "    log_bessel = np.log(i0e(bessel_arg)) + np.abs(bessel_arg)\n",
    "\n",
    "    nll = (\n",
    "        np.log(sigma2)\n",
    "        - np.log(S_obs_clipped)\n",
    "        + (S_obs_clipped**2 + S_model_clipped**2) / (2 * sigma2)\n",
    "        - log_bessel\n",
    "    )\n",
    "    if(np.isnan(nll).any() or np.isinf(nll).any()):\n",
    "        import pdb;pdb.set_trace()\n",
    "    return np.sum(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275525bf-4f5f-4dae-9eab-44fc17c6fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bounds = [-0.0] * 3 + [-0.015]*3 + [0]*3 + [-1]*9 + [-0.25]*3 + [np.log(1e-2)]\n",
    "\n",
    "upper_bounds = [5] * 3 + [0.015]*3 + [5]*3 + [1]*9 + [0.25]*3 +  [np.log(200)]\n",
    "bounds = list(zip(lower_bounds, upper_bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c415a28b-c036-488c-803a-e65e180f14dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.exists(DatFolder+'/Sim_Full_Error_MLE.npy')):\n",
    "    Error_MLE = np.load(DatFolder+'/Sim_Full_Error_MLE.npy')\n",
    "else:\n",
    "    Error_MLE = []\n",
    "    for k in tqdm([1,2,3,4]):\n",
    "        ErrorN2 = []\n",
    "        ENoise = []\n",
    "        for i in tqdm(range(200)):\n",
    "            tObs = Samples[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "            LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI_exp(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze()]), args=(gtabSim, tObs)).x\n",
    "            res = minimize(\n",
    "                    rician_nll_DKI,\n",
    "                    np.hstack([LS_x[:6],LS_x[6:21],np.log(10)]),\n",
    "                    args=(tObs, gtabSim, 200),\n",
    "                    options={'disp':0,\n",
    "                        'gtol': 1e-10,\n",
    "                        'ftol': 1e-10,},\n",
    "                        bounds=bounds\n",
    "                )\n",
    "            \n",
    "            ErrorN2.append(DKIErrors(mat_to_vals(ComputeDTI_exp(res.x[:6])),res.x[6:-1],SampsDT[i],SampsKT[i]))\n",
    "        Error_MLE.append(ErrorN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e7109-6ac0-4448-b3d5-fe45fc43ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "ErrorFull = []\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples[i,k,:]\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessSBI = posterior_samples_1.mean(axis=0)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(GuessSBI[:6],GuessSBI[6:],SampsDT[i],SampsKT[i]))\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "Error_WLS = []\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabSim,fit_method='WLS')\n",
    "\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(tenfit.lower_triangular(),tenfit.kt,SampsDT[i],SampsKT[i]))\n",
    "    Error_WLS.append(ErrorN2)\n",
    "\n",
    "Error_NLLS = []\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabSim,fit_method='NLLS')\n",
    "\n",
    "for k in tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(tenfit.lower_triangular(),tenfit.kt,SampsDT[i],SampsKT[i]))\n",
    "    Error_NLLS.append(ErrorN2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955e8b0-9bbb-4b79-a453-b09aa715fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors_SD =  [1/0.001633,    1/.5e-5,    1/0.001633,    1/.5e-5,   1/ 7.5e-5, 1/0.001633] + [1e2]*15 + [1e2]\n",
    "mu0 = np.zeros(22)\n",
    "\n",
    "V0  = np.diag(priors_SD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826dc97d-41ff-49e3-9434-a3c4c8e8c101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 0.  Your forward model -------------------------------------\n",
    "def f(theta, gtab):\n",
    "    \"\"\"DKI signal model for *one* voxel, given params theta.\"\"\"\n",
    "    dt        = theta[:6]\n",
    "    kt        = theta[6:-1]\n",
    "    S0        = np.exp(theta[-1])          # log-S0 in parameters\n",
    "    return CustomDKISimulator2(dt, kt, gtab, S0)   # (Ngrad,)\n",
    "\n",
    "# Finite-difference Jacobian (6 dt + K kt + 1 S0 parameters)\n",
    "def jacobian(theta, gtab, eps=1e-5):\n",
    "    J = np.zeros((len(gtab.bvals), len(theta)))\n",
    "    f0 = f(theta, gtab)\n",
    "    for i in range(len(theta)):\n",
    "        t_eps        = theta.copy()\n",
    "        t_eps[i]    += eps\n",
    "        J[:, i]      = (f(t_eps, gtab) - f0) / eps\n",
    "    return J, f0\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def vb_gauss_one_voxel(y, gtab, theta0,\n",
    "                       mu0=None, V0=None,  # Gaussian prior\n",
    "                       a0=1e-3, b0=1e-3,   # Gamma prior\n",
    "                       max_iter=100, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Mean-field VB (Gaussian q(theta), Gamma q(1/σ²))\n",
    "    for one voxel with Gaussian noise.\n",
    "    \"\"\"\n",
    "    D = len(theta0)                       # #parameters\n",
    "    if mu0 is None:\n",
    "        mu0 = np.zeros(D)\n",
    "    if V0 is None:\n",
    "        V0 = np.eye(D) * 1e2              # very vague prior\n",
    "    V0_inv = inv(V0)\n",
    "\n",
    "    m  = theta0.copy()                    # variational mean\n",
    "    S  = V0.copy()                        # variational cov\n",
    "    a  = a0 + len(y)/2.\n",
    "    b  = b0 + 1.0                         # will be updated\n",
    "    for it in range(max_iter):\n",
    "        # --- E[precision] ----------------\n",
    "        lam = a / b                       # <1/σ²>\n",
    "\n",
    "        # --- linearise model around current mean\n",
    "        J, f_m = jacobian(m, gtab)\n",
    "        r      = y - f_m                  # residual\n",
    "\n",
    "        # --- Gauss-Newton-style VB updates\n",
    "        S_new  = inv(V0_inv + lam * J.T @ J)\n",
    "        m_new  = m + S_new @ (V0_inv @ (mu0 - m) + lam * J.T @ r)\n",
    "\n",
    "        # --- update Gamma factors\n",
    "        quad   = (r @ r\n",
    "                  + np.trace(J @ S_new @ J.T))\n",
    "        b_new  = b0 + 0.5 * quad\n",
    "\n",
    "        # --- convergence check\n",
    "        if np.linalg.norm(m_new - m) < tol:\n",
    "            m, S, b = m_new, S_new, b_new\n",
    "            break\n",
    "\n",
    "        m, S, b = m_new, S_new, b_new\n",
    "\n",
    "    return m, S, a/b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68edc299-8975-4df6-b7f5-d547070b9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Error_Bay = []\n",
    "for k in tqdm.tqdm([1,2,3,4]):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in tqdm.tqdm(range(200)):\n",
    "        tObs = Samples[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI_exp(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze()]), args=(gtabSim, tObs)).x\n",
    "        try:\n",
    "            m, S, prec = vb_gauss_one_voxel(tObs, gtabSim, np.append(LS_x,np.log(200)),mu0=mu0, V0=V0)\n",
    "            ErrorN2.append(DKIErrors(mat_to_vals(ComputeDTI_exp(m[:6])),m[6:-1],SampsDT[i],SampsKT[i]))\n",
    "        except:\n",
    "            ErrorN2.append(math.nan*np.zeros(5))\n",
    "        \n",
    "    Error_Bay.append(ErrorN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3070a-f505-4656-bfa7-2afaaa89ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorFull = np.array(ErrorFull)\n",
    "Error_NLLS = np.array(Error_NLLS)\n",
    "Error_WLS = np.array(Error_WLS)\n",
    "Error_MLE = np.array(Error_MLE)\n",
    "Error_Bay = np.array(Error_Bay)\n",
    "ErrorNames = ['MK Error', 'AK Error', 'RK Error', 'MKT Error', 'KFA Error']\n",
    "fig,ax = plt.subplots(1,5,figsize=(25.5,3))\n",
    "for i in range(5):\n",
    "    plt.sca(ax[i])\n",
    "    box_plot(Error_WLS[1:,:,i],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,showmeans=False,widths=0.3,positions=[1,2.9,4.8,6.7])\n",
    "    box_plot(Error_NLLS[1:,:,i],NLLSFit-0.2, np.clip(NLLSFit+0.2,0,1),showfliers=False,widths=0.3,positions=[1.3,3.2,5.1,7])\n",
    "    box_plot(Error_MLE[:,:,i],MLEFit-0.2, np.clip(MLEFit+0.2,0,1),showfliers=False,widths=0.3,positions=[1.6,3.5,5.4,7.3])\n",
    "    box_plot(Error_Bay[:,:,i],BayFit-0.2, np.clip(BayFit+0.2,0,1),showfliers=False,widths=0.3,positions=[1.9,3.8,5.7,7.6])\n",
    "    box_plot(ErrorFull[1:,:,i],SBIFit-0.2, np.clip(SBIFit+0.2,0,1),showfliers=False,widths=0.3,positions=[2.2,4.1,6,7.9])\n",
    "\n",
    "    plt.xticks([1.6, 3.5, 5.4, 7.3,],[20,10,5,2],fontsize=32)\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    plt.yticks(fontsize=32)\n",
    "\n",
    "    if(i==4):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(i==0):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=WLSFit, lw=4, label='WLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(i==1):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=NLLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(i==2):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=MLEFit, lw=4, label='MLE'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(i==3):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=BayFit, lw=4, label='Bayes'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "if Save: plt.savefig(image_path+'ErrorsFull2.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7953736-515a-49e3-a5ee-f8b5908cf213",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial15 = HemiSphere(xyz=bvecs[1:16])\n",
    "hsph_initial7 = HemiSphere(xyz=bvecs[1:7])\n",
    "hsph_updated15,_ = disperse_charges(hsph_initial15,5000)\n",
    "hsph_updated7,_ = disperse_charges(hsph_initial7,5000)\n",
    "gtabSimSub = gradient_table(np.array([0]+[1000]*6+[3000]*15).squeeze(), np.vstack([[0,0,0],hsph_updated7.vertices,hsph_updated15.vertices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ceb92-3e12-41c0-8325-29c0cfa45f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DKISimMin.pickle\"):\n",
    "    with open(f\"{network_path}/DKISimMin.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*6000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*2000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*6000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*6000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(2.5*6000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabSimSub.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabSimSub,200,np.random.rand()*30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>800).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKISimMin.pickle\"):\n",
    "        with open(f\"{network_path}/DKISimMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5b6c4-7477-46a1-ad9f-5e3638692978",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "Samples7  = []\n",
    "\n",
    "for Sd,Sk in zip(SampsDT,SampsKT):\n",
    "    Samples7.append([CustomDKISimulator(Sd,Sk,gtabSimSub, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "\n",
    "Samples7 = np.array(Samples7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a50c64-4318-4ef1-ab51-b6e7688baf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Error_NLLS = []\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples7[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(tenfit.lower_triangular(),tenfit.kt,SampsDT[i],SampsKT[i]))\n",
    "    Error_NLLS.append(ErrorN2)\n",
    "Error_WLS = []\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='WLS')\n",
    "\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples7[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(tenfit.lower_triangular(),tenfit.kt,SampsDT[i],SampsKT[i]))\n",
    "    Error_WLS.append(ErrorN2)\n",
    "\n",
    "torch.manual_seed(10)\n",
    "ErrorFull = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples7[i,k,:]\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessSBI = posterior_samples_1.mean(axis=0)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(GuessSBI[:6],GuessSBI[6:],SampsDT[i],SampsKT[i]))\n",
    "    ErrorFull.append(ErrorN2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf68469-a0fa-4b84-8de2-6223606f5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.exists('./data/Sim_Min_Error_MLE.npy')):\n",
    "    Error_MLE = np.load('./data/Sim_Min_Error_MLE.npy')\n",
    "else:\n",
    "    Error_MLE = []\n",
    "    for k in tqdm.tqdm([1,2,3,4]):\n",
    "        ErrorN2 = []\n",
    "        ENoise = []\n",
    "        for i in tqdm.tqdm(range(200)):\n",
    "            tObs = Samples[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "            LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI_exp(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze()]), args=(gtabSim, tObs)).x\n",
    "            res = minimize(\n",
    "                    rician_nll_DKI,\n",
    "                    np.hstack([LS_x[:6],LS_x[6:21],np.log(10)]),\n",
    "                    args=(tObs, gtabSim, 200),\n",
    "                    options={'disp':0,\n",
    "                        'gtol': 1e-10,\n",
    "                        'ftol': 1e-10,},\n",
    "                        bounds=bounds\n",
    "                )\n",
    "            \n",
    "            ErrorN2.append(DKIErrors(mat_to_vals(ComputeDTI_exp(res.x[:6])),res.x[6:-1],SampsDT[i],SampsKT[i]))\n",
    "        Error_MLE.append(ErrorN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba32d8-4073-4b51-be77-93895a5d6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Error_Bay_Min = []\n",
    "\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in tqdm.tqdm(range(200)):\n",
    "        tObs = Samples7[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI_exp(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze()]), args=(gtabSimSub, tObs)).x\n",
    "        try:\n",
    "            m, S, prec = vb_gauss_one_voxel(tObs, gtabSimSub, np.append(LS_x,np.log(200)),mu0=mu0, V0=V0)\n",
    "            ErrorN2.append(DKIErrors(mat_to_vals(ComputeDTI_exp(m[:6])),m[6:-1],SampsDT[i],SampsKT[i]))\n",
    "        except:\n",
    "            ErrorN2.append(math.nan*np.zeros(5))\n",
    "    Error_Bay_Min.append(ErrorN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad99a041-d7cf-42b2-9b2b-1c8864fc1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Error_MLE_Min2 = np.copy(Error_MLE)\n",
    "Error_MLE_Min2[Error_MLE_Min2>100] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6c03e-9ea7-41a5-b70f-72bf6e806562",
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorFull = np.array(ErrorFull)\n",
    "Error_NLLS = np.array(Error_NLLS)\n",
    "Error_WLS = np.array(Error_WLS)\n",
    "Error_MLE_Min2 = np.array(Error_MLE_Min2)\n",
    "Error_Bay_Min = np.array(Error_Bay_Min)\n",
    "ErrorNames = ['MK Error', 'AK Error', 'RK Error', 'MKT Error', 'KFA Error']\n",
    "fig,ax = plt.subplots(1,5,figsize=(25.5,3))\n",
    "for i in range(5):\n",
    "    plt.sca(ax[i])\n",
    "    box_plot(Error_WLS[1:,:,i],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,showmeans=False,widths=0.3,positions=[1,2.9,4.8,6.7])\n",
    "    box_plot(Error_NLLS[1:,:,i],NLLSFit-0.2, np.clip(NLLSFit+0.2,0,1),showfliers=False,widths=0.3,positions=[1.3,3.2,5.1,7])\n",
    "    box_plot(Error_MLE_Min2[1:,:,i],MLEFit-0.2, np.clip(MLEFit+0.2,0,1),showfliers=False,widths=0.3,positions=[1.6,3.5,5.4,7.3])\n",
    "    box_plot(Error_Bay_Min[1:,:,i],BayFit-0.2, np.clip(BayFit+0.2,0,1),showfliers=False,widths=0.3,positions=[1.9,3.8,5.7,7.6])\n",
    "    box_plot(ErrorFull[1:,:,i],SBIFit-0.2, np.clip(SBIFit+0.2,0,1),showfliers=False,widths=0.3,positions=[2.2,4.1,6,7.9])\n",
    "\n",
    "    plt.xticks([1.6, 3.5, 5.4, 7.3,],[20,10,5,2],fontsize=32)\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    plt.yticks(fontsize=32)\n",
    "\n",
    "    if(i==4):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(i==0):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=WLSFit, lw=4, label='WLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(i==1):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=NLLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(i==2):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=MLEFit, lw=4, label='MLE'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(i==3):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=BayFit, lw=4, label='Bayes'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "if Save: plt.savefig(image_path+'ErrorsMin2.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27b5726-3bd1-4292-a48e-cb3aefc38da8",
   "metadata": {},
   "source": [
    "# In-vivo (c-f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e3960-f1fe-4fd7-89cf-ccfb83073c91",
   "metadata": {},
   "source": [
    "### MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb64437e-8910-4026-a713-8fdeff467aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "_, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "\n",
    "\n",
    "data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "# Get the indices of True values\n",
    "true_indices = np.argwhere(mask)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = true_indices.min(axis=0)\n",
    "max_coords = true_indices.max(axis=0)\n",
    "\n",
    "maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "axial_middle = maskdata.shape[2] // 2\n",
    "maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices7 = [0]+selected_indices\n",
    "\n",
    "bvalsHCP7_1 = bvalsHCP[selected_indices7]\n",
    "bvecsHCP7_1 = bvecsHCP[selected_indices7]\n",
    "\n",
    "i=3\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(14):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx = []\n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "true_indx = selected_indices7+[t+69 for t in true_indx]\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "cutout = np.sum(TestData4D[:,:,axial_middle,:69], axis=-1) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d9260-e953-4313-9e0f-7e87495b2980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(params,gtab,y):\n",
    "\n",
    "    dt        = params[:6]\n",
    "    kt        = params[6:-1]\n",
    "    log_S0 = params[-1]\n",
    "    \n",
    "    Signal = CustomDKISimulator2(dt,kt,gtab,np.exp(log_S0))\n",
    "    return y - Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b96a88e-40b7-4433-acf2-be18699990b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "se = 14\n",
    "torch.manual_seed(se)\n",
    "np.random.seed(se)\n",
    "j = 1\n",
    "vL = torch.tensor([0.2*j])\n",
    "vS = torch.tensor([0.01*j])  \n",
    "\n",
    "kk = np.random.randint(0,4)\n",
    "if(kk==0):\n",
    "    DT_guess,KT_guess= GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],se,1)\n",
    "elif(kk==1):\n",
    "    DT_guess,KT_guess = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],se,1)\n",
    "elif(kk==2):\n",
    "    DT_guess,KT_guess = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],se,1)\n",
    "elif(kk==3):\n",
    "    DT_guess,KT_guess = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],se,1)\n",
    "\n",
    "lower_bounds = [-0.0] * 3 + [-0.015]*3 + [0]*3 + [-1]*9 + [-0.25]*3 + [np.log(2)]\n",
    "\n",
    "upper_bounds = [5] * 3 + [0.015]*3 + [5]*3 + [1]*9 + [0.25]*3 + [np.log(20)]\n",
    "bounds = list(zip(lower_bounds, upper_bounds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dfcb09-74cc-4b6e-a7e8-a3c0ead7aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArrShape = TestData[:,:,0].shape\n",
    "NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "torch.manual_seed(10)\n",
    "for i in tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            tObs = TestData[i,j,:]\n",
    "            LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze(),np.log(tObs[gtabExt.bvals==0].mean())]), args=(gtabExt, tObs)).x\n",
    "            res = minimize(\n",
    "                    rician_nll_DKI,\n",
    "                    np.hstack([LS_x[:6],LS_x[6:21],np.log(10)]),\n",
    "                    args=(tObs, gtabExt, np.exp(LS_x[-1])),\n",
    "                    options={'disp':0,'maxfun': 100000,   # ← raise the cap on f+g evaluations\n",
    "                        'maxiter': 20000,   # ← (optional) raise the cap on iterations   # ← raise the cap on f+g evaluations   # ← raise the cap on f+g evaluations\n",
    "                        'gtol': 1e-6,\n",
    "                        'ftol': 1e-6,},\n",
    "                        bounds=bounds\n",
    "                )\n",
    "            NoiseEst[i,j] = res.x\n",
    "NoiseEstInv = np.zeros_like(NoiseEst)\n",
    "for i in range(ArrShape[0]):\n",
    "    for j in range(ArrShape[1]):    \n",
    "        NoiseEstInv[i,j] = np.hstack([mat_to_vals(ComputeDTI(NoiseEst[i,j,:6])),NoiseEst[i,j,6:21],NoiseEst[i,j,-1]])\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(ArrShape[0]):\n",
    "    for j in range(ArrShape[1]):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEstInv[i,j]))),NoiseEstInv[i,j,6:]])\n",
    "MK_MLEFull  = np.zeros(ArrShape)\n",
    "AK_MLEFull  = np.zeros(ArrShape)\n",
    "RK_MLEFull  = np.zeros(ArrShape)\n",
    "for i in range(ArrShape[0]):\n",
    "    for j in range(ArrShape[1]): \n",
    "        Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "        MK_MLEFull[i,j] = Metrics[0]\n",
    "        AK_MLEFull[i,j] = Metrics[1]\n",
    "        RK_MLEFull[i,j] = Metrics[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab571a0-998c-4324-9653-78d788f0d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_MLEFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'MKNLFull_MLE.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_MLEFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'AKNFull_MLE.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_MLEFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'RKNLFull_MLE.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25775599-a056-41f0-b9a1-fbea7a36f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "NoiseEst = np.zeros([62, 68 ,22])\n",
    "torch.manual_seed(10)\n",
    "for i in tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            tObs = TestData4D[i,j,axial_middle,true_indx]\n",
    "            LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze(),np.log(100)]), args=(gtabHCP7, tObs)).x\n",
    "            res = minimize(\n",
    "                    rician_nll_DKI,\n",
    "                    np.hstack([LS_x[:6],LS_x[6:21],np.log(10)]),\n",
    "                    args=(tObs, gtabHCP7, np.exp(LS_x[-1])),\n",
    "                    options={'disp':0},\n",
    "                        bounds=bounds\n",
    "                )\n",
    "            NoiseEst[i,j] = res.x\n",
    "NoiseEstInv = np.zeros_like(NoiseEst)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEstInv[i,j] = np.hstack([mat_to_vals(ComputeDTI(NoiseEst[i,j,:6])),NoiseEst[i,j,6:21],NoiseEst[i,j,-1]])\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(ComputeDTI(NoiseEst[i,j,:6])),NoiseEst[i,j,6:]])\n",
    "MK_SBIMin  = np.zeros([62, 68])\n",
    "AK_SBIMin  = np.zeros([62, 68])\n",
    "RK_SBIMin  = np.zeros([62, 68])\n",
    "MKT_SBIMin = np.zeros([62, 68])\n",
    "KFA_SBIMin = np.zeros([62, 68])\n",
    "for i in tqdm(range(62)):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "        MK_SBIMin[i,j] = Metrics[0]\n",
    "        AK_SBIMin[i,j] = Metrics[1]\n",
    "        RK_SBIMin[i,j] = Metrics[2]\n",
    "        MKT_SBIMin[i,j] = Metrics[3]\n",
    "        KFA_SBIMin[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0016a88-2716-47d8-a0c3-99a95ecaeb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_MLEMin*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'MKNLMin_MLE.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_MLEMin*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'AKNMin_MLE.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_MLEMin*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'RKNLMin_MLE.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86a4825-fb30-455a-90a8-221b8ca434ba",
   "metadata": {},
   "source": [
    "### Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1f3cc-7340-4ee5-8b93-959c7d53e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0.  Your forward model -------------------------------------\n",
    "def f(theta, gtab):\n",
    "    \"\"\"DKI signal model for *one* voxel, given params theta.\"\"\"\n",
    "    dt        = theta[:6]\n",
    "    kt        = theta[6:-1]\n",
    "    S0        = np.exp(theta[-1])          # log-S0 in parameters\n",
    "    return CustomDKISimulator2(dt, kt, gtab, S0)   # (Ngrad,)\n",
    "\n",
    "# Finite-difference Jacobian (6 dt + K kt + 1 S0 parameters)\n",
    "def jacobian(theta, gtab, eps=1e-5):\n",
    "    J = np.zeros((len(gtab.bvals), len(theta)))\n",
    "    f0 = f(theta, gtab)\n",
    "    for i in range(len(theta)):\n",
    "        t_eps        = theta.copy()\n",
    "        t_eps[i]    += eps\n",
    "        J[:, i]      = (f(t_eps, gtab) - f0) / eps\n",
    "    return J, f0\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def vb_gauss_one_voxel(y, gtab, theta0,\n",
    "                       mu0=None, V0=None,  # Gaussian prior\n",
    "                       a0=1e-3, b0=1e-3,   # Gamma prior\n",
    "                       max_iter=100, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Mean-field VB (Gaussian q(theta), Gamma q(1/σ²))\n",
    "    for one voxel with Gaussian noise.\n",
    "    \"\"\"\n",
    "    D = len(theta0)                       # #parameters\n",
    "    if mu0 is None:\n",
    "        mu0 = np.zeros(D)\n",
    "    if V0 is None:\n",
    "        V0 = np.eye(D) * 1e2              # very vague prior\n",
    "    V0_inv = inv(V0)\n",
    "\n",
    "    m  = theta0.copy()                    # variational mean\n",
    "    S  = V0.copy()                        # variational cov\n",
    "    a  = a0 + len(y)/2.\n",
    "    b  = b0 + 1.0                         # will be updated\n",
    "    for it in range(max_iter):\n",
    "        # --- E[precision] ----------------\n",
    "        lam = a / b                       # <1/σ²>\n",
    "\n",
    "        # --- linearise model around current mean\n",
    "        J, f_m = jacobian(m, gtab)\n",
    "        r      = y - f_m                  # residual\n",
    "\n",
    "        # --- Gauss-Newton-style VB updates\n",
    "        S_new  = inv(V0_inv + lam * J.T @ J)\n",
    "        m_new  = m + S_new @ (V0_inv @ (mu0 - m) + lam * J.T @ r)\n",
    "\n",
    "        # --- update Gamma factors\n",
    "        quad   = (r @ r\n",
    "                  + np.trace(J @ S_new @ J.T))\n",
    "        b_new  = b0 + 0.5 * quad\n",
    "\n",
    "        # --- convergence check\n",
    "        if np.linalg.norm(m_new - m) < tol:\n",
    "            m, S, b = m_new, S_new, b_new\n",
    "            break\n",
    "\n",
    "        m, S, b = m_new, S_new, b_new\n",
    "\n",
    "    return m, S, a/b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3272b269-93ca-47e9-9036-e56028ef22ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors_SD =  [1/0.001633,    1/.5e-5,    1/0.001633,    1/.5e-5,   1/ 7.5e-5, 1/0.001633] + [1e2]*15 + [1e2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d39059-fc6c-4629-a59d-2899ae49c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0 = np.zeros(22)\n",
    "\n",
    "V0  = np.diag(priors_SD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae952626-516a-4670-9cfd-5906b13dc79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DatFolder+'BayesG_Full_One.npy'):\n",
    "    NoiseEst = np.load(DatFolder+'BayesG_Full_One.npy')\n",
    "else:\n",
    "    ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "    NoiseEst = np.zeros([62, 68 ,22])\n",
    "    torch.manual_seed(10)\n",
    "    for i in tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                tObs = TestData4D[i,j,axial_middle,:]\n",
    "                LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze(),np.log(tObs[gtabExt.bvals==0].mean())]), args=(gtabExt, tObs)).x\n",
    "                try:\n",
    "                    m, S, prec = vb_gauss_one_voxel(tObs, gtabExt, LS_x,mu0=mu0, V0=V0)\n",
    "                except:\n",
    "                    m = 0\n",
    "                NoiseEst[i,j] = m\n",
    "NoiseEstInv = np.zeros_like(NoiseEst)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEstInv[i,j] = np.hstack([mat_to_vals(ComputeDTI(NoiseEst[i,j,:6])),NoiseEst[i,j,6:21],NoiseEst[i,j,-1]])\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEstInv[i,j]))),NoiseEstInv[i,j,6:]])\n",
    "MK_BayFull  = np.zeros([62, 68])\n",
    "AK_BayFull  = np.zeros([62, 68])\n",
    "RK_BayFull  = np.zeros([62, 68])\n",
    "MKT_BayFull = np.zeros([62, 68])\n",
    "KFA_BayFull = np.zeros([62, 68])\n",
    "for i in tqdm(range(62)):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "        MK_BayFull[i,j] = Metrics[0]\n",
    "        AK_BayFull[i,j] = Metrics[1]\n",
    "        RK_BayFull[i,j] = Metrics[2]\n",
    "        MKT_BayFull[i,j] = Metrics[3]\n",
    "        KFA_BayFull[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695735c3-3ec9-453a-8ece-0caaa00b9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_BayFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'MKNLFull_Bay.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_BayFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'AKNLFull_Bay.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_BayFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'RKNLFull_Bay.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3642ffd7-f4dc-496c-8f57-8bcc1302c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DatFolder+'BayesG_Min_One.npy'):\n",
    "    NoiseEst = np.load(DatFolder+'BayesG_Min_One.npy')\n",
    "else:\n",
    "    ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "    NoiseEst = np.zeros([62, 68 ,22])\n",
    "    torch.manual_seed(10)\n",
    "    for i in tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                tObs = TestData4D[i,j,axial_middle,true_indx]\n",
    "                LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze(),np.log(100)]), args=(gtabHCP7, tObs)).x\n",
    "                try:\n",
    "                    m, S, prec = vb_gauss_one_voxel(tObs, gtabHCP7, LS_x,mu0=mu0, V0=V0)\n",
    "                except:\n",
    "                    m = 0\n",
    "                NoiseEst[i,j] = m\n",
    "NoiseEstInv = np.zeros_like(NoiseEst)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEstInv[i,j] = np.hstack([mat_to_vals(ComputeDTI(NoiseEst[i,j,:6])),NoiseEst[i,j,6:21],NoiseEst[i,j,-1]])\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEstInv[i,j]))),NoiseEstInv[i,j,6:]])\n",
    "MK_BayMin  = np.zeros([62, 68])\n",
    "AK_BayMin  = np.zeros([62, 68])\n",
    "RK_BayMin  = np.zeros([62, 68])\n",
    "MKT_BayMin = np.zeros([62, 68])\n",
    "KFA_BayMin = np.zeros([62, 68])\n",
    "for i in tqdm(range(62)):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "        MK_BayMin[i,j] = Metrics[0]\n",
    "        AK_BayMin[i,j] = Metrics[1]\n",
    "        RK_BayMin[i,j] = Metrics[2]\n",
    "        MKT_BayMin[i,j] = Metrics[3]\n",
    "        KFA_BayMin[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35cc03c-6279-4e57-95e6-b7ac674845dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_BayMin*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'MKNLMin_Bay.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_BayMin*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'AKNLMin_Bay.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_BayMin*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'RKNLMin_Bay.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4608f-6faa-466e-9a4e-2b9c66237948",
   "metadata": {},
   "source": [
    "### WLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497b960-cb26-4729-a933-5cf12892db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodelNL = dki.DiffusionKurtosisModel(gtabExt,fit_method='WLS')\n",
    "dkifitNL = dkimodelNL.fit(TestData[:,:,:])\n",
    "MK_NLFull  = np.zeros([62, 68])\n",
    "AK_NLFull  = np.zeros([62, 68])\n",
    "RK_NLFull  = np.zeros([62, 68])\n",
    "MKT_NLFull = np.zeros([62, 68])\n",
    "KFA_NLFull = np.zeros([62, 68])\n",
    "for i in range(62):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt)\n",
    "        MK_NLFull[i,j] = Metrics[0]\n",
    "        AK_NLFull[i,j] = Metrics[1]\n",
    "        RK_NLFull[i,j] = Metrics[2]\n",
    "        MKT_NLFull[i,j] = Metrics[3]\n",
    "        KFA_NLFull[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8606c-8710-4c8e-bd4f-129e7eacff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'MKNLFull_WLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'AKNLFull_WLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'RKNLFull_WLS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56516a2-d95f-44a3-b206-5b305b940404",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodelNL = dki.DiffusionKurtosisModel(gtabHCP7,fit_method='WLS')\n",
    "dkifitNL = dkimodelNL.fit(TestData[:,:,true_indx])\n",
    "MK_NLMin  = np.zeros([62, 68])\n",
    "AK_NLMin  = np.zeros([62, 68])\n",
    "RK_NLMin  = np.zeros([62, 68])\n",
    "MKT_NLMin = np.zeros([62, 68])\n",
    "KFA_NLMin = np.zeros([62, 68])\n",
    "for i in range(62):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt)\n",
    "        MK_NLMin[i,j] = Metrics[0]\n",
    "        AK_NLMin[i,j] = Metrics[1]\n",
    "        RK_NLMin[i,j] = Metrics[2]\n",
    "        MKT_NLMin[i,j] = Metrics[3]\n",
    "        KFA_NLMin[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6125e3-8b20-47ab-a5ce-54be845f29c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_NLMin*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'MKNLMin_WLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_NLMin*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'AKNLMin_WLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_NLMin*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(image_path+'RKNLMin_WLS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e408e-c446-4244-8965-070143da5c81",
   "metadata": {},
   "source": [
    "## Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ec295-f992-4a91-b76e-15a06d32b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TD = []\n",
    "axial_middles = []\n",
    "masks = []\n",
    "WMs = []\n",
    "for kk in tqdm(range(32)):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk+1)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk+1)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk+1)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middles.append(axial_middle)\n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    TD.append(TestData4D)\n",
    "    masks.append(mask[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,axial_middle])\n",
    "    WM, affine, img = load_nifti('./HCP_data/WM_Masks/c2Pat'+str(kk+1)+'_FP.nii', return_img=True)\n",
    "    WMs.append(np.fliplr(WM[:,:,axial_middle]>0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14a3f1-f6e2-41e3-aacb-81a25383d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices7 = [0]+selected_indices\n",
    "\n",
    "bvalsHCP7_1 = bvalsHCP[selected_indices7]\n",
    "bvecsHCP7_1 = bvecsHCP[selected_indices7]\n",
    "\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(14):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx = []\n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "selected_indices7 = selected_indices7+[t+69 for t in true_indx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85aab66-0526-4737-bd41-0c5b32ce7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gTabsF = []\n",
    "gTabs7 = []\n",
    "\n",
    "MinDat   = []\n",
    "\n",
    "for i in tqdm(range(1,33)):\n",
    "    fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    gTabsF.append(gtabExt)\n",
    "    \n",
    "    bvalsHCP7 = gtabExt.bvals[selected_indices7]\n",
    "    bvecsHCP7 = gtabExt.bvecs[selected_indices7]\n",
    "    gtabHCP7 = gradient_table(bvalsHCP7, bvecsHCP7)\n",
    "    gTabs7.append(gtabHCP7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a185f9c5-e6cc-47cc-81c6-9cdfc01a492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullWLArr = []\n",
    "RKFullWLArr = []\n",
    "AKFullWLArr = []\n",
    "MKTFullWLArr = []\n",
    "KFAFullWLArr = []\n",
    "for kk in tqdm(range(32)):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabsF[kk],fit_method='WLS')\n",
    "    dkifitNL = dkimodelNL.fit(TD[kk][:,:,axial_middles[kk],:])\n",
    "    ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7[i,j] = Metrics[0]\n",
    "            AK_NL7[i,j] = Metrics[1]\n",
    "            RK_NL7[i,j] = Metrics[2]\n",
    "            MKT_NL7[i,j] = Metrics[3]\n",
    "            KFA_NL7[i,j] = Metrics[4]\n",
    "    MKFullWLArr.append(MK_NL7)\n",
    "    RKFullWLArr.append(RK_NL7)\n",
    "    AKFullWLArr.append(AK_NL7)\n",
    "    MKTFullWLArr.append(MKT_NL7)\n",
    "    KFAFullWLArr.append(KFA_NL7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5d7b9-108b-4dd6-863c-5ee18aab26f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKMinWLArr = []\n",
    "RKMinWLArr = []\n",
    "AKMinWLArr = []\n",
    "MKTMinWLArr = []\n",
    "KFAMinWLArr = []\n",
    "for kk in tqdm(range(32)):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabs7[kk],fit_method='WLS')\n",
    "    dkifitNL = dkimodelNL.fit(TD[kk][:,:,axial_middles[kk],selected_indices7])\n",
    "    ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7[i,j] = Metrics[0]\n",
    "            AK_NL7[i,j] = Metrics[1]\n",
    "            RK_NL7[i,j] = Metrics[2]\n",
    "            MKT_NL7[i,j] = Metrics[3]\n",
    "            KFA_NL7[i,j] = Metrics[4]\n",
    "    MKMinWLArr.append(MK_NL7)\n",
    "    RKMinWLArr.append(RK_NL7)\n",
    "    AKMinWLArr.append(AK_NL7)\n",
    "    MKTMinWLArr.append(MKT_NL7)\n",
    "    KFAMinWLArr.append(KFA_NL7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed592b46-6315-422f-a410-11578ee18b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullNLArr = []\n",
    "RKFullNLArr = []\n",
    "AKFullNLArr = []\n",
    "MKTFullNLArr = []\n",
    "KFAFullNLArr = []\n",
    "for kk in tqdm(range(32)):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabsF[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(TD[kk][:,:,axial_middles[kk],:])\n",
    "    ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7[i,j] = Metrics[0]\n",
    "            AK_NL7[i,j] = Metrics[1]\n",
    "            RK_NL7[i,j] = Metrics[2]\n",
    "            MKT_NL7[i,j] = Metrics[3]\n",
    "            KFA_NL7[i,j] = Metrics[4]\n",
    "    MKFullNLArr.append(MK_NL7)\n",
    "    RKFullNLArr.append(RK_NL7)\n",
    "    AKFullNLArr.append(AK_NL7)\n",
    "    MKTFullNLArr.append(MKT_NL7)\n",
    "    KFAFullNLArr.append(KFA_NL7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9abf8a2-279e-421b-9899-8dc4d2ccc4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKMinNLArr = []\n",
    "RKMinNLArr = []\n",
    "AKMinNLArr = []\n",
    "MKTMinNLArr = []\n",
    "KFAMinNLArr = []\n",
    "for kk in tqdm(range(32)):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabs7[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(TD[kk][:,:,axial_middles[kk],selected_indices7])\n",
    "    ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7[i,j] = Metrics[0]\n",
    "            AK_NL7[i,j] = Metrics[1]\n",
    "            RK_NL7[i,j] = Metrics[2]\n",
    "            MKT_NL7[i,j] = Metrics[3]\n",
    "            KFA_NL7[i,j] = Metrics[4]\n",
    "    MKMinNLArr.append(MK_NL7)\n",
    "    RKMinNLArr.append(RK_NL7)\n",
    "    AKMinNLArr.append(AK_NL7)\n",
    "    MKTMinNLArr.append(MKT_NL7)\n",
    "    KFAMinNLArr.append(KFA_NL7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78628b-efc3-467b-9df5-98d6c76954f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DatFolder+'Full_MK_HCP_MLE.npy'):\n",
    "    MK_MLE_Full = np.load(DatFolder+'Full_MK_HCP_MLE.npy',allow_pickle=True)\n",
    "    AK_MLE_Full = np.load(DatFolder+'Full_AK_HCP_MLE.npy',allow_pickle=True)\n",
    "    RK_MLE_Full = np.load(DatFolder+'Full_RK_HCP_MLE.npy',allow_pickle=True)\n",
    "else:\n",
    "    MK_MLE_Full = []\n",
    "    AK_MLE_Full = []\n",
    "    RK_MLE_Full = []\n",
    "    for kk in tqdm(range(1,33)):\n",
    "        ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "        NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "        torch.manual_seed(10)\n",
    "        for i in tqdm(range(ArrShape[0])):\n",
    "            for j in range(ArrShape[1]):\n",
    "                if(np.sum(TD[kk][:,:,axial_middles[kk],:],axis=-1) == 0):\n",
    "                    pass\n",
    "                else:\n",
    "                    tObs = TD[kk][:,:,axial_middles[kk]]\n",
    "                    LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze(),np.log(tObs[gtabF[kk].bvals==0].mean())]), args=(gtabsF[kk], tObs)).x\n",
    "                    res = Fullimize(\n",
    "                            rician_nll_DKI,\n",
    "                            np.hstack([LS_x[:6],LS_x[6:21],np.log(10)]),\n",
    "                            args=(tObs, gtabExt, np.exp(LS_x[-1])),\n",
    "                            options={'disp':0,'maxfun': 100000,   # ← raise the cap on f+g evaluations\n",
    "                                'maxiter': 20000,   # ← (optional) raise the cap on iterations   # ← raise the cap on f+g evaluations   # ← raise the cap on f+g evaluations\n",
    "                                'gtol': 1e-6,\n",
    "                                'ftol': 1e-6,},\n",
    "                                bounds=bounds\n",
    "                        )\n",
    "                    NoiseEst[i,j] = res.x\n",
    "        NoiseEstInv = np.zeros_like(NoiseEst)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]):    \n",
    "                NoiseEstInv[i,j] = np.hstack([mat_to_vals(ComputeDTI(NoiseEst[i,j,:6])),NoiseEst[i,j,6:21],NoiseEst[i,j,-1]])\n",
    "        NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]):    \n",
    "                NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEstInv[i,j]))),NoiseEstInv[i,j,6:]])\n",
    "        MK_MLEFull  = np.zeros(ArrShape)\n",
    "        AK_MLEFull  = np.zeros(ArrShape)\n",
    "        RK_MLEFull  = np.zeros(ArrShape)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]): \n",
    "                Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "                MK_MLEFull[i,j] = Metrics[0]\n",
    "                AK_MLEFull[i,j] = Metrics[1]\n",
    "                RK_MLEFull[i,j] = Metrics[2]\n",
    "        MKFulMLEArr.append(MK_MLEFull)\n",
    "        AK_MLE_Full.append(AK_MLEFull)\n",
    "        RK_MLE_Full.append(RK_MLEFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8da1cb-45f1-47b1-a37f-638998336741",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DatFolder+'Min_MK_HCP_MLE.npy'):\n",
    "    MK_MLE_Min = np.load(DatFolder+'Min_MK_HCP_MLE.npy',allow_pickle=True)\n",
    "    AK_MLE_Min = np.load(DatFolder+'Min_AK_HCP_MLE.npy',allow_pickle=True)\n",
    "    RK_MLE_Min = np.load(DatFolder+'Min_RK_HCP_MLE.npy',allow_pickle=True)\n",
    "else:\n",
    "    MK_MLE_Min = []\n",
    "    AK_MLE_Min = []\n",
    "    RK_MLE_Min = []\n",
    "    for kk in tqdm(range(1,33)):\n",
    "        ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "        NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "        torch.manual_seed(10)\n",
    "        for i in tqdm(range(ArrShape[0])):\n",
    "            for j in range(ArrShape[1]):\n",
    "                if(np.sum(TD[kk][:,:,axial_middles[kk],:],axis=-1) == 0):\n",
    "                    pass\n",
    "                else:\n",
    "                    tObs = TD[kk][:,:,axial_middles[kk],selected_indices7]\n",
    "                    LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze(),np.log(tObs[gtabF[kk].bvals==0].mean())]), args=(gtabs7[kk], tObs)).x\n",
    "                    res = Minimize(\n",
    "                            rician_nll_DKI,\n",
    "                            np.hstack([LS_x[:6],LS_x[6:21],np.log(10)]),\n",
    "                            args=(tObs, gtabExt, np.exp(LS_x[-1])),\n",
    "                            options={'disp':0,'maxfun': 100000,   # ← raise the cap on f+g evaluations\n",
    "                                'maxiter': 20000,   # ← (optional) raise the cap on iterations   # ← raise the cap on f+g evaluations   # ← raise the cap on f+g evaluations\n",
    "                                'gtol': 1e-6,\n",
    "                                'ftol': 1e-6,},\n",
    "                                bounds=bounds\n",
    "                        )\n",
    "                    NoiseEst[i,j] = res.x\n",
    "        NoiseEstInv = np.zeros_like(NoiseEst)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]):    \n",
    "                NoiseEstInv[i,j] = np.hstack([mat_to_vals(ComputeDTI(NoiseEst[i,j,:6])),NoiseEst[i,j,6:21],NoiseEst[i,j,-1]])\n",
    "        NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]):    \n",
    "                NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEstInv[i,j]))),NoiseEstInv[i,j,6:]])\n",
    "        MK_MLEMin  = np.zeros(ArrShape)\n",
    "        AK_MLEMin  = np.zeros(ArrShape)\n",
    "        RK_MLEMin  = np.zeros(ArrShape)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]): \n",
    "                Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "                MK_MLEMin[i,j] = Metrics[0]\n",
    "                AK_MLEMin[i,j] = Metrics[1]\n",
    "                RK_MLEMin[i,j] = Metrics[2]\n",
    "        MKFulMLEArr.append(MK_MLEMin)\n",
    "        AK_MLE_Min.append(AK_MLEMin)\n",
    "        RK_MLE_Min.append(RK_MLEMin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f736ae-1307-406f-a741-ad30042ac61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DatFolder+'Full_MK_HCP_Bay.npy'):\n",
    "    MK_Bay_Full = np.load(DatFolder+'Full_MK_HCP_Bay.npy',allow_pickle=True)\n",
    "    AK_Bay_Full = np.load(DatFolder+'Full_AK_HCP_Bay.npy',allow_pickle=True)\n",
    "    RK_Bay_Full = np.load(DatFolder+'Full_RK_HCP_Bay.npy',allow_pickle=True)\n",
    "else:\n",
    "    MK_Bay_Full = []\n",
    "    AK_Bay_Full = []\n",
    "    RK_Bay_Full = []\n",
    "    for kk in tqdm(range(1,33)):\n",
    "        ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "        NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "        torch.manual_seed(10)\n",
    "        for i in tqdm(range(ArrShape[0])):\n",
    "            for j in range(ArrShape[1]):\n",
    "                if(np.sum(TD[kk][:,:,axial_middles[kk],:],axis=-1) == 0):\n",
    "                    pass\n",
    "                else:\n",
    "                    tObs = TD[kk][:,:,axial_middles[kk]]\n",
    "                    LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze(),np.log(100)]), args=(gtabsF[kk], tObs)).x\n",
    "                    try:\n",
    "                        m, S, prec = vb_gauss_one_voxel(tObs, gtabHCP7, LS_x,mu0=mu0, V0=V0)\n",
    "                    except:\n",
    "                        m = 0\n",
    "                    NoiseEst[i,j] = m\n",
    "        NoiseEstInv = np.zeros_like(NoiseEst)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]):    \n",
    "                NoiseEstInv[i,j] = np.hstack([mat_to_vals(ComputeDTI(NoiseEst[i,j,:6])),NoiseEst[i,j,6:21],NoiseEst[i,j,-1]])\n",
    "        NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]):    \n",
    "                NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEstInv[i,j]))),NoiseEstInv[i,j,6:]])\n",
    "        MK_BayFull  = np.zeros(ArrShape)\n",
    "        AK_BayFull  = np.zeros(ArrShape)\n",
    "        RK_BayFull  = np.zeros(ArrShape)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]): \n",
    "                Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "                MK_BayFull[i,j] = Metrics[0]\n",
    "                AK_BayFull[i,j] = Metrics[1]\n",
    "                RK_BayFull[i,j] = Metrics[2]\n",
    "        MKFulBayArr.append(MK_BayFull)\n",
    "        AK_Bay_Full.append(AK_BayFull)\n",
    "        RK_Bay_Full.append(RK_BayFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4250330-c816-4499-82db-6887cf2b979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DatFolder+'Min_MK_HCP_Bay.npy'):\n",
    "    MK_Bay_Min = np.load(DatFolder+'Min_MK_HCP_Bay.npy',allow_pickle=True)\n",
    "    AK_Bay_Min = np.load(DatFolder+'Min_AK_HCP_Bay.npy',allow_pickle=True)\n",
    "    RK_Bay_Min = np.load(DatFolder+'Min_RK_HCP_Bay.npy',allow_pickle=True)\n",
    "else:\n",
    "    MK_Bay_Min = []\n",
    "    AK_Bay_Min = []\n",
    "    RK_Bay_Min = []\n",
    "    for kk in tqdm(range(1,33)):\n",
    "        ArrShape = TD[kk][:,:,axial_middles[kk],0].shape\n",
    "        NoiseEst = np.zeros(list(ArrShape) + [22])\n",
    "        torch.manual_seed(10)\n",
    "        for i in tqdm(range(ArrShape[0])):\n",
    "            for j in range(ArrShape[1]):\n",
    "                if(np.sum(TD[kk][:,:,axial_middles[kk],:],axis=-1) == 0):\n",
    "                    pass\n",
    "                else:\n",
    "                    tObs = TD[kk][:,:,axial_middles[kk],selected_indices7]\n",
    "                    LS_x = least_squares(residuals, x0=np.hstack([invertComputeDTI(vals_to_mat(DT_guess.squeeze())),KT_guess.squeeze(),np.log(100)]), args=(gtabs7[kk], tObs)).x\n",
    "                    try:\n",
    "                        m, S, prec = vb_gauss_one_voxel(tObs, gtabHCP7, LS_x,mu0=mu0, V0=V0)\n",
    "                    except:\n",
    "                        m = 0\n",
    "                    NoiseEst[i,j] = res.x\n",
    "        NoiseEstInv = np.zeros_like(NoiseEst)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]):    \n",
    "                NoiseEstInv[i,j] = np.hstack([mat_to_vals(ComputeDTI(NoiseEst[i,j,:6])),NoiseEst[i,j,6:21],NoiseEst[i,j,-1]])\n",
    "        NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]):    \n",
    "                NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEstInv[i,j]))),NoiseEstInv[i,j,6:]])\n",
    "        MK_BayMin  = np.zeros(ArrShape)\n",
    "        AK_BayMin  = np.zeros(ArrShape)\n",
    "        RK_BayMin  = np.zeros(ArrShape)\n",
    "        for i in range(ArrShape[0]):\n",
    "            for j in range(ArrShape[1]): \n",
    "                Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "                MK_BayMin[i,j] = Metrics[0]\n",
    "                AK_BayMin[i,j] = Metrics[1]\n",
    "                RK_BayMin[i,j] = Metrics[2]\n",
    "        MKFulBayArr.append(MK_BayMin)\n",
    "        AK_Bay_Min.append(AK_BayMin)\n",
    "        RK_Bay_Min.append(RK_BayMin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e68b1-b501-4f9c-a33c-ea82ffbea4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_bay = []\n",
    "for i in range(32):\n",
    "    NS1 = MK_Bay_Min[i].astype(np.float32)\n",
    "    NS2 = MK_Bay_Full[i].astype(np.float32)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_bay.append(result)\n",
    "\n",
    "SSIM_MLE = []\n",
    "for i in range(32):\n",
    "    NS1 = MK_MLE_Min[i].astype(np.float32)\n",
    "    NS2 = MK_MLE_Full[i].astype(np.float32)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_MLE.append(result)\n",
    "\n",
    "SSIM_NL = []\n",
    "for i in range(32):\n",
    "    NS1 = MKMinNLArr[i]\n",
    "    NS2 = MKFullNLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_NL.append(result)\n",
    "\n",
    "SSIM_WL = []\n",
    "for i in range(32):\n",
    "    NS1 = MKMinWLArr[i]\n",
    "    NS2 = MKFullWLArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_WL.append(result)\n",
    "\n",
    "SSIM_SBI= []\n",
    "for i in range(32):\n",
    "    NS1 = MKMinArr[i]\n",
    "    NS2 = MKFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_SBI.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7756d3-6ae1-4b3c-a4a6-813594ac41da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b5a13-191e-4af1-a1a6-bb276da63fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "y_data = np.array(SSIM_SBI)\n",
    "g_pos = np.array([1])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM_NL)\n",
    "g_pos = np.array([2])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM_WL)\n",
    "g_pos = np.array([3])\n",
    "colors = [WLSFit]\n",
    "colors2 = ['lightsalmon']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "y_data = np.array(SSIM_MLE)\n",
    "g_pos = np.array([4])\n",
    "colors = [MLEFit]\n",
    "colors2 = ['lightsteelblue']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "y_data = np.array(SSIM_bay)\n",
    "g_pos = np.array([5])\n",
    "colors = [BayFit]\n",
    "colors2 = ['lavender']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "plt.yticks([0,0.2,0.4,0.6,0.8])\n",
    "plt.xticks([1,2,3,4,5],['SBI','NLLS','WLS','MLE','Bay.'],fontsize=32,rotation=90)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_MK_Other.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018777f0-03bd-4220-a0ec-bc3a313fad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_bay = []\n",
    "for i in range(32):\n",
    "    NS1 = AK_Bay_Min[i].astype(np.float32)\n",
    "    NS2 = AK_Bay_Full[i].astype(np.float32)\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_bay.append(result)\n",
    "\n",
    "SSIM_MLE = []\n",
    "for i in range(32):\n",
    "    NS1 = AK_MLE_Min[i].astype(np.float32)\n",
    "    NS2 = AK_MLE_Full[i].astype(np.float32)\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_MLE.append(result)\n",
    "\n",
    "SSIM_NL = []\n",
    "for i in range(32):\n",
    "    NS1 = AKMinNLArr[i]\n",
    "    NS2 = AKFullNLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_NL.append(result)\n",
    "\n",
    "SSIM_WL = []\n",
    "for i in range(32):\n",
    "    NS1 = AKMinWLArr[i]\n",
    "    NS2 = AKFullWLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_WL.append(result)\n",
    "\n",
    "SSIM_SBI= []\n",
    "for i in range(32):\n",
    "    NS1 =AKMinArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =AKFullArr[i]\n",
    "    NS2 = gaussian_filter(NS2, sigma=0.5)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM_SBI.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced094f3-628f-4a81-a4c4-2f5a4602f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "y_data = np.array(SSIM_SBI)\n",
    "g_pos = np.array([1])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM_NL)\n",
    "g_pos = np.array([2])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM_WL)\n",
    "g_pos = np.array([3])\n",
    "colors = [WLSFit]\n",
    "colors2 = ['lightsalmon']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "y_data = np.array(SSIM_MLE)\n",
    "g_pos = np.array([4])\n",
    "colors = [MLEFit]\n",
    "colors2 = ['lightsteelblue']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "y_data = np.array(SSIM_bay)\n",
    "g_pos = np.array([5])\n",
    "colors = [BayFit]\n",
    "colors2 = ['lavender']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "plt.yticks([0,0.2,0.4,0.6,0.8])\n",
    "plt.ylim([0,1])\n",
    "plt.xticks([1,2,3,4,5],['SBI','NLLS','WLS','MLE','Bay.'],fontsize=32,rotation=90)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_AK_Other.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34acefe-95ea-4c2d-960c-3a7c60343180",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_bay = []\n",
    "for i in range(32):\n",
    "    NS1 = RK_Bay_Min[i].astype(np.float32)\n",
    "    NS2 = RK_Bay_Full[i].astype(np.float32)\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_bay.append(result)\n",
    "\n",
    "SSIM_MLE = []\n",
    "for i in range(32):\n",
    "    NS1 = RK_MLE_Min[i].astype(np.float32)\n",
    "    NS2 = RK_MLE_Full[i].astype(np.float32)\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_MLE.append(result)\n",
    "\n",
    "SSIM_NL = []\n",
    "for i in range(32):\n",
    "    NS1 = RKMinNLArr[i]\n",
    "    NS2 = RKFullNLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_NL.append(result)\n",
    "\n",
    "SSIM_WL = []\n",
    "for i in range(32):\n",
    "    NS1 = RKMinWLArr[i]\n",
    "    NS2 = RKFullWLArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range = 1)\n",
    "    SSIM_WL.append(result)\n",
    "\n",
    "SSIM_SBI= []\n",
    "for i in range(32):\n",
    "    NS1 =RKMinArr[i]\n",
    "    NS1 = gaussian_filter(NS1, sigma=0.5)\n",
    "    NS2 =RKFullArr[i]\n",
    "    Ma = masks[i]\n",
    "    result = masked_local_ssim(NS1, NS2, Ma, win_size=7,dat_range=1)\n",
    "    SSIM_SBI.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68c903-1bfd-4110-849b-3e6b9ab31a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.2,4.8))#, sharex=True)\n",
    "\n",
    "y_data = np.array(SSIM_SBI)\n",
    "g_pos = np.array([1])\n",
    "colors = ['mediumturquoise']\n",
    "colors2 = ['paleturquoise']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM_NL)\n",
    "g_pos = np.array([2])\n",
    "colors = ['sandybrown']\n",
    "colors2 = ['peachpuff']\n",
    "\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "y_data = np.array(SSIM_WL)\n",
    "g_pos = np.array([3])\n",
    "colors = [WLSFit]\n",
    "colors2 = ['lightsalmon']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "y_data = np.array(SSIM_MLE)\n",
    "g_pos = np.array([4])\n",
    "colors = [MLEFit]\n",
    "colors2 = ['lightsteelblue']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "y_data = np.array(SSIM_bay)\n",
    "g_pos = np.array([5])\n",
    "colors = [BayFit]\n",
    "colors2 = ['lavender']\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,widths=0.2,scatter=True)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "plt.yticks([0,0.2,0.4,0.6,0.8])\n",
    "plt.ylim([-0.1,1])\n",
    "plt.xticks([1,2,3,4,5],['SBI','NLLS','WLS','MLE','Bay.'],fontsize=32,rotation=90)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_RK_Other.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfbd60-cac8-4c8d-afb0-d4e16a64dd18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
