{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f961d685-737e-464b-bb40-baf5ae2b7a38",
   "metadata": {},
   "source": [
    "# Front matter and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb91541-4dc6-47fb-b4c4-6828168b5553",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e3d45-362e-4672-8e43-9875ca81d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dill as pickle\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import lognorm,norm\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "\n",
    "# Define font properties\n",
    "font = {\n",
    "    'family': 'sans-serif',  # Use sans-serif family\n",
    "    'sans-serif': ['Helvetica'],  # Specify Helvetica as the sans-serif font\n",
    "    'size': 14  # Set the default font size\n",
    "}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# Set tick label sizes\n",
    "plt.rc('ytick', labelsize=24)\n",
    "plt.rc('xtick', labelsize=24)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n",
    "# Customize axes spines and legend appearance\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "\n",
    "from sbi import analysis as analysis\n",
    "from sbi import utils as utils\n",
    "from sbi.inference import SNPE, simulate_for_sbi\n",
    "from sbi.inference.potentials.posterior_based_potential import posterior_estimator_based_potential\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")\n",
    "from sbi.utils import process_prior,BoxUniform\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Categorical,Normal\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "from dipy.sims.voxel import single_tensor\n",
    "from dipy.data import get_fnames\n",
    "from dipy.io.gradients import read_bvals_bvecs\n",
    "from dipy.core.gradients import gradient_table\n",
    "from dipy.reconst.dti import (decompose_tensor, from_lower_triangular)\n",
    "from dipy.io.image import load_nifti\n",
    "from dipy.segment.mask import median_otsu\n",
    "import dipy.reconst.dti as dti\n",
    "import dipy.reconst.dki as dki\n",
    "from dipy.align.reslice import reslice\n",
    "from dipy.core.sphere import disperse_charges, Sphere, HemiSphere\n",
    "\n",
    "import pymatreader as pmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d0e97f-d8b3-49e1-b1dd-7a0e1e844384",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717fb08-d502-465a-a830-70812a9846f7",
   "metadata": {},
   "source": [
    "### DTI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b35f2-0c20-45ec-b58d-0c25f3410e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vals_to_mat(dt):\n",
    "    DTI = np.zeros((3,3))\n",
    "    DTI[0,0] = dt[0]\n",
    "    DTI[0,1],DTI[1,0] =  dt[1],dt[1]\n",
    "    DTI[1,1] =  dt[2]\n",
    "    DTI[0,2],DTI[2,0] =  dt[3],dt[3]\n",
    "    DTI[1,2],DTI[2,1] =  dt[4],dt[4]\n",
    "    DTI[2,2] =  dt[5]\n",
    "    return DTI\n",
    "\n",
    "def mat_to_vals(DTI):\n",
    "    dt = np.zeros(6)\n",
    "    dt[0] = DTI[0,0]\n",
    "    dt[1] = DTI[0,1]\n",
    "    dt[2] = DTI[1,1]\n",
    "    dt[3] = DTI[0,2]\n",
    "    dt[4] = DTI[1,2]\n",
    "    dt[5] = DTI[2,2]\n",
    "    return dt\n",
    "\n",
    "def fill_lower_diag(a):\n",
    "    b = [a[0],a[3],a[1],a[4],a[5],a[2]]\n",
    "    n = 3\n",
    "    mask = np.tri(n,dtype=bool) \n",
    "    out = np.zeros((n,n),dtype=float)\n",
    "    out[mask] = b\n",
    "    return out\n",
    "\n",
    "def ComputeDTI(params):\n",
    "    L = fill_lower_diag(params)\n",
    "    \n",
    "    np.fill_diagonal(L, np.abs(np.diagonal(L)))\n",
    "\n",
    "    A = L @ L.T\n",
    "    return A\n",
    "\n",
    "def ForceLowFA(dt):\n",
    "    # Modify the matrix to ensure low FA (more isotropic)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(dt)\n",
    "    \n",
    "    # Make the eigenvalues more similar to enforce low FA\n",
    "    mean_eigenvalue = np.mean(eigenvalues)\n",
    "\n",
    "    adjusted_eigenvalues = np.clip(eigenvalues, mean_eigenvalue * np.random.rand(), mean_eigenvalue * 1.0)\n",
    "    \n",
    "    # Reconstruct the matrix with the adjusted eigenvalues\n",
    "    dt_low_fa = eigenvectors @ np.diag(adjusted_eigenvalues) @ eigenvectors.T\n",
    "    \n",
    "    return dt_low_fa\n",
    "    \n",
    "def FracAni(evals,MD):\n",
    "    numerator = np.sqrt(3 * np.sum((evals - MD) ** 2))\n",
    "    denominator = np.sqrt(2) * np.sqrt(np.sum(evals ** 2))\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "def clip_negative_eigenvalues(matrix):\n",
    "    # Perform eigenvalue decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "    \n",
    "    # Clip negative eigenvalues to 0\n",
    "    clipped_eigenvalues = np.maximum(eigenvalues, 1e-5)\n",
    "    \n",
    "    # Reconstruct the matrix with the clipped eigenvalues\n",
    "    clipped_matrix = eigenvectors @ np.diag(clipped_eigenvalues) @ np.linalg.inv(eigenvectors)\n",
    "    \n",
    "    return clipped_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea42bca-723a-4a92-821e-2a1b5c2beed4",
   "metadata": {},
   "source": [
    "### DKI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09482812-1b86-42b0-b203-ff4b5911ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FitDT(Dat,seed=1):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    # DT_abc\n",
    "    data = Dat[:,0]\n",
    "    shape,loc,scale = lognorm.fit(data)\n",
    "    \n",
    "    dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "\n",
    "    #DT_rest\n",
    "    data = Dat[:,1]\n",
    "    loc,scale = norm.fit(data)\n",
    "    \n",
    "    # Compute the fitted PDF\n",
    "    dti2_fitted = stats.norm(loc=loc, scale=scale)\n",
    "\n",
    "    return dti1_fitted,dti2_fitted\n",
    "\n",
    "def FitKT(Dat,seed=1):\n",
    "    np.random.seed(seed)    \n",
    "    # Fitting x4\n",
    "    data = Dat[:,0]\n",
    "    shape,loc,scale = lognorm.fit(data)\n",
    "    x4_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "    \n",
    "    # Fitting R1\n",
    "    data = Dat[:,3]\n",
    "    loc,scale = norm.fit(data)\n",
    "    R1_fitted = norm(loc,scale)\n",
    "    \n",
    "    # Fitting x2\n",
    "    data = Dat[:,9]\n",
    "    shape,loc,scale = lognorm.fit(data)\n",
    "    x2_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "\n",
    "    # Fitting R2\n",
    "    data = Dat[:,12]\n",
    "    loc,scale = norm.fit(data)\n",
    "    R2_fitted = norm(loc,scale)\n",
    "\n",
    "\n",
    "    return x4_fitted,R1_fitted,x2_fitted,R2_fitted\n",
    "\n",
    "def GenDTKT(DT_Fits,KT_Fits,seed,size):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    DT = np.zeros([size,6])\n",
    "    KT = np.zeros([size,15])\n",
    "\n",
    "    DT[:,0] = DT_Fits[0].rvs(size)\n",
    "    DT[:,2] = DT_Fits[0].rvs(size)\n",
    "    DT[:,5] = DT_Fits[0].rvs(size)\n",
    "\n",
    "    DT[:,1] = DT_Fits[1].rvs(size)\n",
    "    DT[:,3] = DT_Fits[1].rvs(size)\n",
    "    DT[:,4] = DT_Fits[1].rvs(size)\n",
    "\n",
    "    for k in range(3):\n",
    "        KT[:,k] = KT_Fits[0].rvs(size)\n",
    "    for k in range(3,9):\n",
    "        KT[:,k] = KT_Fits[1].rvs(size)\n",
    "    for k in range(9,12):\n",
    "        KT[:,k] = KT_Fits[2].rvs(size)\n",
    "    for k in range(12,15):\n",
    "        KT[:,k] = KT_Fits[3].rvs(size)\n",
    "\n",
    "    return DT,KT\n",
    "    \n",
    "def DKIMetrics(dt,kt,analytical=True):\n",
    "    if(dt.ndim == 1):\n",
    "        dt = vals_to_mat(dt)\n",
    "    evals,evecs = np.linalg.eigh(dt)\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evals = evals[idx]\n",
    "    evecs = evecs[:, idx]\n",
    "    \n",
    "    params = np.concatenate([evals,np.hstack(evecs),kt])\n",
    "    params2 = np.concatenate([evals,np.hstack(evecs),-kt])\n",
    "\n",
    "    mk = dki.mean_kurtosis(params,analytical=analytical,min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    ak = dki.axial_kurtosis(params,analytical=analytical,min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    rk = dki.radial_kurtosis(params,analytical=analytical,min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    mkt = dki.mean_kurtosis_tensor(params, min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    kfa = kurtosis_fractional_anisotropy_test(params)\n",
    "\n",
    "    return mk,ak,rk,mkt,kfa\n",
    "    \n",
    "def kurtosis_fractional_anisotropy_test(dki_params):\n",
    "    r\"\"\"Compute the anisotropy of the kurtosis tensor (KFA).\n",
    "\n",
    "    See :footcite:p:`Glenn2015` and :footcite:p:`NetoHenriques2021` for further\n",
    "    details about the method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dki_params : ndarray (x, y, z, 27) or (n, 27)\n",
    "        All parameters estimated from the diffusion kurtosis model.\n",
    "        Parameters are ordered as follows:\n",
    "            1) Three diffusion tensor's eigenvalues\n",
    "            2) Three lines of the eigenvector matrix each containing the first,\n",
    "                second and third coordinates of the eigenvector\n",
    "            3) Fifteen elements of the kurtosis tensor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kfa : array\n",
    "        Calculated mean kurtosis tensor.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The KFA is defined as :footcite:p:`Glenn2015`:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "         KFA \\equiv\n",
    "         \\frac{||\\mathbf{W} - MKT \\mathbf{I}^{(4)}||_F}{||\\mathbf{W}||_F}\n",
    "\n",
    "    where $W$ is the kurtosis tensor, MKT the kurtosis tensor mean, $I^{(4)}$ is\n",
    "    the fully symmetric rank 2 isotropic tensor and $||...||_F$ is the tensor's\n",
    "    Frobenius norm :footcite:p:`Glenn2015`.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. footbibliography::\n",
    "\n",
    "    \"\"\"\n",
    "    Wxxxx = dki_params[..., 12]\n",
    "    Wyyyy = dki_params[..., 13]\n",
    "    Wzzzz = dki_params[..., 14]\n",
    "    Wxxxy = dki_params[..., 15]\n",
    "    Wxxxz = dki_params[..., 16]\n",
    "    Wxyyy = dki_params[..., 17]\n",
    "    Wyyyz = dki_params[..., 18]\n",
    "    Wxzzz = dki_params[..., 19]\n",
    "    Wyzzz = dki_params[..., 20]\n",
    "    Wxxyy = dki_params[..., 21]\n",
    "    Wxxzz = dki_params[..., 22]\n",
    "    Wyyzz = dki_params[..., 23]\n",
    "    Wxxyz = dki_params[..., 24]\n",
    "    Wxyyz = dki_params[..., 25]\n",
    "    Wxyzz = dki_params[..., 26]\n",
    "\n",
    "\n",
    "    W = 1.0 / 5.0 * (Wxxxx + Wyyyy + Wzzzz + 2 * Wxxyy + 2 * Wxxzz + 2 * Wyyzz)\n",
    "    # Compute's equation numerator\n",
    "    A = (\n",
    "        (Wxxxx - W) ** 2\n",
    "        + (Wyyyy - W) ** 2\n",
    "        + (Wzzzz - W) ** 2\n",
    "        + 4 * (Wxxxy**2 + Wxxxz**2 + Wxyyy**2 + Wyyyz**2 + Wxzzz**2 + Wyzzz**2)\n",
    "        + 6 * ((Wxxyy - W / 3) ** 2 + (Wxxzz - W / 3) ** 2 + (Wyyzz - W / 3) ** 2)\n",
    "        + 12 * (Wxxyz**2 + Wxyyz**2 + Wxyzz**2)\n",
    "    )\n",
    "    # Compute's equation denominator\n",
    "    B = (\n",
    "        Wxxxx**2\n",
    "        + Wyyyy**2\n",
    "        + Wzzzz**2\n",
    "        + 4 * (Wxxxy**2 + Wxxxz**2 + Wxyyy**2 + Wyyyz**2 + Wxzzz**2 + Wyzzz**2)\n",
    "        + 6 * (Wxxyy**2 + Wxxzz**2 + Wyyzz**2)\n",
    "        + 12 * (Wxxyz**2 + Wxyyz**2 + Wxyzz**2)\n",
    "    )\n",
    "\n",
    "    # Compute KFA\n",
    "    KFA = np.zeros(A.shape)\n",
    "    KFA = np.sqrt(A / B)\n",
    "\n",
    "    return KFA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd64dcc-b517-435f-ba95-d9e6a4496319",
   "metadata": {},
   "source": [
    "### Simulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e445080b-782a-4aad-a29b-f1a19ba283fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomSimulator(Mat,gtab,S0,snr=None):\n",
    "    evals,evecs = np.linalg.eigh(Mat)\n",
    "    signal = single_tensor(gtab, S0=S0, evals=evals, evecs=evecs)\n",
    "    if(snr is None):\n",
    "        return signal\n",
    "    else:\n",
    "        return AddNoise(signal,S0,snr)\n",
    "\n",
    "def Simulator(bvals,bvecs,S0,params,SNR):\n",
    "\n",
    "    dt = ComputeDTI(params)\n",
    "    signal_dti = CustomSimulator(dt,gradient_table(bvals, bvecs),S0,SNR)\n",
    "    \n",
    "    return signal_dti\n",
    "\n",
    "\n",
    "def GenRicciNoise(signal,S0,snr):\n",
    "\n",
    "    size = signal.shape\n",
    "    sigma = S0 / snr\n",
    "    noise1 = np.random.normal(0, sigma, size=size)\n",
    "    noise2 = np.random.normal(0, sigma, size=size)\n",
    "\n",
    "    return np.sqrt((signal+noise1) ** 2 + noise2 ** 2)\n",
    "\n",
    "\n",
    "def AddNoise(signal,S0,snr):\n",
    "    \n",
    "    return GenRicciNoise(signal,S0,snr)\n",
    "\n",
    "def CustomDKISimulator(dt,kt,gtab,S0,snr=None):\n",
    "    if(dt.ndim == 1):\n",
    "        dt = vals_to_mat(dt)\n",
    "    evals,evecs = np.linalg.eigh(dt)\n",
    "    params = np.concatenate([evals,np.hstack(evecs),kt])\n",
    "    signal = dki.dki_prediction(params,gtab,S0)\n",
    "    if(snr is None):\n",
    "        return signal\n",
    "    else:\n",
    "        return AddNoise(signal,S0,snr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287aaa10-3d36-4b90-9266-02914c612c3a",
   "metadata": {},
   "source": [
    "### SBI Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79408e28-c4fa-45d3-b60e-5c3102c1e2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIPrior:\n",
    "    def __init__(self, lower_abs : Tensor, upper_abs : Tensor, \n",
    "                       lower_rest: Tensor, upper_rest: Tensor,\n",
    "                        return_numpy: bool = False):\n",
    "\n",
    "        self.dist_abs = BoxUniform(low= lower_abs* torch.ones(3), high=upper_abs * torch.ones(3))\n",
    "        self.dist_rest = BoxUniform(low=lower_rest * torch.ones(3), high=upper_rest *torch.ones(3))\n",
    "        self.return_numpy = return_numpy\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        \n",
    "        abc  = self.dist_abs.sample(sample_shape)\n",
    "        rest = self.dist_rest.sample(sample_shape)\n",
    "        \n",
    "        if self.return_numpy:   \n",
    "            params = np.hstack([abc,rest]) \n",
    "        else:\n",
    "            params = torch.hstack([abc,rest])\n",
    "\n",
    "        return params\n",
    "        \n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        \n",
    "        abc  = values[:,:3]\n",
    "        rest = values[:,3:]\n",
    "\n",
    "        log_prob_abc  = self.dist_abs.log_prob(abc)\n",
    "        log_prob_rest = self.dist_rest.log_prob(rest)\n",
    "        return log_prob_abc+log_prob_rest\n",
    "\n",
    "class DTIPriorS0:\n",
    "    def __init__(self, lower_abs : Tensor, upper_abs : Tensor, \n",
    "                       lower_rest: Tensor, upper_rest: Tensor,\n",
    "                       lower_S0: Tensor, upper_S0: Tensor,\n",
    "                        return_numpy: bool = False):\n",
    "\n",
    "        self.dist_abs = BoxUniform(low= lower_abs* torch.ones(3), high=upper_abs * torch.ones(3))\n",
    "        self.dist_rest = BoxUniform(low=lower_rest * torch.ones(3), high=upper_rest *torch.ones(3))\n",
    "        self.dist_S0 = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "        self.return_numpy = return_numpy\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        \n",
    "        abc  = self.dist_abs.sample(sample_shape)\n",
    "        rest = self.dist_rest.sample(sample_shape)\n",
    "        S0   = self.dist_S0.sample(sample_shape)\n",
    "        \n",
    "        if self.return_numpy:   \n",
    "            params = np.hstack([abc,rest,S0]) \n",
    "        else:\n",
    "            params = torch.hstack([abc,rest,S0])\n",
    "\n",
    "        return params\n",
    "        \n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        \n",
    "        abc  = values[:,:3]\n",
    "        rest = values[:,3:-1]\n",
    "        S0   = values[:,-1]\n",
    "\n",
    "        log_prob_abc  = self.dist_abs.log_prob(abc)\n",
    "        log_prob_rest = self.dist_rest.log_prob(rest)\n",
    "        log_prob_S0 = self.dist_S0.log_prob(S0)\n",
    "        return log_prob_abc+log_prob_rest+log_prob_S0\n",
    "\n",
    "class DTIPriorS0Direc:\n",
    "    def __init__(self, lower_abs : Tensor, upper_abs : Tensor, \n",
    "                       lower_rest: Tensor, upper_rest: Tensor,\n",
    "                       lower_S0: Tensor, upper_S0: Tensor,\n",
    "                        return_numpy: bool = False):\n",
    "\n",
    "        self.dist_abs = BoxUniform(low= lower_abs* torch.ones(3), high=upper_abs * torch.ones(3))\n",
    "        self.dist_rest = BoxUniform(low=lower_rest * torch.ones(3), high=upper_rest *torch.ones(3))\n",
    "        self.dist_S0 = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "        self.direction_choice = Categorical(probs=torch.ones(1, 5))\n",
    "        self.return_numpy = return_numpy\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        \n",
    "        abc  = self.dist_abs.sample(sample_shape)\n",
    "        rest = self.dist_rest.sample(sample_shape)\n",
    "        S0   = self.dist_S0.sample(sample_shape)\n",
    "        direc = self.direction_choice.sample(sample_shape)       \n",
    "        \n",
    "        if self.return_numpy:   \n",
    "            params = np.hstack([abc,rest,S0,direc]) \n",
    "        else:\n",
    "            params = torch.hstack([abc,rest,S0,direc])\n",
    "\n",
    "        return params\n",
    "        \n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        \n",
    "        abc  = values[:,:3]\n",
    "        rest = values[:,3:-2]\n",
    "        S0   = values[:,-2]\n",
    "        direc   = values[:,-1]\n",
    "\n",
    "        log_prob_abc   = self.dist_abs.log_prob(abc)\n",
    "        log_prob_rest  = self.dist_rest.log_prob(rest)\n",
    "        log_prob_S0    = self.dist_S0.log_prob(S0)\n",
    "        log_prob_direc =  self.direction_choice.log_prob(direc)\n",
    "        return log_prob_abc+log_prob_rest+log_prob_S0+log_prob_direc\n",
    "\n",
    "class DTIPriorS0Noise:\n",
    "    def __init__(self, lower_abs : Tensor, upper_abs : Tensor, \n",
    "                       lower_rest: Tensor, upper_rest: Tensor,\n",
    "                       lower_S0: Tensor, upper_S0: Tensor,\n",
    "                       lower_noise: Tensor, upper_noise: Tensor,\n",
    "                        return_numpy: bool = False):\n",
    "\n",
    "        self.dist_abs = BoxUniform(low= lower_abs* torch.ones(3), high=upper_abs * torch.ones(3))\n",
    "        self.dist_rest = BoxUniform(low=lower_rest * torch.ones(3), high=upper_rest *torch.ones(3))\n",
    "        self.dist_S0 = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "        self.dist_noise = BoxUniform(low=torch.tensor([lower_noise]), high=torch.tensor([upper_noise]))\n",
    "        self.return_numpy = return_numpy\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        \n",
    "        abc     = self.dist_abs.sample(sample_shape)\n",
    "        rest    = self.dist_rest.sample(sample_shape)\n",
    "        S0      = self.dist_S0.sample(sample_shape)\n",
    "        noise   = self.dist_noise.sample(sample_shape)\n",
    "        \n",
    "        if self.return_numpy:   \n",
    "            params = np.hstack([abc,rest,S0,noise]) \n",
    "        else:\n",
    "            params = torch.hstack([abc,rest,S0,noise])\n",
    "\n",
    "        return params\n",
    "        \n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        \n",
    "        abc     = values[:,:3]\n",
    "        rest    = values[:,3:-2]\n",
    "        S0      = values[:,-2]\n",
    "        noise   = values[:,-1]\n",
    "\n",
    "        log_prob_abc  = self.dist_abs.log_prob(abc)\n",
    "        log_prob_rest = self.dist_rest.log_prob(rest)\n",
    "        log_prob_S0 = self.dist_S0.log_prob(S0)\n",
    "        log_prob_noise = self.dist_noise.log_prob(noise)\n",
    "        return log_prob_abc+log_prob_rest+log_prob_S0+log_prob_noise\n",
    "\n",
    "def histogram_mode(data, bins=50):\n",
    "    # Calculate the histogram\n",
    "    counts, bin_edges = np.histogram(data, bins=bins)\n",
    "    \n",
    "    # Find the bin with the maximum count (highest frequency)\n",
    "    max_bin_index = np.argmax(counts)\n",
    "    \n",
    "    # Calculate the mode as the midpoint of the bin with the highest count\n",
    "    mode = (bin_edges[max_bin_index] + bin_edges[max_bin_index + 1]) / 2\n",
    "    \n",
    "    return mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fc460-912e-409a-af59-c18f71ef2b05",
   "metadata": {},
   "source": [
    "### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff72b7da-8284-4f11-b85c-92ab12af1795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Errors(Guess,Truth,gtab,signal_true,signal_provided,S0Guess=200):\n",
    "    # Eigenvalue error\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess)\n",
    "    evals_guess = np.sort(evals_guess_raw)\n",
    "    evals_true_raw,evecs_true = np.linalg.eigh(Truth)\n",
    "    evals_true = np.sort(evals_true_raw)\n",
    "    \n",
    "    EigError = np.linalg.norm(evals_guess-evals_true)\n",
    "\n",
    "    # Mean diffusivitiy\n",
    "    mean_true = np.mean(evals_true)\n",
    "    mean_guess = np.mean(evals_guess)\n",
    "    MD = abs(mean_true-mean_guess)\n",
    "\n",
    "    # Fractional Anisotropy\n",
    "    FA_true  = FracAni(evals_true,mean_true)\n",
    "    FA_guess = FracAni(evals_guess,mean_guess)\n",
    "    FA = abs(FA_true-FA_guess)                                        \n",
    "\n",
    "    # Frobenius error\n",
    "    Frob =  np.linalg.norm(Guess-Truth, 'fro')\n",
    "\n",
    "    # Signal error\n",
    "    signal_guess = single_tensor(gtab, S0=S0Guess, evals=evals_guess_raw, evecs=evecs_guess)\n",
    "    Err  = np.linalg.norm(signal_true-signal_guess)/len(signal_true)\n",
    "    Corr = np.corrcoef(signal_true,signal_guess)[0,1]\n",
    "    \n",
    "    Err2  = np.linalg.norm(signal_provided-signal_guess[:len(signal_provided)])/len(signal_provided)\n",
    "    Corr2 = np.corrcoef(signal_provided,signal_guess[:len(signal_provided)])[0,1]\n",
    "    \n",
    "    return MD,FA,EigError,Frob,Err,Corr,Err2,Corr2\n",
    "\n",
    "def ErrorsMDFA(Guess,Truth):\n",
    "    # Eigenvalue error\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess)\n",
    "    evals_guess = np.sort(evals_guess_raw)\n",
    "    evals_true_raw,evecs_true = np.linalg.eigh(Truth)\n",
    "    evals_true = np.sort(evals_true_raw)\n",
    "\n",
    "    # Mean diffusivitiy\n",
    "    mean_true = np.mean(evals_true)\n",
    "    mean_guess = np.mean(evals_guess)\n",
    "    if(not mean_true == 0):\n",
    "        MD = abs(mean_true-mean_guess)\n",
    "    else:\n",
    "        MD = abs(mean_true-mean_guess)\n",
    "\n",
    "    # Fractional Anisotropy\n",
    "    FA_true  = FracAni(evals_true,mean_true)\n",
    "    FA_guess = FracAni(evals_guess,mean_guess)\n",
    "    if(not FA_true == 0):\n",
    "        FA = abs(FA_true-FA_guess)\n",
    "    else:\n",
    "        FA = abs(FA_true-FA_guess)\n",
    "                                    \n",
    "    \n",
    "    return MD,FA\n",
    "    \n",
    "def PercsMDFA(Guess,Truth):\n",
    "    # Eigenvalue error\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess)\n",
    "    evals_guess = np.sort(evals_guess_raw)\n",
    "    evals_true_raw,evecs_true = np.linalg.eigh(Truth)\n",
    "    evals_true = np.sort(evals_true_raw)\n",
    "\n",
    "    # Mean diffusivitiy\n",
    "    mean_true = np.mean(evals_true)\n",
    "    mean_guess = np.mean(evals_guess)\n",
    "    if(not mean_true == 0):\n",
    "        MD = abs(mean_true-mean_guess)/mean_true\n",
    "    else:\n",
    "        MD = abs(mean_true-mean_guess)/mean_true\n",
    "\n",
    "    # Fractional Anisotropy\n",
    "    FA_true  = FracAni(evals_true,mean_true)\n",
    "    FA_guess = FracAni(evals_guess,mean_guess)\n",
    "    if(not FA_true == 0):\n",
    "        FA = abs(FA_true-FA_guess)/FA_true\n",
    "    else:\n",
    "        FA = abs(FA_true-FA_guess)/FA_true\n",
    "                                    \n",
    "    \n",
    "    return MD,FA\n",
    "\n",
    "\n",
    "def DKIErrors(GuessDT,GuessKT,TruthDT,TruthKT):\n",
    "    guess = DKIMetrics(GuessDT,GuessKT,False)\n",
    "    truth = DKIMetrics(TruthDT,TruthKT,False)\n",
    "\n",
    "    #mk diff\n",
    "    mk = abs(guess[0]-truth[0])\n",
    "    ak = abs(guess[1]-truth[1])\n",
    "    rk = abs(guess[2]-truth[2])\n",
    "    mkt = abs(guess[3]-truth[3])\n",
    "    kfa = abs(guess[4]-truth[4])\n",
    "\n",
    "    return mk,ak,rk,mkt,kfa\n",
    "\n",
    "def Percs(GuessDT,GuessKT,TruthDT,TruthKT):\n",
    "    guess = DKIMetrics(GuessDT,GuessKT,False)\n",
    "    truth = DKIMetrics(TruthDT,TruthKT,False)\n",
    "    \n",
    "    #mk diff\n",
    "    mk = abs(guess[0]-truth[0])/abs(truth[0])\n",
    "    ak = abs(guess[1]-truth[1])/abs(truth[1])\n",
    "    rk = abs(guess[2]-truth[2])/abs(truth[2])\n",
    "    mkt = abs(guess[3]-truth[3])/abs(truth[3])\n",
    "    kfa = abs(guess[4]-truth[4])/abs(truth[4])\n",
    "    \n",
    "    return mk,ak,rk,mkt,kfa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae18af9a-8a6a-41f3-bc95-33001a251354",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21026b37-3475-4748-9de6-89a6d72a53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viol_plot(A,col,hatch=False,**kwargs):\n",
    "    A_T = np.transpose(A)\n",
    "    filtered_A = []\n",
    "    for column in A_T:\n",
    "        # Remove NaNs\n",
    "        column = column[~np.isnan(column)]\n",
    "        # Identify outliers using Z-score\n",
    "        z_scores = stats.zscore(column)\n",
    "        abs_z_scores = np.abs(z_scores)\n",
    "        # Filter data within 3 standard deviations\n",
    "        filtered_entries = (abs_z_scores < 1000)\n",
    "        filtered_column = column[filtered_entries]\n",
    "        filtered_A.append(filtered_column)\n",
    "    \n",
    "    vp = plt.violinplot(filtered_A,showmeans=True,**kwargs)  \n",
    "    for v in vp['bodies']:\n",
    "        v.set_facecolor(col)\n",
    "    vp['cbars'].set_color(col)\n",
    "    vp['cmins'].set_color(col)\n",
    "    vp['cmaxes'].set_color(col)\n",
    "    vp['cmeans'].set_color('black')\n",
    "    if(hatch):\n",
    "        vp['bodies'][0].set_hatch('//')\n",
    "\n",
    "def box_plot(data, edge_color, fill_color, hatch=None, linewidth=1.5, **kwargs):\n",
    "    # Clean data to remove NaNs column-wise\n",
    "    if(np.ndim(data) == 1):\n",
    "        cleaned_data = data[~np.isnan(data)]\n",
    "    else:\n",
    "        cleaned_data = [d[~np.isnan(d)] for d in data]\n",
    "    # Create the box plot with cleaned data\n",
    "    bp = plt.boxplot(cleaned_data, patch_artist=True, **kwargs)\n",
    "    \n",
    "    for element in ['boxes', 'whiskers', 'means', 'medians', 'caps']:\n",
    "        plt.setp(bp[element], color=edge_color, linewidth=linewidth)\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set(facecolor=fill_color, linewidth=linewidth)      \n",
    "        if hatch is not None:\n",
    "            patch.set(hatch=hatch)\n",
    "\n",
    "    return bp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d49c87e-9d2b-4549-beee-b41f4af56a86",
   "metadata": {},
   "source": [
    "## Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "9b627ef6-dc96-4125-be0a-b41beb7ffe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximilianeggl/miniconda3/envs/SBI_DTI/lib/python3.12/site-packages/sbi/utils/user_input_checks_utils.py:389: UserWarning: No prior bounds were passed, consider passing lower_bound\n",
      "            and / or upper_bound if your prior has bounded support.\n",
      "  warnings.warn(\n",
      "/Users/maximilianeggl/miniconda3/envs/SBI_DTI/lib/python3.12/site-packages/sbi/utils/user_input_checks_utils.py:69: UserWarning: Prior is lacking mean attribute, estimating prior mean from samples.\n",
      "  warnings.warn(\n",
      "/Users/maximilianeggl/miniconda3/envs/SBI_DTI/lib/python3.12/site-packages/sbi/utils/user_input_checks_utils.py:80: UserWarning: Prior is lacking variance attribute, estimating prior variance from\n",
      "                samples...\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "network_path = './Networks/'\n",
    "image_path   = './Images/'\n",
    "if not os.path.exists(image_path):\n",
    "    os.makedirs(image_path)\n",
    "NoiseLevels = [None,20,10,5,2]\n",
    "\n",
    "TrainingSamples = 50000\n",
    "InferSamples    = 500\n",
    "\n",
    "lower_abs,upper_abs = -0.07,0.07\n",
    "lower_rest,upper_rest = -0.015,0.015\n",
    "lower_S0 = 25\n",
    "upper_S0 = 2000\n",
    "Save = False\n",
    "\n",
    "TrueCol  = 'k'\n",
    "NoisyCol = 'k'\n",
    "WLSFit   = np.array([225,190,106])/255\n",
    "SBIFit   = np.array([64,176,166])/255\n",
    "\n",
    "Errors_name = ['MD comparison','FA comparison','eig. comparison','Frobenius','Signal comparison','Correlation','Signal comparison','Correlation2']\n",
    "custom_prior = DTIPriorS0(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0)\n",
    "priorS0, *_ = process_prior(custom_prior) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01707d9a-eca9-48ac-8ae1-d830dbaa35c6",
   "metadata": {},
   "source": [
    "## DKI Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579daa3-3e8e-48a5-a203-1c5366e6f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 1\n",
    "fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "\n",
    "data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "# Get the indices of True values\n",
    "true_indices = np.argwhere(mask)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = true_indices.min(axis=0)\n",
    "max_coords = true_indices.max(axis=0)\n",
    "\n",
    "maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "FlatTD = TestData.reshape(maskdata.shape[0]*maskdata.shape[1],138)\n",
    "FlatTD = FlatTD[FlatTD[:,:69].sum(axis=-1)>0]\n",
    "FlatTD = FlatTD[~np.array(FlatTD<0).any(axis=-1)]\n",
    "\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabExt)\n",
    "tenfit = dkimodel.fit(FlatTD)\n",
    "DKIHCP = tenfit.kt\n",
    "DTIHCP = tenfit.lower_triangular()\n",
    "DKIFull = np.array(DKIHCP)\n",
    "DTIFull = np.array(DTIHCP)\n",
    "\n",
    "\n",
    "DTIFilt1 = DTIFull[(abs(DKIFull)<10).all(axis=1)]\n",
    "DKIFilt1 = DKIFull[(abs(DKIFull)<10).all(axis=1)]\n",
    "DTIFilt = DTIFilt1[(DKIFilt1>-3/7).all(axis=1)]\n",
    "DKIFilt = DKIFilt1[(DKIFilt1>-3/7).all(axis=1)]\n",
    "\n",
    "TrueMets = []\n",
    "FA       = []\n",
    "for (dt,kt) in tqdm.tqdm(zip(DTIFilt,DKIFilt)):\n",
    "    TrueMets.append(DKIMetrics(dt,kt))\n",
    "    FA.append(FracAni(np.linalg.eigh(vals_to_mat(dt))[0],np.mean(np.linalg.eigh(vals_to_mat(dt))[0])))\n",
    "TrueMets = np.array(TrueMets)\n",
    "TrueFA = np.array(FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeab163-c71c-4fd2-b94b-5d57afd1ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full fit\n",
    "DT1_full,DT2_full = FitDT(DTIFilt,1)\n",
    "x4_full,R1_full,x2_full,R2_full = FitKT(DKIFilt,1)\n",
    "\n",
    "# LowFA Fit\n",
    "DT1_lfa,DT2_lfa = FitDT(DTIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "x4_lfa,R1_lfa,x2_lfa,R2_lfa = FitKT(DKIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "\n",
    "# HighFA Fit\n",
    "DT1_hfa,DT2_hfa = FitDT(DTIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "x4_hfa,R1_hfa,x2_hfa,R2_hfa = FitKT(DKIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "\n",
    "# UltraLowFA Fit\n",
    "DT1_ulfa,DT2_ulfa = FitDT(DTIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa = FitKT(DKIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "\n",
    "# HigherAK Fit\n",
    "DT1_hak,DT2_hak = FitDT(DTIFilt[TrueMets[:,1]>0.9,:],1)\n",
    "x4_hak,R1_hak,x2_hak,R2_hak = FitKT(DKIFilt[TrueMets[:,1]>0.9,:],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2175941-5649-49c1-884f-2677ff2e0632",
   "metadata": {},
   "source": [
    "# Fig 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f084d3-4650-48e2-a7dc-002eb5244cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_1/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393315f-c34b-49fe-befc-d5133da1d9f6",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6918a4-e713-4f38-9377-74f5f165c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6492be8-3855-4f42-a7f2-4a9229fa67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(6):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices_20 = [0]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(19):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices_20))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices_20], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices_20.append(next_index)\n",
    "\n",
    "selected_indices_20 = np.array(selected_indices_20)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices3 = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP3))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(15):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP3))) - set(selected_indices3))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices3], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices3.append(next_index)\n",
    "\n",
    "selected_indices3 = np.array(selected_indices3)\n",
    "selected_indices3 = selected_indices3[selected_indices3>0]\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices3_48 = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP3))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(28):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP3))) - set(selected_indices3_48))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices3_48], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices3_48.append(next_index)\n",
    "\n",
    "selected_indices3_48= np.array(selected_indices3_48)\n",
    "selected_indices3_48= selected_indices3_48[selected_indices3_48>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be2c830-99f3-4aea-ba3d-59fcadd618dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.linspace(0, 2 * np.pi, 100)\n",
    "v = np.linspace(0, np.pi, 100)\n",
    "\n",
    "x = 4 * np.outer(np.cos(u), np.sin(v))\n",
    "y = 4 * np.outer(np.sin(u), np.sin(v))\n",
    "z = 4 * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "\n",
    "x1 = 2 * np.outer(np.cos(u), np.sin(v))\n",
    "y1 = 2 * np.outer(np.sin(u), np.sin(v))\n",
    "z1 = 2 * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "#for i in range(2):\n",
    "#    ax.plot_surface(x+random.randint(-5,5), y+random.randint(-5,5), z+random.randint(-5,5),  rstride=4, cstride=4, color='b', linewidth=0, alpha=0.5)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(0,0,0,s=50,color='k',label=r'B\\textsubscript{0}')\n",
    "\n",
    "ax.plot_surface(x1, y1, z1,  rstride=4, cstride=4, color=WLSFit, linewidth=0, alpha=0.25)\n",
    "ax.scatter(2*bvecsHCP[np.sum(bvecsHCP,axis=1)!=0][:,0],2*bvecsHCP[np.sum(bvecsHCP,axis=1)!=0][:,1],2*bvecsHCP[np.sum(bvecsHCP,axis=1)!=0][:,2],s=50,\n",
    "           color=WLSFit-0.2,label=r'B = 1000')\n",
    "\n",
    "ax.plot_surface(x, y, z,  rstride=4, cstride=4, color=SBIFit, linewidth=0, alpha=0.25)\n",
    "ax.scatter(4*bvecsHCP3[np.sum(bvecsHCP3,axis=1)!=0][:,0],4*bvecsHCP3[np.sum(bvecsHCP3,axis=1)!=0][:,1],4*bvecsHCP3[np.sum(bvecsHCP3,axis=1)!=0][:,2],s=50,\n",
    "           color=SBIFit-0.2,label=r'B = 3000')\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.legend(ncols=3,fontsize=18,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,loc=2,bbox_to_anchor=(0.05,0.3))\n",
    "plt.title('Full Set \\n 69 measurements',y=0.85,fontsize=24)\n",
    "if Save: plt.savefig(FigLoc+'MultiShell.pdf',format='pdf',bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1e4fe-7420-404c-a284-91c564e5a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.linspace(0, 2 * np.pi, 100)\n",
    "v = np.linspace(0, np.pi, 100)\n",
    "\n",
    "x = 4 * np.outer(np.cos(u), np.sin(v))\n",
    "y = 4 * np.outer(np.sin(u), np.sin(v))\n",
    "z = 4 * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "\n",
    "x1 = 2 * np.outer(np.cos(u), np.sin(v))\n",
    "y1 = 2 * np.outer(np.sin(u), np.sin(v))\n",
    "z1 = 2 * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "#for i in range(2):\n",
    "#    ax.plot_surface(x+random.randint(-5,5), y+random.randint(-5,5), z+random.randint(-5,5),  rstride=4, cstride=4, color='b', linewidth=0, alpha=0.5)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(0,0,0,s=50,color='k',label=r'$B_0$')\n",
    "\n",
    "ax.plot_surface(x1, y1, z1,  rstride=4, cstride=4, color=WLSFit, linewidth=0, alpha=0.25)\n",
    "ax.scatter(2*bvecsHCP[selected_indices,0],2*bvecsHCP[selected_indices,1],2*bvecsHCP[selected_indices,2],s=50,\n",
    "           color=WLSFit-0.2,label=r'$B = 1000$')\n",
    "\n",
    "ax.plot_surface(x, y, z,  rstride=4, cstride=4, color=SBIFit, linewidth=0, alpha=0.25)\n",
    "ax.scatter(4*bvecsHCP3[selected_indices3,0],4*bvecsHCP3[selected_indices3,1],4*bvecsHCP3[selected_indices3,2],s=50,\n",
    "           color=SBIFit-0.2,label=r'$B = 3000$')\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('Minimum Set \\n 22 measurements',y=0.85,fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'MiniSet.pdf',format='pdf',bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81764283-d6cb-45e5-83b9-0f805b978347",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.linspace(0, 2 * np.pi, 100)\n",
    "v = np.linspace(0, np.pi, 100)\n",
    "\n",
    "x = 4 * np.outer(np.cos(u), np.sin(v))\n",
    "y = 4 * np.outer(np.sin(u), np.sin(v))\n",
    "z = 4 * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "\n",
    "x1 = 2 * np.outer(np.cos(u), np.sin(v))\n",
    "y1 = 2 * np.outer(np.sin(u), np.sin(v))\n",
    "z1 = 2 * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "#for i in range(2):\n",
    "#    ax.plot_surface(x+random.randint(-5,5), y+random.randint(-5,5), z+random.randint(-5,5),  rstride=4, cstride=4, color='b', linewidth=0, alpha=0.5)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(0,0,0,s=50,color='k',label=r'$B_0$')\n",
    "\n",
    "ax.plot_surface(x1, y1, z1,  rstride=4, cstride=4, color=WLSFit, linewidth=0, alpha=0.25)\n",
    "ax.scatter(2*bvecsHCP[selected_indices_20,0],2*bvecsHCP[selected_indices_20,1],2*bvecsHCP[selected_indices_20,2],s=50,\n",
    "           color=WLSFit-0.2,label=r'$B = 1000$')\n",
    "\n",
    "ax.plot_surface(x, y, z,  rstride=4, cstride=4, color=SBIFit, linewidth=0, alpha=0.25)\n",
    "ax.scatter(4*bvecsHCP3[selected_indices3_48,0],4*bvecsHCP3[selected_indices3_48,1],4*bvecsHCP3[selected_indices3_48,2],s=50,\n",
    "           color=SBIFit-0.2,label=r'$B = 3000$')\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('Medium Set \\n 48 measurements',y=0.85,fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'MedSet.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4b46b-8d88-4cc6-881a-3412631e9c4b",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b4be9-a46d-45ea-b1db-2eaf50c62041",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial = HemiSphere(xyz=bvecs[1:])\n",
    "hsph_initial20 = HemiSphere(xyz=bvecs[1:20])\n",
    "hsph_initial7 = HemiSphere(xyz=bvecs[1:7])\n",
    "hsph_updated,potentials = disperse_charges(hsph_initial,5000)\n",
    "hsph_updated20,potentials = disperse_charges(hsph_initial20,5000)\n",
    "hsph_updated7,potentials = disperse_charges(hsph_initial7,5000)\n",
    "\n",
    "gtabSimF = gradient_table(np.array([0]+[1000]*64).squeeze(), np.vstack([[0,0,0],hsph_updated.vertices]))\n",
    "gtabSim20 = gradient_table(np.array([0]+[1000]*19).squeeze(), np.vstack([[0,0,0],hsph_updated20.vertices]))\n",
    "gtabSim7 = gradient_table(np.array([0]+[1000]*6).squeeze(), np.vstack([[0,0,0],hsph_updated7.vertices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8cc0e1-dd6e-4da5-b4ff-b655654a4e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIPriorDirec:\n",
    "    def __init__(self, lower_abs : Tensor, upper_abs : Tensor, \n",
    "                       lower_rest: Tensor, upper_rest: Tensor,\n",
    "                        return_numpy: bool = False):\n",
    "\n",
    "        self.dist_abs = BoxUniform(low= lower_abs* torch.ones(3), high=upper_abs * torch.ones(3))\n",
    "        self.dist_rest = BoxUniform(low=lower_rest * torch.ones(3), high=upper_rest *torch.ones(3))\n",
    "        self.direction_choice = Categorical(probs=torch.ones(1, 5))\n",
    "        self.return_numpy = return_numpy\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        \n",
    "        abc  = self.dist_abs.sample(sample_shape)\n",
    "        rest = self.dist_rest.sample(sample_shape)\n",
    "        direc = self.direction_choice.sample(sample_shape)\n",
    "        \n",
    "        if self.return_numpy:   \n",
    "            params = np.hstack([abc,rest,direc]) \n",
    "        else:\n",
    "            params = torch.hstack([abc,rest,direc])\n",
    "\n",
    "        return params\n",
    "        \n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        \n",
    "        abc   = values[:,:3]\n",
    "        rest  = values[:,3:-1]\n",
    "        direc = values[:,-1]\n",
    "\n",
    "        log_prob_abc  = self.dist_abs.log_prob(abc)\n",
    "        log_prob_rest = self.dist_rest.log_prob(rest)\n",
    "        log_prob_direc =  self.direction_choice.log_prob(direc)\n",
    "        return log_prob_abc+log_prob_rest+log_prob_direc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c262be14-5ff5-4a2d-bb0e-15cb73e81e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "gTabs = [gtabSimF]\n",
    "for _ in range(4):\n",
    "    x = np.random.permutation(np.arange(65))\n",
    "    bvecs_shuffle = gtabSimF.bvecs[x]\n",
    "    bvals_shuffle = gtabSimF.bvals[x]\n",
    "    \n",
    "    gTabs.append(gradient_table(bvals_shuffle, bvecs_shuffle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa8745f-894e-4e6c-8f89-e18d3f894aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/EgPosterior.pickle\"):\n",
    "    with open(f\"{network_path}/EgPosterior.pickle\", \"rb\") as handle:\n",
    "        posterior = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        if(np.random.rand()<0.8):\n",
    "            dt = ForceLowFA(dt)\n",
    "        cG = gTabs[int(params[-1])]\n",
    "        Obs.append(CustomSimulator(dt,cG,200,None))\n",
    "        Par.append(np.append(mat_to_vals(dt),params[-1]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/EgPosterior.pickle\"):\n",
    "        with open(f\"{network_path}/EgPosterior.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f51a66-f808-4ffa-927d-46ecaaf8c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "Samples  = []\n",
    "DTISim = []\n",
    "S0Sim    = []\n",
    "\n",
    "params = priorS0.sample([5000])\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim.append(dt)\n",
    "    S0Sim.append(params[i,-1])\n",
    "    Samples.append([CustomSimulator(dt,gtabSimF, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples = np.array(Samples).squeeze()\n",
    "Samples = np.moveaxis(Samples, 0, -1)\n",
    "\n",
    "Samples20  = []\n",
    "DTISim20 = []\n",
    "S0Sim20    = []\n",
    "\n",
    "params = priorS0.sample([5000])\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim20.append(dt)\n",
    "    S0Sim20.append(params[i,-1])\n",
    "    Samples20.append([CustomSimulator(dt,gtabSim20, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples20 = np.array(Samples20).squeeze()\n",
    "Samples20 = np.moveaxis(Samples20, 0, -1)\n",
    "\n",
    "Samples7  = []\n",
    "DTISim7 = []\n",
    "S0Sim7    = []\n",
    "\n",
    "params = priorS0.sample([5000])\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim7.append(dt)\n",
    "    S0Sim7.append(params[i,-1])\n",
    "    Samples7.append([CustomSimulator(dt,gtabSim7, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples7 = np.array(Samples7).squeeze()\n",
    "Samples7 = np.moveaxis(Samples7, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9bbfb-bc71-44fe-a570-349eeb15cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(5)\n",
    "j = np.random.choice(64)\n",
    "gT = gTabs[i]\n",
    "dT = DTISim[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75958b-13c0-4217-8e57-f1b1dccd8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "tObs = CustomSimulator(dT,gT,200,None)\n",
    "posterior_samples_1 = posterior.sample((InferSamples,), x=tObs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413ea88-ffee-44fb-a585-2505058da58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.subplots(figsize=(6,1))\n",
    "    plt.plot(Samples[0][:,i],c=SBIFit,lw=3)\n",
    "    plt.axis('off')\n",
    "    if Save: plt.savefig(FigLoc+'EgSamples'+str(i)+'.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a33e9-f290-4b8e-a729-872eb61139e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32987d63-c3d4-4171-83ef-67e8fdaa0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "j = 20\n",
    "gT = gtabSimF\n",
    "dT = DTISim[j]\n",
    "tObs = CustomSimulator(dT,gT,200,None)\n",
    "posterior_samples_1 = posterior.sample((InferSamples,), x=tObs)\n",
    "\n",
    "signal_dti = CustomSimulator(vals_to_mat([histogram_mode(p) for p in posterior_samples_1.T][:-1]),gT,200,None)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tObs,lw=3,c='k')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'EgInfPre.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tObs,lw=3,c='k',label='True signal')\n",
    "plt.plot(signal_dti,lw=2,c=SBIFit,ls='--',label='Recon. signal')\n",
    "plt.axis('off')\n",
    "legend = plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1.2,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)  # Set the linewidth of legend lines)\n",
    "for handle in legend.get_lines():\n",
    "    handle.set_linewidth(6)  # Set desired linewidth\n",
    "if Save: plt.savefig(FigLoc+'EgInfPost.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c189484-1ff0-421b-86b2-850d6a1c634b",
   "metadata": {},
   "source": [
    "# Fig 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a57a7-839f-4ed8-a6db-44916d4b110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_2/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3230ec7-6c56-4214-be21-ceb4e2661dcd",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f6316-182c-494c-a0be-b0f8e187f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "params = priorS0.sample()\n",
    "dtTruth = ComputeDTI(params)\n",
    "dtTruth = ForceLowFA(dtTruth)\n",
    "Truth = CustomSimulator(dtTruth,gtabSimF,S0=200,snr=None)\n",
    "\n",
    "    \n",
    "dt_evals,dt_evecs = np.linalg.eigh(dtTruth)\n",
    "\n",
    "SNR = [CustomSimulator(dtTruth,gtabSimF, S0=200,snr=scale) for scale in NoiseLevels[1:]]\n",
    "    \n",
    "SNR = np.array(SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d1be1-6712-420c-bb24-451577aa6ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(Truth,'k',lw=2,label='True signal')\n",
    "plt.plot(SNR[0],'gray',lw=2,ls='--',label='Noisy signal')\n",
    "plt.axis('off')\n",
    "legend= plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1.09,1.95),fontsize=28,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "for handle in legend.get_lines():\n",
    "    handle.set_linewidth(6)  # Set desired linewidth\n",
    "if Save: plt.savefig(FigLoc+'EgSig20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(Truth,'k',lw=2)\n",
    "plt.plot(SNR[1],'gray',lw=2,ls='--')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'EgSig10.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(Truth,'k',lw=2)\n",
    "plt.plot(SNR[2],'gray',lw=2,ls='--')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'EgSig5.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(Truth,'k',lw=2)\n",
    "plt.plot(SNR[3],'gray',lw=2,ls='--')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'EgSig2.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578625a-ed1c-447b-98f4-70552aa250e0",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d951cf-2e45-4e22-bced-6c2fb895099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNR20 = np.vstack([CustomSimulator(dtTruth,gtabSimF, S0=200,snr=20) for k in range(100)])\n",
    "SNR10 = np.vstack([CustomSimulator(dtTruth,gtabSimF, S0=200,snr=10) for k in range(100)])\n",
    "SNR5 = np.vstack([CustomSimulator(dtTruth,gtabSimF, S0=200,snr=5) for k in range(100)])\n",
    "SNR2 = np.vstack([CustomSimulator(dtTruth,gtabSimF, S0=200,snr=2) for k in range(100)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55acd95-e235-4a98-a9a7-d93dff2be89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenmodel = dti.TensorModel(gtabSimF,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(SNR20)\n",
    "FA20 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD20 = dti.mean_diffusivity(tenfit.evals)\n",
    "tenfit = tenmodel.fit(SNR10)\n",
    "FA10 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD10 = dti.mean_diffusivity(tenfit.evals)\n",
    "tenfit = tenmodel.fit(SNR5)\n",
    "FA5 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD5 = dti.mean_diffusivity(tenfit.evals)\n",
    "tenfit = tenmodel.fit(SNR2)\n",
    "FA2 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD2 = dti.mean_diffusivity(tenfit.evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a487b83-34c5-4576-9ac2-fc5cb29047b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(6.4,2.4))\n",
    "viol_plot(np.array([FA20,FA10,FA5,FA2]).T,WLSFit)\n",
    "\n",
    "l = plt.axhline(FracAni(dt_evals,np.mean(dt_evals)),c='k',lw=3,ls='--',label='True FA')\n",
    "plt.xticks([1,2,3,4],[20,10,5,2],fontsize=28)\n",
    "plt.xticks(fontsize=28)\n",
    "plt.xlabel('SNR',fontsize=32)\n",
    "plt.ylabel('FA',fontsize=32)\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=WLSFit, edgecolor='k', label='Fit FA'),\n",
    "    Line2D([0], [0], color='k', lw=3, ls='--', label='True FA')\n",
    "]\n",
    "plt.legend(handles=legend_elements,loc = 'lower left',bbox_to_anchor=(0.05,0.5),\n",
    "           fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,ncols=2)\n",
    "plt.yticks([0,0.5,1])\n",
    "if Save: plt.savefig(FigLoc+'EgNoiseFA.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "\n",
    "plt.subplots(figsize=(6.4,2.4))\n",
    "viol_plot(np.array([MD20,MD10,MD5,MD2]).T,WLSFit)\n",
    "\n",
    "l = plt.axhline(np.mean(dt_evals),c='k',lw=3,ls='--',label='True MD')\n",
    "plt.xticks([1,2,3,4],[20,10,5,2],fontsize=28)\n",
    "plt.xticks(fontsize=28)\n",
    "plt.xlabel('SNR',fontsize=32)\n",
    "plt.ylabel('MD',fontsize=32)\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=WLSFit, edgecolor='k', label='Fit MD'),\n",
    "    Line2D([0], [0], color='k', lw=3, ls='--', label='True MD')\n",
    "]\n",
    "plt.legend(handles=legend_elements,loc = 'lower left',bbox_to_anchor=(0.05,0.5),\n",
    "           fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,ncols=2)\n",
    "plt.yticks([0,0.001,0.002])\n",
    "plt.ylim((-7.687787458229293e-05, 0.0025))\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "if Save: plt.savefig(FigLoc+'EgNoiseMD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0278a7c-6b55-4134-82f1-e4b67836db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prior = DTIPriorS0(lower_abs,upper_abs,lower_rest,upper_rest,0,30)\n",
    "priorNoise, *_ = process_prior(custom_prior) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b48034-d829-43ab-a616-126fd0c9d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimFull.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSimF,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTISimFull.pickle\"):\n",
    "        with open(f\"{network_path}/DTISimFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc261ac-4601-4747-b84a-a590e9a0f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "MD20 = []\n",
    "FA20 = []\n",
    "for S in tqdm.tqdm(SNR20):\n",
    "    posterior_samples_1 = posteriorFull.sample((InferSamples,), x=S,show_progress_bars=False)\n",
    "    CustomSimulator(vals_to_mat([histogram_mode(p) for p in posterior_samples_1.T][:-1]),gT,200,None)\n",
    "    Guess = vals_to_mat(posterior_samples_1.mean(axis=0))\n",
    "    Guess_clean = clip_negative_eigenvalues(Guess)\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess_clean)\n",
    "    MD20.append(np.mean(evals_guess_raw))\n",
    "    FA20.append(FracAni(evals_guess_raw,MD20[-1]))\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "MD10 = []\n",
    "FA10 = []\n",
    "for S in tqdm.tqdm(SNR10):\n",
    "    posterior_samples_1 = posteriorFull.sample((InferSamples,), x=S,show_progress_bars=False)\n",
    "    Guess = vals_to_mat(posterior_samples_1.mean(axis=0))\n",
    "    Guess_clean = clip_negative_eigenvalues(Guess)\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess_clean)\n",
    "    if((evals_guess_raw<0).any()): print(True)\n",
    "    MD10.append(np.mean(evals_guess_raw))\n",
    "    FA10.append(FracAni(evals_guess_raw,MD10[-1]))\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "MD5 = []\n",
    "\n",
    "\n",
    "FA5 = []\n",
    "for S in tqdm.tqdm(SNR5):\n",
    "    posterior_samples_1 = posteriorFull.sample((InferSamples,), x=S,show_progress_bars=False)\n",
    "    Guess = vals_to_mat(posterior_samples_1.mean(axis=0))\n",
    "    Guess_clean = clip_negative_eigenvalues(Guess)\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess_clean)\n",
    "    if((evals_guess_raw<0).any()): print(True)\n",
    "    MD5.append(np.mean(evals_guess_raw))\n",
    "    FA5.append(FracAni(evals_guess_raw,MD5[-1]))\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "MD2 = []\n",
    "FA2 = []\n",
    "for S in tqdm.tqdm(SNR2):\n",
    "    posterior_samples_1 = posteriorFull.sample((InferSamples,), x=S,show_progress_bars=False)\n",
    "    Guess = vals_to_mat(posterior_samples_1.mean(axis=0))\n",
    "    Guess_clean = clip_negative_eigenvalues(Guess)\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess_clean)\n",
    "    if((evals_guess_raw<0).any()): print(True)\n",
    "    MD2.append(np.mean(evals_guess_raw))\n",
    "    FA2.append(FracAni(evals_guess_raw,MD2[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219a61e-45e2-493a-af27-689924cabe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(6.4,2.4))\n",
    "viol_plot(np.array([FA20,FA10,FA5,FA2]).T,SBIFit)\n",
    "\n",
    "l = plt.axhline(FracAni(dt_evals,np.mean(dt_evals)),c='k',lw=3,ls='--',label='True FA')\n",
    "plt.xticks([1,2,3,4],[20,10,5,2],fontsize=28)\n",
    "plt.xticks(fontsize=28)\n",
    "plt.xlabel('SNR',fontsize=32)\n",
    "plt.ylabel('FA',fontsize=32)\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=SBIFit, edgecolor='k', label='Fit FA'),\n",
    "    Line2D([0], [0], color='k', lw=3, ls='--', label='True FA')\n",
    "]\n",
    "plt.legend(handles=legend_elements,loc = 'lower left',bbox_to_anchor=(0.05,0.5),\n",
    "           fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,ncols=2)\n",
    "plt.yticks([0,0.5,1])\n",
    "if Save: plt.savefig(FigLoc+'EgNoiseFA_SBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "    \n",
    "plt.subplots(figsize=(6.4,2.4))\n",
    "viol_plot(np.array([MD20,MD10,MD5,MD2]).T,SBIFit)\n",
    "\n",
    "l = plt.axhline(np.mean(dt_evals),c='k',lw=3,ls='--',label='True MD')\n",
    "plt.xticks([1,2,3,4],[20,10,5,2],fontsize=28)\n",
    "plt.xticks(fontsize=28)\n",
    "plt.xlabel('SNR',fontsize=32)\n",
    "plt.ylabel('MD',fontsize=32)\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=SBIFit, edgecolor='k', label='Fit MD'),\n",
    "    Line2D([0], [0], color='k', lw=3, ls='--', label='True MD')\n",
    "]\n",
    "plt.legend(handles=legend_elements,loc = 'lower left',bbox_to_anchor=(-0.05,-0.05),\n",
    "           fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,ncols=2)\n",
    "plt.yticks([0,0.001,0.002])\n",
    "plt.ylim((-7.687787458229293e-05, 0.0025))\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "if Save: plt.savefig(FigLoc+'EgNoiseMD_SBI.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e9d50e-f8c7-40a0-ae50-4f372badfd75",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a93c59-3bc7-41d7-8b40-cc8004e27262",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "ErrorFull = []\n",
    "NoiseApproxFull = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim[i])\n",
    "        tObs = Samples[k,:,i]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSimF, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "        mat_guess = clip_negative_eigenvalues(mat_guess)\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSimF,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApproxFull.append(ENoise)\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "NoiseApproxFull = np.array(NoiseApproxFull)    \n",
    "\n",
    "Error_s = []\n",
    "for k,gtab,Samps,DTIS in zip([65,20,7],[gtabSimF,gtabSim20,gtabSim7],[Samples,Samples20,Samples7],[DTISim,DTISim20,DTISim7]):\n",
    "    tenmodel = dti.TensorModel(gtab,fit_method='NLLS')\n",
    "    Error_n = []\n",
    "    for S,Noise in zip(Samps,NoiseLevels):\n",
    "        Error = []\n",
    "        for i in range(500):\n",
    "            tenfit = tenmodel.fit(S[:,i])\n",
    "            tensor_vals = dti.lower_triangular(tenfit.quadratic_form)\n",
    "            DT_test = vals_to_mat(tensor_vals)\n",
    "            Error.append(Errors(DT_test,DTIS[i],gtab,Samps[0][:,i],S[:,i]))\n",
    "        Error_n.append(Error)\n",
    "    Error_s.append(Error_n)\n",
    "Error_s = np.array(Error_s)\n",
    "Error_s = np.swapaxes(Error_s,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df5aec-f06e-4c1c-b5eb-7b2df1ce243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(9,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(ErrorFull).T,Errors_name)):\n",
    "    plt.sca(a) \n",
    "    bp = box_plot(E[:,1:].T,SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3,2.3,3.3,4.3],showfliers=False,widths=0.3)\n",
    "    bp2 = box_plot(Error_s[1:,0,:,ll],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    plt.sca(a)\n",
    "    if(ll>3):\n",
    "        plt.xlabel('SNR', fontsize=24)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:])\n",
    "    ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.yticks(fontsize=32)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "\n",
    "    if(ll==0):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(ll==1):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=WLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "\n",
    "    plt.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'SimDatDTIErrors1.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef0fe1-bab4-4e2c-9017-ca63926bbbef",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb01e4-7c08-4e0d-ba26-83a413114af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimMin.pickle\", \"rb\") as handle:\n",
    "        posterior7 = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSim7,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior7 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTISimMin.pickle\"):\n",
    "        with open(f\"{network_path}/DTISimMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c3560-9de6-4037-a85d-13253dd668b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "Error7 = []\n",
    "NoiseApprox7 = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim7[i])\n",
    "        tObs = Samples7[k,:,i]\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSim7, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posterior7.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSim7,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApprox7.append(ENoise)\n",
    "    Error7.append(ErrorN2)\n",
    "\n",
    "NoiseApprox7 = np.array(NoiseApprox7)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243f9b28-cfb2-4d77-b1d8-f1a2abd2bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(9,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(Error7).T,Errors_name)):\n",
    "    plt.sca(a) \n",
    "    bp = box_plot(E[:,1:].T,SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3,2.3,3.3,4.3],showfliers=False,widths=0.3)\n",
    "    bp2 = box_plot(Error_s[1:,-1,:,ll],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    plt.sca(a)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.yticks(fontsize=32)\n",
    "    if(ll==1):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        #plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.05),\n",
    "        #           fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(ll==2):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=WLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        #plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "        #           fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    #PlotSig(1,1.3,ymax,ydiff2=ymax*0.01,ydiff1=ymax*0.1)\n",
    "    plt.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'SimDatDTIErrors1_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0923c0-67be-4f86-9b1c-191654ce68f9",
   "metadata": {},
   "source": [
    "# Fig 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29467140-7c3d-4864-98ba-3707b1d34c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_3/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ff76e-544b-4ac4-9d09-4a16812ad61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial = HemiSphere(xyz=bvecs[1:])\n",
    "hsph_updated,_ = disperse_charges(hsph_initial,5000)\n",
    "bvecs = np.vstack([[0,0,0],hsph_updated.vertices])\n",
    "bvalsExt = np.hstack([bvals, 3000*np.ones_like(bvals)])\n",
    "bvecsExt = np.vstack([bvecs, bvecs])\n",
    "bvalsExt[65] = 0\n",
    "gtabSim = gradient_table(bvalsExt, bvecsExt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a12387a-fdfa-449b-a1b2-c138d9092053",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6deaca7-7ee4-45c2-9ff0-8aa01c95c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DKISimFull.pickle\"):\n",
    "    with open(f\"{network_path}/DKISimFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*6000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*2000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*6000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*6000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(2.5*6000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabSim.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabSim,200,np.random.rand()*30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>800).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKISimFull.pickle\"):\n",
    "        with open(f\"{network_path}/DKISimFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98490cad-3984-48db-a92a-8cfbfcc6ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "j = 1\n",
    "vL = torch.tensor([0.2*j])\n",
    "vS = torch.tensor([0.01*j])  \n",
    "\n",
    "kk = np.random.randint(0,4)\n",
    "if(kk==0):\n",
    "    DT,KT = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],2,1)\n",
    "elif(kk==1):\n",
    "    DT,KT = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],2,1)\n",
    "elif(kk==2):\n",
    "    DT,KT = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],2,1)\n",
    "elif(kk==3):\n",
    "    DT,KT = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],2,1)\n",
    "\n",
    "tObs = CustomDKISimulator(DT.squeeze(),KT.squeeze(),gtabSim,200,20)\n",
    "tTrue = CustomDKISimulator(DT.squeeze(),KT.squeeze(),gtabSim,200,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4279dc0-f2d5-4b85-9804-d13ac4882440",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=True)\n",
    "GuessDKI = posterior_samples_1.mean(axis=0)\n",
    "GuessSig = CustomDKISimulator(GuessDKI[:6],GuessDKI[6:],gtabSim,200)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k',label='True signal')\n",
    "plt.plot(GuessSig,lw=2,c=SBIFit,ls='--',label='SBI Recon.')\n",
    "plt.axis('off')\n",
    "legend = plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1.09,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "for handle in legend.get_lines():\n",
    "    handle.set_linewidth(6)  # Set desired linewidth\n",
    "if Save: plt.savefig(FigLoc+'FullReconSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9297bb3a-5f74-4a8c-b54a-0da825073b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodel = dki.DiffusionKurtosisModel(gtabSim,fit_method='NLLS')\n",
    "tenfit = dkimodel.fit(tObs)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k')\n",
    "plt.plot(tenfit.predict(gtabSim,200),lw=2,c=WLSFit,ls='--',label='NLLS Recon.')\n",
    "plt.axis('off')\n",
    "legend = plt.legend(ncols=2,loc=1,bbox_to_anchor =  (0.9,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "for handle in legend.get_lines():\n",
    "    handle.set_linewidth(6)  # Set desired linewidth\n",
    "if Save: plt.savefig(FigLoc+'FullReconWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b90aa2-db10-43e5-8e05-7b3f7b7a4096",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d532987-27d0-4e08-8589-81ff05571d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "Mets = []\n",
    "MetsSBI = []\n",
    "for i in tqdm.tqdm([20,10,5,2]):\n",
    "    m = []\n",
    "    m2 = []\n",
    "    for k in range(50):\n",
    "        tObs = CustomDKISimulator(np.squeeze(DT), np.squeeze(KT),gtabSim, S0=200, snr=i)#\n",
    "        dkimodel = dki.DiffusionKurtosisModel(gtabSim,fit_method='NLLS')\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        m.append(DKIMetrics(tenfit.lower_triangular(),tenfit.kt,False))\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessDKI = posterior_samples_1.mean(axis=0)\n",
    "        m2.append(DKIMetrics(GuessDKI[:6],GuessDKI[6:],False))\n",
    "    Mets.append(m)\n",
    "    MetsSBI.append(m2)\n",
    "Mets = np.array(Mets)\n",
    "MetsSBI = np.array(MetsSBI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5d470-157e-4c6d-9894-60385e4923b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,5,figsize=(22.5,3))\n",
    "for i in range(5):\n",
    "    plt.sca(ax[i])\n",
    "    viol_plot(Mets[:,:,i].T,WLSFit,)\n",
    "    viol_plot(MetsSBI[:,:,i].T,SBIFit,widths=0.3,positions=[1.3,2.3,3.3,4.3],)\n",
    "    ax[i].ticklabel_format(axis='y',style='sci',scilimits=(0,0.001))\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.axhline(DKIMetrics(np.squeeze(DT),np.squeeze(KT),False)[i],lw=3,ls='--',c='k')\n",
    "    #plt.yticks(fontsize=32)\n",
    "\n",
    "    plt.sca(ax[i])\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.yticks(fontsize=32)\n",
    "    if(i==4):\n",
    "        plt.yticks([0,1])\n",
    "\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=WLSFit, edgecolor='k', label='NLLS')\n",
    "]\n",
    "ax[0].legend(handles=legend_elements, ncols=1,loc=1,bbox_to_anchor=(0.8,1),fontsize=32,columnspacing=0.5,handlelength=0.8,handletextpad=0.3)\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=SBIFit, edgecolor='k', label='SBI')\n",
    "]\n",
    "ax[1].legend(handles=legend_elements, ncols=1,loc=1,bbox_to_anchor=(0.8,1),fontsize=32,columnspacing=0.5,handlelength=0.8,handletextpad=0.3)\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='k', lw=3, ls='--', label='True value')\n",
    "]\n",
    "ax[2].legend(handles=legend_elements, ncols=1,loc=1,bbox_to_anchor=(1.1,1.1),fontsize=32,columnspacing=0.5,handlelength=0.8,handletextpad=0.3)\n",
    "\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'EgSigMetricsFull.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf4eaa-1db0-4c54-bf74-51932a3334d8",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4ed0c-871e-408f-8973-55e21e3a6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],1,40)\n",
    "DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],1,40)\n",
    "DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],1,40)\n",
    "DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],1,40)\n",
    "DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,40)\n",
    "\n",
    "SampsDT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "SampsKT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "\n",
    "Samples  = []\n",
    "\n",
    "for Sd,Sk in zip(SampsDT,SampsKT):\n",
    "    Samples.append([CustomDKISimulator(Sd,Sk,gtabSim, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "\n",
    "Samples = np.array(Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00192bd5-9b97-446c-aef2-3b4af01f56e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "ErrorFull = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples[i,k,:]\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessSBI = posterior_samples_1.mean(axis=0)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(GuessSBI[:6],GuessSBI[6:],SampsDT[i],SampsKT[i]))\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "Error_s = []\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabSim,fit_method='NLLS')\n",
    "\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(tenfit.lower_triangular(),tenfit.kt,SampsDT[i],SampsKT[i]))\n",
    "    Error_s.append(ErrorN2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32064e59-73fa-4b8b-b399-bc2433d727ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorFull = np.array(ErrorFull)\n",
    "Error_s = np.array(Error_s)\n",
    "ErrorNames = ['MK Error', 'AK Error', 'RK Error', 'MKT Error', 'KFA Error']\n",
    "fig,ax = plt.subplots(1,5,figsize=(22.5,3))\n",
    "for i in range(5):\n",
    "    plt.sca(ax[i])\n",
    "    box_plot(Error_s[1:,:,i],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,showmeans=False,widths=0.3)\n",
    "    box_plot(ErrorFull[1:,:,i],SBIFit-0.2, np.clip(SBIFit+0.2,0,1),showfliers=False,widths=0.3,positions=[1.3,2.3,3.3,4.3])\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    plt.yticks(fontsize=32)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'ErrorsFull.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d711f3-ecab-4e8e-b514-d7295ae43846",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52920cf-19e4-40ba-bc54-96d26daebcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial15 = HemiSphere(xyz=bvecs[1:16])\n",
    "hsph_initial7 = HemiSphere(xyz=bvecs[1:7])\n",
    "hsph_updated15,_ = disperse_charges(hsph_initial15,5000)\n",
    "hsph_updated7,_ = disperse_charges(hsph_initial7,5000)\n",
    "gtabSimSub = gradient_table(np.array([0]+[1000]*6+[3000]*15).squeeze(), np.vstack([[0,0,0],hsph_updated7.vertices,hsph_updated15.vertices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d51d85c-a04d-4cd0-97b6-13d33e5e0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DKISimMin.pickle\"):\n",
    "    with open(f\"{network_path}/DKISimMin.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*6000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*2000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*6000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*6000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(2.5*6000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabSimSub.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabSimSub,200,np.random.rand()*30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>800).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKISimMin.pickle\"):\n",
    "        with open(f\"{network_path}/DKISimMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6bdf39-6088-45ad-92c2-fa4a6d175318",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "j = 1\n",
    "vL = torch.tensor([0.2*j])\n",
    "vS = torch.tensor([0.01*j])  \n",
    "\n",
    "kk = np.random.randint(0,4)\n",
    "if(kk==0):\n",
    "    DT,KT = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],2,1)\n",
    "elif(kk==1):\n",
    "    DT,KT = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],2,1)\n",
    "elif(kk==2):\n",
    "    DT,KT = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],2,1)\n",
    "elif(kk==3):\n",
    "    DT,KT = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],2,1)\n",
    "\n",
    "tObs = CustomDKISimulator(np.squeeze(DT),np.squeeze(KT),gtabSimSub,200,20)\n",
    "tTrue = CustomDKISimulator(np.squeeze(DT),np.squeeze(KT),gtabSim,200,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b976b7-8ad0-4176-87ab-1a2a4770c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b087be-0056-42ab-93b1-5b0e7439c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "GuessDKI = posterior_samples_1.mean(axis=0)\n",
    "GuessSig = CustomDKISimulator(GuessDKI[:6],GuessDKI[6:],gtabSim,200)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k',label='True signal')\n",
    "plt.plot(GuessSig,lw=2,c=SBIFit,ls='--',label='SBI Recon.')\n",
    "plt.axis('off')\n",
    "plt.fill_betweenx(np.arange(0,500,50),0*np.ones(10),7*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.fill_betweenx(np.arange(0,500,50),64*np.ones(10),79*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.ylim(-9.996985449425491, 209.99985644997255)\n",
    "legend = plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1.12,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "for handle in legend.get_lines():\n",
    "    handle.set_linewidth(6)  # Set desired linewidth\n",
    "if Save: plt.savefig(FigLoc+'7ReconSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae74aee-a22e-45f5-a3b5-49b5a69bd6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "tenfit = dkimodel.fit(tObs)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k')\n",
    "plt.plot(tenfit.predict(gtabSim,200),lw=2,c=WLSFit,ls='--',label='NLLS Recon.')\n",
    "plt.axis('off')\n",
    "legend = plt.legend(ncols=2,loc=1,bbox_to_anchor =  (0.9,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "for handle in legend.get_lines():\n",
    "    handle.set_linewidth(6)  # Set desired linewidth\n",
    "plt.fill_betweenx(np.arange(0,500,50),0*np.ones(10),7*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.fill_betweenx(np.arange(0,500,50),64*np.ones(10),79*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.ylim(-9.996985449425491, 209.99985644997255)\n",
    "if Save: plt.savefig(FigLoc+'7ReconWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1bd02-c673-4fad-afd1-c54363126de5",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff16bf-9a5e-46a8-9453-2842544e3321",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "Mets = []\n",
    "MetsSBI = []\n",
    "for i in tqdm.tqdm([20,10,5,2]):\n",
    "    m = []\n",
    "    m2 = []\n",
    "    for k in range(50):\n",
    "        tObs = CustomDKISimulator(np.squeeze(DT), np.squeeze(KT),gtabSimSub, S0=200, snr=i)#\n",
    "        dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        m.append(DKIMetrics(tenfit.lower_triangular(),tenfit.kt,False))\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessDKI = posterior_samples_1.mean(axis=0)\n",
    "        m2.append(DKIMetrics(GuessDKI[:6],GuessDKI[6:],False))\n",
    "    Mets.append(m)\n",
    "    MetsSBI.append(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffc7d7-4601-4345-9048-72998e1ea099",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mets = np.array(Mets)\n",
    "MetsSBI = np.array(MetsSBI)\n",
    "fig,ax = plt.subplots(1,5,figsize=(22.5,3))\n",
    "for i in range(5):\n",
    "    plt.sca(ax[i])\n",
    "    viol_plot(Mets[:,:,i].T,WLSFit,)\n",
    "    viol_plot(MetsSBI[:,:,i].T,SBIFit,widths=0.3,positions=[1.3,2.3,3.3,4.3],)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.axhline(DKIMetrics(np.squeeze(DT),np.squeeze(KT),False)[i],lw=3,ls='--',c='k')\n",
    "    plt.yticks(fontsize=32)\n",
    "    if(i==4):\n",
    "        plt.yticks([0,1])\n",
    "if Save: plt.savefig(FigLoc+'EgSigMetrics7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f2d41-66ae-4b4c-b234-96fed0f022c2",
   "metadata": {},
   "source": [
    "## f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e098d3e-ecf5-4ee6-80f6-47081d8532a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "Samples7  = []\n",
    "\n",
    "for Sd,Sk in zip(SampsDT,SampsKT):\n",
    "    Samples7.append([CustomDKISimulator(Sd,Sk,gtabSimSub, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "\n",
    "Samples7 = np.array(Samples7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a331976-ed14-407a-8c1d-e2b31abc7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "ErrorFull = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples7[i,k,:]\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessSBI = posterior_samples_1.mean(axis=0)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(GuessSBI[:6],GuessSBI[6:],SampsDT[i],SampsKT[i]))\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "Error_s = []\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples7[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(tenfit.lower_triangular(),tenfit.kt,SampsDT[i],SampsKT[i]))\n",
    "    Error_s.append(ErrorN2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ef157-9dc7-4a2b-958d-d152f3ed5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorFull = np.array(ErrorFull)\n",
    "Error_s = np.array(Error_s)\n",
    "ErrorNames = ['MK Error', 'AK Error', 'RK Error', 'MKT Error', 'KFA Error']\n",
    "fig,ax = plt.subplots(1,5,figsize=(22.5,3))\n",
    "for i in range(5):\n",
    "    plt.sca(ax[i])\n",
    "    box_plot(Error_s[1:,:,i],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    box_plot(ErrorFull[1:,:,i],SBIFit-0.2, np.clip(SBIFit+0.2,0,1),showfliers=False,widths=0.3,positions=[1.3,2.3,3.3,4.3])\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    plt.yticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Errors7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b079e1-c405-462b-9bb9-39721d97377a",
   "metadata": {},
   "source": [
    "# Fig 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8f1ba-84f2-4343-abdf-84a506446661",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_4/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c18fa-4118-42ea-9a33-1f8995ad7dc5",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd27b2-186a-4907-bdbc-bf83f58585b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdwi = './HCP_data/Pat'+str(1)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(1)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(1)+'/bvecs_1k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = [0]+selected_indices\n",
    "\n",
    "bvalsHCP7 = bvalsHCP[selected_indices]\n",
    "bvecsHCP7 = bvecsHCP[selected_indices]\n",
    "gtabHCP7 = gradient_table(bvalsHCP7, bvecsHCP7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1861ad70-3e54-4b54-9a07-fc1659a6d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prior = DTIPriorS0Noise(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0,0,30)\n",
    "priorS0Noise, *_ = process_prior(custom_prior) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fac026-4927-4faf-b4c3-71a24acf8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIHCPFull.pickle\"):\n",
    "    with open(f\"{network_path}/DTIHCPFull.pickle\", \"rb\") as handle:\n",
    "        posterior2 = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    bvals = gtabHCP.bvals\n",
    "    bvecs = gtabHCP.bvecs\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorS0.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        dt = ForceLowFA(dt)\n",
    "        Obs.append(CustomSimulator(dt,gtabHCP,params[-1],np.random.rand()*30 + 20))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior2 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{save_path}/DTIHCPFull.pickle\"):\n",
    "        with open(f\"{save_path}/DTIHCPFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior2, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f119a07e-f7b4-4e84-bc05-ec56593d0443",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArrShape = maskdata[:,:,axial_middle,0].shape\n",
    "NoiseEst = np.zeros([55,64,7])\n",
    "VarEst   = np.zeros([55,64])\n",
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        torch.manual_seed(10)\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            np.random.seed(1)\n",
    "            torch.manual_seed(1)\n",
    "            posterior_samples_1 = posterior2.sample((InferSamples,), x=maskdata[i,j,axial_middle,:],show_progress_bars=False)\n",
    "            NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb0973-b879-4d45-986b-8d143387ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(55):\n",
    "    for j in range(64):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1]])\n",
    "MD_SBIFull = np.zeros([55,64])\n",
    "FA_SBIFull = np.zeros([55,64])\n",
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "        MD_SBIFull[i,j] = np.mean(Eigs)\n",
    "        FA_SBIFull[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "FA_SBIFull[np.isnan(FA_SBIFull)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146872e2-8862-493e-aa15-630326e6513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenmodel = dti.TensorModel(gtabHCP,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(maskdata[:,:,axial_middle])\n",
    "FAFull = dti.fractional_anisotropy(tenfit.evals)\n",
    "MDFull = dti.mean_diffusivity(tenfit.evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a66437c-abd6-4ff9-9476-c5fe93432779",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            FAFull[i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c966ad9-e02b-4822-ae36-f037e2d6de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(MD_SBIFull.T,cmap='gray')\n",
    "plt.axis('off')\n",
    "vmin, vmax = img.get_clim()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_MD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c9f41b-7ba1-41e2-939c-d8e437671633",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MDFull.T,cmap='gray',vmin=vmin, vmax=vmax)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_MD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d091f16e-561a-495a-a8b6-679abc97dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MD_SBIFull.T-MDFull.T\n",
    "data[~mask[:,:,axial_middle].T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=np.nanmin(data), vcenter=0, vmax=np.nanmax(data))\n",
    "plt.imshow(data,cmap='seismic',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "ticks = [np.nanmin(data), 0, np.nanmax(data)]  # Adjust the number of ticks as needed\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_MD_Diff.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9b72f-45d8-41b3-b030-0de13cf00cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.abs(MD_SBIFull*mask[:,:,axial_middle]-MDFull*mask[:,:,axial_middle])))\n",
    "ssim_noise = ssim(MD_SBIFull, MDFull, data_range=np.max([MD_SBIFull.max(),MDFull.max()])-np.min([MD_SBIFull.min(),MDFull.min()]))\n",
    "print(ssim_noise)\n",
    "Num = 2*np.abs(MD_SBIFull*mask[:,:,axial_middle]-MDFull*mask[:,:,axial_middle])\n",
    "Den = np.abs(MDFull*mask[:,:,axial_middle])+np.abs(MD_SBIFull*mask[:,:,axial_middle])\n",
    "print(np.nanmedian(Num/Den))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5695e966-9ed5-4ae0-a22e-b998f0f296f6",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98661a9-7d30-4e64-8716-f6ed92053481",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIHCPMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTIHCPMin.pickle\", \"rb\") as handle:\n",
    "        posterior7_2 = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    bvals = gtabHCP.bvals\n",
    "    bvecs = gtabHCP.bvecs\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorS0.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        dt = ForceLowFA(dt)\n",
    "        Obs.append(CustomSimulator(dt,gtabHCP7,params[-1],np.random.rand()*30 + 20))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior7_2 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{save_path}/DTIHCPMin.pickle\"):\n",
    "        with open(f\"{save_path}/DTIHCPMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7_2, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fcd07-8d70-4dbf-93e4-b672ddcaba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArrShape = maskdata[:,:,axial_middle,0].shape\n",
    "NoiseEst = np.zeros([55,64,7])\n",
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        torch.manual_seed(10)\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            posterior_samples_1 = posterior7_2.sample((InferSamples,), x=maskdata[i,j,axial_middle,selected_indices],show_progress_bars=False)\n",
    "            NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742850a0-e986-48f0-a400-79624555eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(55):\n",
    "    for j in range(64):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1]])\n",
    "MD_SBI7 = np.zeros([55,64])\n",
    "FA_SBI7 = np.zeros([55,64])\n",
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "        MD_SBI7[i,j] = np.mean(Eigs)\n",
    "        FA_SBI7[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "FA_SBI7[np.isnan(FA_SBI7)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067bcc4-e8d9-408c-8d15-2e6ee1b9fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenmodel = dti.TensorModel(gtabHCP7,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(maskdata[:,:,axial_middle,selected_indices])\n",
    "FA7 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD7 = dti.mean_diffusivity(tenfit.evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529803a8-74e2-44a3-94c4-5bac3ad7ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            FA7[i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b987c-3dfc-4e83-b02d-f03f6801614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(MD_SBI7.T,cmap='gray')\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_MD_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d398649-abb6-4c4e-8b02-e575abf05c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MD7.T,cmap='gray',vmin=vmin, vmax=vmax)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_MD_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3ef39-b15d-4076-80a7-e9f030476c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs(MD_SBIFull.T-MD_SBI7.T)\n",
    "data[~mask[:,:,axial_middle].T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.nanmax(data)/2, vmax=np.nanmax(data))\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'DTI_MDSBIErr.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.nanmax(data)/2, vmax=np.nanmax(data))\n",
    "ticks = [0, np.round(np.nanmax(data),3)]  # Adjust the number of ticks as needed\n",
    "data = np.abs(MDFull.T-MD7.T)\n",
    "data[~mask[:,:,axial_middle].T] = np.nan\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'DTI_MDWLSErr.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac25cad-973d-40a9-884b-7604e4bc0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.abs(MD_SBIFull*mask[:,:,axial_middle]-MD_SBI7*mask[:,:,axial_middle])))\n",
    "ssim_noise = ssim(MD_SBIFull*mask[:,:,axial_middle], MD_SBI7*mask[:,:,axial_middle], data_range=MD_SBIFull.max()-MD_SBIFull.min())\n",
    "print(ssim_noise)\n",
    "Num = 2*np.abs(MD_SBIFull*mask[:,:,axial_middle]-MD_SBI7*mask[:,:,axial_middle])\n",
    "Den = np.abs(MD_SBI7*mask[:,:,axial_middle])+np.abs(MD_SBIFull*mask[:,:,axial_middle])\n",
    "print(np.nanmedian(Num/Den))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb9453b-89fe-4acc-939d-b09887da7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.abs(MDFull*mask[:,:,axial_middle]-MD7*mask[:,:,axial_middle])))\n",
    "ssim_noise = ssim(MDFull*mask[:,:,axial_middle], MD7*mask[:,:,axial_middle], data_range=MDFull.max()-MDFull.min())\n",
    "print(ssim_noise)\n",
    "Num = 2*np.abs(MDFull*mask[:,:,axial_middle]-MD7*mask[:,:,axial_middle])\n",
    "Den = np.abs(MD7*mask[:,:,axial_middle])+np.abs(MDFull*mask[:,:,axial_middle])\n",
    "print(np.nanmedian(Num/Den))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2e8d4-affd-429f-90b1-38b63f0a5d4f",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a84cc-6f5b-4c4f-86d6-784b3448a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(FA_SBIFull.T,cmap='gray')\n",
    "plt.axis('off')\n",
    "vmin, vmax = img.get_clim()\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_FA.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46f2af-7467-413c-9423-ff3191dbd40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(FAFull.T,cmap='gray')\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "tick_labels = ['{:.2e}'.format(t) for t in cbar.get_ticks()]\n",
    "cbar.set_ticks(cbar.get_ticks())\n",
    "cbar.set_ticklabels(tick_labels)\n",
    "cbar.update_ticks()\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_FA.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ad1b7-a270-4e64-971e-b486b5d1178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FA_SBIFull.T-FAFull.T\n",
    "data[~mask[:,:,axial_middle].T] = np.nan\n",
    "plt.imshow(data,cmap='seismic',vmin=-1, vmax=1)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "ticks = [-1, 0, 1]  # Adjust the number of ticks as needed\n",
    "cbar.set_ticks(ticks)\n",
    "if Save: plt.savefig(FigLoc+'HCP_FA_Diff.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480c563-007b-43db-9686-dab4a89c5d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.abs(FA_SBIFull*mask[:,:,axial_middle]-FAFull*mask[:,:,axial_middle])))\n",
    "ssim_noise = ssim(FA_SBIFull, FAFull, data_range=np.max([FA_SBIFull.max(),FAFull.max()])-np.min([FA_SBIFull.min(),FAFull.min()]))\n",
    "print(ssim_noise)\n",
    "Num = 2*np.abs(FA_SBIFull*mask[:,:,axial_middle]-FAFull*mask[:,:,axial_middle])\n",
    "Den = np.abs(FAFull*mask[:,:,axial_middle])+np.abs(FA_SBIFull*mask[:,:,axial_middle])\n",
    "print(np.nanmedian(Num/Den))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097378d3-bd31-4c46-a53d-f2d730bf2755",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d478309-a4cf-466c-8035-2b4e04936c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(FA_SBI7.T,cmap='gray')\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_FA_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43272bdc-b629-4b89-becc-2f884b8635df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(FA7.T,cmap='gray',vmin=vmin, vmax=vmax)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_FA_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f222f-b192-49e0-a2dc-f7c45bd2fb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs(FA_SBIFull.T-FA_SBI7.T)\n",
    "data[~mask[:,:,axial_middle].T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=0.5, vmax=1)\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'DTI_FASBIErr.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=0.5, vmax=1)\n",
    "ticks = [0, 1]  # Adjust the number of ticks as needed\n",
    "data = np.abs(FAFull.T-FA7.T)\n",
    "data[~mask[:,:,axial_middle].T] = np.nan\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "\n",
    "cbar.set_ticks(ticks)\n",
    "if Save: plt.savefig(FigLoc+'DTI_FAWLSErr.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd934c0-2497-4c59-8fb1-d3ea59b68904",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.abs(FA_SBIFull*mask[:,:,axial_middle]-FA_SBI7*mask[:,:,axial_middle])))\n",
    "ssim_noise = ssim(FA_SBIFull*mask[:,:,axial_middle], FA_SBI7*mask[:,:,axial_middle], data_range=FA_SBIFull.max()-FA_SBIFull.min())\n",
    "print(ssim_noise)\n",
    "Num = 2*np.abs(FA_SBIFull*mask[:,:,axial_middle]-FA_SBI7*mask[:,:,axial_middle])\n",
    "Den = np.abs(FA_SBI7*mask[:,:,axial_middle])+np.abs(FA_SBIFull*mask[:,:,axial_middle])\n",
    "print(np.nanmedian(Num/Den))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105674a-f2c2-4e03-af6d-8e65334fad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.abs(FAFull*mask[:,:,axial_middle]-FA7*mask[:,:,axial_middle])))\n",
    "ssim_noise = ssim(FAFull*mask[:,:,axial_middle], FA7*mask[:,:,axial_middle], data_range=FAFull.max()-FAFull.min())\n",
    "print(ssim_noise)\n",
    "Num = 2*np.abs(FAFull*mask[:,:,axial_middle]-FA7*mask[:,:,axial_middle])\n",
    "Den = np.abs(FA7*mask[:,:,axial_middle])+np.abs(FAFull*mask[:,:,axial_middle])\n",
    "print(np.nanmedian(Num/Den))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5323b2-b1dd-4d92-b51a-e756092f4485",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c62af2-ae9f-42db-818e-899a571daa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prior = DTIPriorS0Direc(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0)\n",
    "priorDirec, *_ = process_prior(custom_prior) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a777b1-6e4e-4b7b-86c4-55443abda5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gTabsF = []\n",
    "gTabs20 = []\n",
    "gTabs7 = []\n",
    "\n",
    "Indices20 = []\n",
    "Indices7  = []\n",
    "FullDat   = []\n",
    "\n",
    "Masks = []\n",
    "AMs   = []\n",
    "WMs   = []\n",
    "for i in tqdm.tqdm(range(1,6)):\n",
    "    fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    gTabsF.append(gtabHCP)\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "\n",
    "    Masks.append(mask)\n",
    "    AMs.append(axial_middle)\n",
    "    TestData = maskdata[:, :, axial_middle, :]\n",
    "    FlatTD = TestData.reshape(maskdata.shape[0]*maskdata.shape[1],69)\n",
    "    FlatTD = FlatTD[FlatTD.sum(axis=-1)>0]\n",
    "    FlatTD = FlatTD[~np.array(FlatTD<0).any(axis=-1)]\n",
    "    FullDat.append(FlatTD)\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    distance_matrix = squareform(pdist(bvecsHCP))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(6):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    bvalsHCP7 = bvalsHCP[selected_indices]\n",
    "    bvecsHCP7 = bvecsHCP[selected_indices]\n",
    "    gtabHCP7 = gradient_table(bvalsHCP7, bvecsHCP7)\n",
    "\n",
    "    gTabs7.append(gtabHCP7)\n",
    "    Indices7.append(selected_indices)\n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    distance_matrix = squareform(pdist(bvecsHCP))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(19):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    \n",
    "    bvalsHCP20 = bvalsHCP[selected_indices]\n",
    "    bvecsHCP20 = bvecsHCP[selected_indices]\n",
    "    gtabHCP20 = gradient_table(bvalsHCP20, bvecsHCP20)\n",
    "    gTabs20.append(gtabHCP20)\n",
    "    Indices20.append(selected_indices)\n",
    "\n",
    "    WM, affine, img = load_nifti('./HCP_data/c2vol0000_'+str(kk+1)+'.nii', return_img=True)\n",
    "    WM, affine = reslice(WM, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    \n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    WMs.append(WM[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa2ee7-57e9-41dd-ba6d-ae393e6d5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIMultiHCPFull.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMultiHCPFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(5*TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-2])\n",
    "        dt = ForceLowFA(dt)\n",
    "        cG = gTabsF[int(params[-1])]\n",
    "        Obs.append(np.hstack((1000*params[-1],CustomSimulator(dt,cG,params[-2],np.random.rand()*30 + 20))))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-2]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorDirec)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMultiHCPFull.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMultiHCPFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb311102-360c-47f2-a859-d749596f38d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIMultiHCPMid.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMultiHCPMid.pickle\", \"rb\") as handle:\n",
    "        posteriorMid = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(5*TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-2])\n",
    "        dt = ForceLowFA(dt)\n",
    "        cG = gTabs20[int(params[-1])]\n",
    "        Obs.append(np.hstack((1000*params[-1],CustomSimulator(dt,cG,params[-2],np.random.rand()*30 + 20))))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-2]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorDirec)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorMid = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMultiHCPMid.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMultiHCPMid.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorMid, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb6a97-ad5b-4247-adc4-36462cc9c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIMultiHCPMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMultiHCPMin.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(5*TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-2])\n",
    "        dt = ForceLowFA(dt)\n",
    "        cG = gTabs7[int(params[-1])]\n",
    "        Obs.append(np.hstack((1000*params[-1],CustomSimulator(dt,cG,params[-2],np.random.rand()*30 + 20))))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-2]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorDirec)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorMin = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMultiHCPMin.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMultiHCPMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorMin, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c3c4a-f25a-47c2-9062-9f4880077ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDFullArr = []\n",
    "FAFullArr = []\n",
    "for kk in range(5):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    ArrShape = maskdata[:,:,axial_middle,0].shape\n",
    "    NoiseEst = np.zeros(list(maskdata.shape[:2])+[7])\n",
    "    VarEst   = np.zeros(maskdata.shape[:2])\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            torch.manual_seed(10)\n",
    "            if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                np.random.seed(1)\n",
    "                torch.manual_seed(1)\n",
    "                posterior_samples_1 = posteriorFull.sample((InferSamples,), x=np.hstack([kk*1000,maskdata[i,j,axial_middle,:]]),show_progress_bars=False)\n",
    "                NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1:]])\n",
    "    MD_SBIFull = np.zeros(ArrShape)\n",
    "    FA_SBIFull = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "            MD_SBIFull[i,j] = np.mean(Eigs)\n",
    "            FA_SBIFull[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "    FA_SBIFull[np.isnan(FA_SBIFull)] = 0\n",
    "    MDFullArr.append(MD_SBIFull)\n",
    "    FAFullArr.append(FA_SBIFull)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,3,figsize=(14.4,6.4))\n",
    "    ax[0].imshow(maskdata[:,:,axial_middle,0].T)\n",
    "    plt.sca(ax[1])\n",
    "    img = plt.imshow(MD_SBIFull.T,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.sca(ax[2])\n",
    "    img = plt.imshow(FA_SBIFull.T,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9f058-7ad1-41a2-a2a2-792f0e93cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDMidArr = []\n",
    "FAMidArr = []\n",
    "for kk in range(5):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    ArrShape = maskdata[:,:,axial_middle,0].shape\n",
    "    NoiseEst = np.zeros(list(maskdata.shape[:2])+[7])\n",
    "    VarEst   = np.zeros(maskdata.shape[:2])\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            torch.manual_seed(10)\n",
    "            if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                np.random.seed(1)\n",
    "                torch.manual_seed(1)\n",
    "                posterior_samples_1 = posteriorMid.sample((InferSamples,), x=np.hstack([kk*1000,maskdata[i,j,axial_middle,Indices20[kk]]]),show_progress_bars=False)\n",
    "                NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1:]])\n",
    "    MD_SBIFull = np.zeros(ArrShape)\n",
    "    FA_SBIFull = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "            MD_SBIFull[i,j] = np.mean(Eigs)\n",
    "            FA_SBIFull[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "    FA_SBIFull[np.isnan(FA_SBIFull)] = 0\n",
    "    MDMidArr.append(MD_SBIFull)\n",
    "    FAMidArr.append(FA_SBIFull)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,3,figsize=(14.4,6.4))\n",
    "    ax[0].imshow(maskdata[:,:,axial_middle,0].T)\n",
    "    plt.sca(ax[1])\n",
    "    img = plt.imshow(MD_SBIFull.T,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.sca(ax[2])\n",
    "    img = plt.imshow(FA_SBIFull.T,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ffa715-995a-4ee6-9b2a-295e13bd7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD7Arr = []\n",
    "FA7Arr = []\n",
    "for kk in range(5):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    ArrShape = maskdata[:,:,axial_middle,0].shape\n",
    "    NoiseEst = np.zeros(list(maskdata.shape[:2])+[7])\n",
    "    VarEst   = np.zeros(maskdata.shape[:2])\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            torch.manual_seed(10)\n",
    "            if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                np.random.seed(1)\n",
    "                torch.manual_seed(1)\n",
    "                posterior_samples_1 = posteriorMin.sample((InferSamples,), x=np.hstack([kk*1000,maskdata[i,j,axial_middle,Indices7[kk]]]),show_progress_bars=False)\n",
    "                NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1:]])\n",
    "    MD_SBIFull = np.zeros(ArrShape)\n",
    "    FA_SBIFull = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "            MD_SBIFull[i,j] = np.mean(Eigs)\n",
    "            FA_SBIFull[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "    FA_SBIFull[np.isnan(FA_SBIFull)] = 0\n",
    "    MD7Arr.append(MD_SBIFull)\n",
    "    FA7Arr.append(FA_SBIFull)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,3,figsize=(14.4,6.4))\n",
    "    ax[0].imshow(maskdata[:,:,axial_middle,0].T)\n",
    "    plt.sca(ax[1])\n",
    "    img = plt.imshow(MD_SBIFull.T,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.sca(ax[2])\n",
    "    img = plt.imshow(FA_SBIFull.T,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21f2a0-fef4-482c-89c9-1248689cac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDFullNLArr = []\n",
    "FAFullNLArr = []\n",
    "for kk in range(5):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    ArrShape = maskdata[:,:,axial_middle,0].shape\n",
    "    tenmodel = dti.TensorModel(gtabHCP,return_S0_hat = True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(maskdata[:,:,axial_middle])\n",
    "    FAFull = dti.fractional_anisotropy(tenfit.evals)\n",
    "    MDFull = dti.mean_diffusivity(tenfit.evals)\n",
    "    MDFullNLArr.append(MDFull)\n",
    "    FAFullNLArr.append(FAFull)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,3,figsize=(14.4,6.4))\n",
    "    ax[0].imshow(maskdata[:,:,axial_middle,0].T)\n",
    "    plt.sca(ax[1])\n",
    "    img = plt.imshow(MDFull.T,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.sca(ax[2])\n",
    "    img = plt.imshow(FAFull.T,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d346c-27ae-4c44-be72-00699b937d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD20NLArr = []\n",
    "FA20NLArr = []\n",
    "for kk in range(5):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP20 = gradient_table(bvalsHCP[np.array(Indices20)[kk]], bvecsHCP[np.array(Indices20)[kk]])\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    ArrShape = maskdata[:,:,axial_middle,0].shape\n",
    "    tenmodel = dti.TensorModel(gtabHCP20,return_S0_hat = True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(maskdata[:,:,axial_middle,np.array(Indices20)[kk]])\n",
    "    FAFull = dti.fractional_anisotropy(tenfit.evals)\n",
    "    MDFull = dti.mean_diffusivity(tenfit.evals)\n",
    "    MD20NLArr.append(MDFull)\n",
    "    FA20NLArr.append(FAFull)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,3,figsize=(14.4,6.4))\n",
    "    ax[0].imshow(maskdata[:,:,axial_middle,0].T)\n",
    "    plt.sca(ax[1])\n",
    "    img = plt.imshow(MDFull.T,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.sca(ax[2])\n",
    "    img = plt.imshow(FAFull.T,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505bd1b-b2b5-4b0a-8c08-8021f5d726a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD7NLArr = []\n",
    "FA7NLArr = []\n",
    "for kk in range(5):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP7 = gradient_table(bvalsHCP[np.array(Indices7)[kk]], bvecsHCP[np.array(Indices7)[kk]])\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    ArrShape = maskdata[:,:,axial_middle,0].shape\n",
    "    tenmodel = dti.TensorModel(gtabHCP7,return_S0_hat = True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(maskdata[:,:,axial_middle,np.array(Indices7)[kk]])\n",
    "    FAFull = dti.fractional_anisotropy(tenfit.evals)\n",
    "    MDFull = dti.mean_diffusivity(tenfit.evals)\n",
    "    MD7NLArr.append(MDFull)\n",
    "    FA7NLArr.append(FAFull)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,3,figsize=(14.4,6.4))\n",
    "    ax[0].imshow(maskdata[:,:,axial_middle,0].T)\n",
    "    plt.sca(ax[1])\n",
    "    img = plt.imshow(MDFull.T,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.sca(ax[2])\n",
    "    img = plt.imshow(FAFull.T,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb7d4a-1c1a-4b14-a34d-e6f3067a7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccM7 = []\n",
    "for i in range(5):\n",
    "    M7 = MD7Arr[i]\n",
    "    MF = MDFullArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    AccM7.append(np.mean(np.abs(M7-MF)*Ma))\n",
    "\n",
    "AccM20 = []\n",
    "for i in range(5):\n",
    "    M7 = MDMidArr[i]\n",
    "    MF = MDFullArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    AccM20.append(np.mean(np.abs(M7-MF)*Ma))\n",
    "\n",
    "AccMFulls = []\n",
    "for i in range(5):\n",
    "    M7 = MDFullArr[i]\n",
    "    MF = MDFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    AccMFulls.append(np.mean(np.abs(M7-MF)*Ma))\n",
    "\n",
    "AccM7NL = []\n",
    "for i in range(5):\n",
    "    M7 = MD7NLArr[i]\n",
    "    MF = MDFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    AccM7NL.append(np.mean(np.abs(M7-MF)*Ma))\n",
    "\n",
    "AccM20NL = []\n",
    "for i in range(5):\n",
    "    M7 = MD20NLArr[i]\n",
    "    MF = MDFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    AccM20NL.append(np.mean(np.abs(M7-MF)*Ma))\n",
    "\n",
    "PrecM7 = []\n",
    "for i in range(5):\n",
    "    M1 = MD7Arr[i]\n",
    "    WM = WMs[i]\n",
    "    PrecM7.append(np.std(M1[WM[:,:,AMs[i]].astype(bool)]))\n",
    "\n",
    "PrecM20 = []\n",
    "for i in range(5):\n",
    "    M1 = MDMidArr[i]\n",
    "    WM = WMs[i]\n",
    "    PrecM20.append(np.std(M1[WM[:,:,AMs[i]].astype(bool)]))\n",
    "    \n",
    "PrecMFull = []\n",
    "for i in range(5):\n",
    "    M1 = MDFullArr[i]\n",
    "    WM = WMs[i]\n",
    "    PrecMFull.append(np.std(M1[WM[:,:,AMs[i]].astype(bool)]))\n",
    "\n",
    "PrecM7NL = []\n",
    "for i in range(5):\n",
    "    M1 = MD7NLArr[i]\n",
    "    WM = WMs[i]\n",
    "    PrecM7NL.append(np.std(M1[WM[:,:,AMs[i]].astype(bool)]))\n",
    "\n",
    "PrecM20NL = []\n",
    "for i in range(5):\n",
    "    M1 = MD20NLArr[i]\n",
    "    WM = WMs[i]\n",
    "    PrecM20NL.append(np.std(M1[WM[:,:,AMs[i]].astype(bool)]))\n",
    "    \n",
    "PrecMFullNL = []\n",
    "for i in range(5):\n",
    "    M1 = MDFullNLArr[i]\n",
    "    WM = WMs[i]\n",
    "    PrecMFullNL.append(np.std(M1[WM[:,:,AMs[i]].astype(bool)]))\n",
    "\n",
    "SSIM7 = []\n",
    "for i in range(5):\n",
    "    M7 = MD7Arr[i]\n",
    "    MF = MDFullArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    SSIM7.append(ssim(M7*Ma,MF*Ma,data_range=np.max(M7)-np.min(M7)))\n",
    "\n",
    "SSIM20 = []\n",
    "for i in range(5):\n",
    "    M7 = MDMidArr[i]\n",
    "    MF = MDFullArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    SSIM20.append(ssim(M7*Ma,MF*Ma,data_range=np.max(M7)-np.min(M7)))\n",
    "\n",
    "SSIMFulls = []\n",
    "for i in range(5):\n",
    "    M7 = MDFullArr[i]\n",
    "    MF = MDFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    SSIMFulls.append(ssim(M7*Ma,MF*Ma,data_range=np.max(M7)-np.min(M7)))\n",
    "\n",
    "SSIM7NL = []\n",
    "for i in range(5):\n",
    "    M7 = MD7NLArr[i]\n",
    "    MF = MDFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    SSIM7NL.append(ssim(M7*Ma,MF*Ma,data_range=np.max(M7)-np.min(M7)))\n",
    "\n",
    "SSIM20NL = []\n",
    "for i in range(5):\n",
    "    M7 = MD20NLArr[i]\n",
    "    MF = MDFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    SSIM20NL.append(ssim(M7*Ma,MF*Ma,data_range=np.max(M7)-np.min(M7)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be337c47-82d2-4faf-91f8-cbd20af28241",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1)#, sharex=True)\n",
    "fig.subplots_adjust(hspace=0.05)  # adjust space between Axes\n",
    "\n",
    "# Plotting on ax1\n",
    "plt.sca(ax1)\n",
    "plt.scatter(np.ones(5), SSIMFulls, c='gray', zorder=10, alpha=0.5)\n",
    "box_plot(np.array(SSIMFulls), 'black', 'gray', widths=0.3)\n",
    "\n",
    "plt.scatter(1.7 * np.ones(5), SSIM20, c='gray', zorder=10, alpha=0.5)\n",
    "box_plot(np.array(SSIM20), SBIFit - 0.2, np.clip(SBIFit + 0.2, 0, 1), positions=[1.7], widths=0.3)\n",
    "\n",
    "plt.scatter(2 * np.ones(5), SSIM7, c='gray', zorder=10, alpha=0.5)\n",
    "box_plot(np.array(SSIM7), SBIFit - 0.2, np.clip(SBIFit + 0.2, 0, 1), positions=[2], widths=0.3)\n",
    "\n",
    "plt.scatter(2.8 * np.ones(5), SSIM20NL, c='gray', zorder=10, alpha=0.5)\n",
    "box_plot(np.array(SSIM20NL), WLSFit - 0.2, np.clip(WLSFit + 0.2, 0, 1), positions=[2.8], widths=0.3)\n",
    "\n",
    "plt.scatter(3.1 * np.ones(5), SSIM7NL, c='gray', zorder=10, alpha=0.5)\n",
    "box_plot(np.array(SSIM7NL), WLSFit - 0.2, np.clip(WLSFit + 0.2, 0, 1), positions=[3.1], widths=0.3)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "plt.yticks(fontsize=32)\n",
    "# Plotting on ax2\n",
    "plt.sca(ax2)\n",
    "plt.scatter(np.ones(5), SSIMFulls, c='gray', zorder=10, alpha=0.5)\n",
    "box_plot(np.array(SSIMFulls), 'black', 'gray', widths=0.3)\n",
    "\n",
    "plt.scatter(1.7 * np.ones(5), SSIM20, c='gray', zorder=10, alpha=0.5)\n",
    "box_plot(np.array(SSIM20), SBIFit - 0.2, np.clip(SBIFit + 0.2, 0, 1), positions=[1.7], widths=0.3)\n",
    "\n",
    "plt.scatter(2 * np.ones(5), SSIM7, c='gray', zorder=10, alpha=0.5)\n",
    "box_plot(np.array(SSIM7), SBIFit - 0.2, np.clip(SBIFit + 0.2, 0, 1), positions=[2], widths=0.3)\n",
    "\n",
    "plt.scatter(2.8 * np.ones(5), SSIM20NL, c='gray', zorder=10, alpha=0.5)\n",
    "box_plot(np.array(SSIM20NL), WLSFit - 0.2, np.clip(WLSFit + 0.2, 0, 1), positions=[2.8], widths=0.3)\n",
    "\n",
    "plt.scatter(3.1 * np.ones(5), SSIM7NL, c='gray', zorder=10, alpha=0.5)\n",
    "box_plot(np.array(SSIM7NL), WLSFit - 0.2, np.clip(WLSFit + 0.2, 0, 1), positions=[3.1], widths=0.3)\n",
    "\n",
    "plt.axhline(0.66, lw=3, ls='--', c='k')\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "\n",
    "ax1.set_ylim(.86, 1.)  # outliers only\n",
    "ax2.set_ylim(0, .7)  # most of the data\n",
    "\n",
    "ax2.set_xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "ax1.set_xticks([]) \n",
    "plt.yticks(fontsize=32)\n",
    "\n",
    "d = .5  # proportion of vertical to horizontal extent of the slanted line\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12,\n",
    "              linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "if Save:\n",
    "    plt.savefig(FigLoc + 'DTIHCP_SSIM_MD.pdf', format='PDF', transparent=True, bbox_inches='tight')\n",
    "\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0fd4c0-7c04-4daa-9edb-dafbc44945a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c57de-2be1-45e7-a291-3b0fadfdd479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OOMFormatter(ScalarFormatter):\n",
    "    def __init__(self, order=0, fformat=\"%1.1e\", offset=True, mathText=False):\n",
    "        self.oom = order\n",
    "        self.fformat = fformat\n",
    "        ScalarFormatter.__init__(self, useOffset=offset, useMathText=mathText)\n",
    "\n",
    "    def _set_orderOfMagnitude(self, nothing):\n",
    "        self.orderOfMagnitude = self.oom\n",
    "\n",
    "    def _set_format(self):\n",
    "        self.format = self.fformat\n",
    "        if self._useMathText:\n",
    "            self.format = self.format  # Using raw string for math formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9079239-bbae-4076-88e2-d9a3c934a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('text', usetex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237bcb5-fac3-47ba-a4ad-2c285cc4bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "plt.sca(ax1)\n",
    "plt.scatter(np.ones(5),AccMFulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccMFulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),AccM20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),AccM7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),AccM20NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM20NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),AccM7NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM7NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax1.set_ylim(0.00012, 0.0015)\n",
    "ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "ax1.yaxis.set_ticks(np.arange(0.0002, 0.0016, 0.0004))\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.sca(ax2)\n",
    "plt.scatter(np.ones(5),AccMFulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccMFulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),AccM20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),AccM7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),AccM20NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM20NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),AccM7NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM7NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax2.set_ylim(0, 0.00009)\n",
    "ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "ax2.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "ax2.set_xticklabels(['Full', 'Mid', 'Min', 'Mid', 'Min'], fontsize=32, rotation=90)\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "\n",
    "# Show plot\n",
    "if Save:\n",
    "    plt.savefig(FigLoc + 'DTIHCP_Acc_MD.pdf', format='PDF', transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43907c4-c96d-432f-90fd-a8ca57b65b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "plt.sca(ax1)\n",
    "plt.scatter(np.ones(5),PrecMFull,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecMFull),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),widths=0.3)\n",
    "plt.scatter(1.3*np.ones(5),PrecM20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecM20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3],widths=0.3)\n",
    "plt.scatter(1.6*np.ones(5),PrecM7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecM7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.6],widths=0.3)\n",
    "x = np.arange(0.7,2,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(PrecMFull)\n",
    "y2 = np.ones_like(x)*0.9*np.median(PrecMFull)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "plt.scatter(2.5*np.ones(5),PrecMFullNL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecMFullNL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.5],widths=0.3)\n",
    "plt.scatter(2.8*np.ones(5),PrecM20NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecM20NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),PrecM7NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecM7NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=32)\n",
    "plt.xticks([1,1.3,1.6,2.5,2.8,3.1],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "x = np.arange(2.2,3.5,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(PrecMFullNL)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(PrecMFullNL)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax1.set_ylim(0.001, 0.003)\n",
    "ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax1.yaxis.set_ticks(np.arange(0.0002, 0.0016, 0.0004))\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.sca(ax2)\n",
    "plt.scatter(np.ones(5),PrecMFull,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecMFull),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),widths=0.3)\n",
    "plt.scatter(1.3*np.ones(5),PrecM20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecM20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3],widths=0.3)\n",
    "plt.scatter(1.6*np.ones(5),PrecM7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecM7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.6],widths=0.3)\n",
    "x = np.arange(0.7,2,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(PrecMFull)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(PrecMFull)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "plt.scatter(2.5*np.ones(5),PrecMFullNL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecMFullNL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.5],widths=0.3)\n",
    "plt.scatter(2.8*np.ones(5),PrecM20NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecM20NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),PrecM7NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecM7NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=32)\n",
    "plt.xticks([1,1.3,1.6,2.5,2.8,3.1],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "x = np.arange(2.2,3.5,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(PrecMFullNL)\n",
    "y2 = np.ones_like(x)*0.9*np.median(PrecMFullNL)\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax2.set_ylim(0.0004,0.0007)\n",
    "ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "#ax2.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "plt.xticks([1,1.3,1.6,2.5,2.8,3.1],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DTIHCP_Prec_MD.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d538dd-be85-45cc-93ab-87061ee14021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bootstrap\n",
    "data = (SSIM7,)  # samples must be in a sequence\n",
    "res = bootstrap(data, np.mean, confidence_level=0.9,\n",
    "                random_state=0)\n",
    "print(np.mean(data))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358588e6-774e-4e83-b6d5-fc9883c4e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccM7 = []\n",
    "for i in range(5):\n",
    "    M7 = FA7Arr[i]\n",
    "    MF = FAFullArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    AccM7.append(np.mean(np.abs(M7-MF)*Ma))\n",
    "\n",
    "AccM20 = []\n",
    "for i in range(5):\n",
    "    M7 = FAMidArr[i]\n",
    "    MF = FAFullArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    AccM20.append(np.mean(np.abs(M7-MF)*Ma))\n",
    "\n",
    "AccMFulls = []\n",
    "for i in range(5):\n",
    "    M7 = FAFullArr[i]\n",
    "    MF = FAFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    AccMFulls.append(np.mean(np.abs(M7-MF)*Ma))\n",
    "\n",
    "AccM7NL = []\n",
    "for i in range(5):\n",
    "    M7 = FA7NLArr[i]\n",
    "    MF = FAFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    AccM7NL.append(np.mean(np.abs(M7-MF)*Ma))\n",
    "\n",
    "AccM20NL = []\n",
    "for i in range(5):\n",
    "    M7 = FA20NLArr[i]\n",
    "    MF = FAFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    AccM20NL.append(np.mean(np.abs(M7-MF)*Ma))\n",
    "SSIM7 = []\n",
    "for i in range(5):\n",
    "    M7 = FA7Arr[i]\n",
    "    MF = FAFullArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    SSIM7.append(ssim(M7*Ma,MF*Ma,data_range=np.max(M7)-np.min(M7)))\n",
    "\n",
    "SSIM20 = []\n",
    "for i in range(5):\n",
    "    M7 = FAMidArr[i]\n",
    "    MF = FAFullArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    SSIM20.append(ssim(M7*Ma,MF*Ma,data_range=np.max(M7)-np.min(M7)))\n",
    "\n",
    "SSIMFulls = []\n",
    "for i in range(5):\n",
    "    M7 = FAFullArr[i]\n",
    "    MF = FAFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    SSIMFulls.append(ssim(M7*Ma,MF*Ma,data_range=np.max(M7)-np.min(M7)))\n",
    "\n",
    "SSIM7NL = []\n",
    "for i in range(5):\n",
    "    M7 = FA7NLArr[i]\n",
    "    MF = FAFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    SSIM7NL.append(ssim(M7*Ma,MF*Ma,data_range=np.max(M7)-np.min(M7)))\n",
    "\n",
    "SSIM20NL = []\n",
    "for i in range(5):\n",
    "    M7 = FA20NLArr[i]\n",
    "    MF = FAFullNLArr[i]\n",
    "    Ma = Masks[i][:,:,AMs[i]]\n",
    "    SSIM20NL.append(ssim(M7*Ma,MF*Ma,data_range=np.max(M7)-np.min(M7)))\n",
    "PrecM7 = []\n",
    "for i in range(5):\n",
    "    M1 = FA7Arr[i]\n",
    "    WM = WMs[i]\n",
    "    PrecM7.append(np.std(M1[WM[:,:,AMs[i]].astype(bool)]))\n",
    "\n",
    "PrecM20 = []\n",
    "for i in range(5):\n",
    "    M1 = FAMidArr[i]\n",
    "    WM = WMs[i]\n",
    "    PrecM20.append(np.std(M1[WM[:,:,AMs[i]].astype(bool)]))\n",
    "    \n",
    "PrecMFull = []\n",
    "for i in range(5):\n",
    "    M1 = FAFullArr[i]\n",
    "    WM = WMs[i]\n",
    "    PrecMFull.append(np.std(M1[WM[:,:,AMs[i]].astype(bool)]))\n",
    "\n",
    "PrecM7NL = []\n",
    "for i in range(5):\n",
    "    M1 = FA7NLArr[i]\n",
    "    WM = WMs[i]\n",
    "    PrecM7NL.append(np.std(M1[WM[:,:,AMs[i]].astype(bool)]))\n",
    "\n",
    "PrecM20NL = []\n",
    "for i in range(5):\n",
    "    M1 = FA20NLArr[i]\n",
    "    WM = WMs[i]\n",
    "    PrecM20NL.append(np.std(M1[WM[:,:,AMs[i]].astype(bool)]))\n",
    "    \n",
    "PrecMFullNL = []\n",
    "for i in range(5):\n",
    "    M1 = FAFullNLArr[i]\n",
    "    WM = WMs[i]\n",
    "    PrecMFullNL.append(np.std(M1[WM[:,:,AMs[i]].astype(bool)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d32bca1-4b7a-4f67-8ede-052b81d7c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "plt.sca(ax1)\n",
    "plt.scatter(np.ones(5),AccMFulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccMFulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),AccM20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),AccM7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),AccM20NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM20NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),AccM7NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM7NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax1.set_ylim(0.1, 0.5)\n",
    "#ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax1.yaxis.set_ticks(np.arange(0.0002, 0.0016, 0.0004))\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.sca(ax2)\n",
    "plt.scatter(np.ones(5),AccMFulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccMFulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),AccM20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),AccM7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),AccM20NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM20NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),AccM7NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(AccM7NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax2.set_ylim(0.0,0.08)\n",
    "#ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "#ax2.set_xticks([1, 1.7, 2, 2.8, 3.1])\n",
    "ax2.set_xticklabels(['Full', 'Mid', 'Min', 'Mid', 'Min'], fontsize=32, rotation=90)\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DTIHCP_Acc_FA.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0071fa5-fa54-4c06-9ed3-53cc4409fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.ones(5),SSIMFulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIMFulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),SSIM20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),SSIM7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),SSIM20NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM20NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),SSIM7NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM7NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "plt.axhline(0.66,lw=3,ls='--',c='k')\n",
    "plt.yticks(fontsize=32)\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.ylim([0,1])\n",
    "if Save: plt.savefig(FigLoc+'DTIHCP_SSIM_FA.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6d0f7-4f1e-499c-a229-82880c5328c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.ones(5),PrecMFull,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecMFull),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),widths=0.3)\n",
    "plt.scatter(1.3*np.ones(5),PrecM20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecM20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3],widths=0.3)\n",
    "plt.scatter(1.6*np.ones(5),PrecM7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecM7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.6],widths=0.3)\n",
    "\n",
    "plt.scatter(2.5*np.ones(5),PrecMFullNL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecMFullNL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.5],widths=0.3)\n",
    "plt.scatter(2.8*np.ones(5),PrecM20NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecM20NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),PrecM7NL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(PrecM7NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.3,1.6,2.5,2.8,3.1],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=32)\n",
    "\n",
    "x = np.arange(0.7,2,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(PrecMFull)\n",
    "y2 = np.ones_like(x)*0.9*np.median(PrecMFull)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(2.2,3.5,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(PrecMFullNL)\n",
    "y2 = np.ones_like(x)*0.9*np.median(PrecMFullNL)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DTIHCP_Prec_FA.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47abe78-d1cc-4af0-82e1-38a49b53d5dc",
   "metadata": {},
   "source": [
    "# Fig 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d414e5c9-86c7-472e-88e6-470d239c39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_5/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794079f9-1bf8-481c-8b96-2af4415ae2d2",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b02eed-32af-450e-8510-f7d0cfd6180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "_, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "\n",
    "\n",
    "data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "# Get the indices of True values\n",
    "true_indices = np.argwhere(mask)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = true_indices.min(axis=0)\n",
    "max_coords = true_indices.max(axis=0)\n",
    "\n",
    "maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "axial_middle = maskdata.shape[2] // 2\n",
    "maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf93df-9b51-4a39-a131-25622ba07181",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIHCPFull.pickle\"):\n",
    "    with open(f\"{network_path}/DKIHCPFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(13000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(13000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(26000))   \n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT2,DT3,DT5])\n",
    "    KT = np.vstack([KT2,KT3,KT5])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([4*13000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabExt.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabExt,S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    if not os.path.exists(f\"{network_path}/DKIHCPFull.pickle\"):\n",
    "        with open(f\"{network_path}/DKIHCPFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n",
    "\n",
    "ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "NoiseEst = np.zeros([62, 68 ,22])\n",
    "torch.manual_seed(10)\n",
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            posterior_samples_1 = posteriorFull.sample((InferSamples,), x=TestData4D[i,j,axial_middle,:],show_progress_bars=False)\n",
    "            NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7193684-88ca-4c15-b9e0-b0bd8cdfad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae35385-9e3c-4f70-9de9-e486cb622612",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK_SBIFull  = np.zeros([62, 68])\n",
    "AK_SBIFull  = np.zeros([62, 68])\n",
    "RK_SBIFull  = np.zeros([62, 68])\n",
    "MKT_SBIFull = np.zeros([62, 68])\n",
    "KFA_SBIFull = np.zeros([62, 68])\n",
    "for i in tqdm.tqdm(range(62)):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "        MK_SBIFull[i,j] = Metrics[0]\n",
    "        AK_SBIFull[i,j] = Metrics[1]\n",
    "        RK_SBIFull[i,j] = Metrics[2]\n",
    "        MKT_SBIFull[i,j] = Metrics[3]\n",
    "        KFA_SBIFull[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8828df-b482-4538-ae17-7c8738b719ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodelNL = dki.DiffusionKurtosisModel(gtabExt,fit_method='NLLS')\n",
    "dkifitNL = dkimodelNL.fit(TestData[:,:,:])\n",
    "MK_NLFull  = np.zeros([62, 68])\n",
    "AK_NLFull  = np.zeros([62, 68])\n",
    "RK_NLFull  = np.zeros([62, 68])\n",
    "MKT_NLFull = np.zeros([62, 68])\n",
    "KFA_NLFull = np.zeros([62, 68])\n",
    "for i in range(62):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt)\n",
    "        MK_NLFull[i,j] = Metrics[0]\n",
    "        AK_NLFull[i,j] = Metrics[1]\n",
    "        RK_NLFull[i,j] = Metrics[2]\n",
    "        MKT_NLFull[i,j] = Metrics[3]\n",
    "        KFA_NLFull[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56944c23-031e-4725-8799-2d3189d4d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "KFA_SBIFull[np.isnan(KFA_SBIFull)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314332f4-0efd-4e54-8d6c-2af60ef20a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKSBIFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'AKSBIFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'RKSBIFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MKT_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKTSBIFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((KFA_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'KFASBIFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7b7b0-dc2b-4949-a685-8ea77370e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKNLFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'AKNLFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'RKNLFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MKT_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKTNLFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((KFA_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'KFANLFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c558de52-056a-4e88-bfc4-60dca119fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for S,N in zip([MK_NLFull,AK_NLFull,RK_NLFull,MKT_NLFull,KFA_NLFull],[MK_SBIFull,AK_SBIFull,RK_SBIFull,MKT_SBIFull,KFA_SBIFull]):\n",
    "    print('==')\n",
    "    print(np.mean(np.abs(S*mask2[:,:,axial_middle]-N*mask2[:,:,axial_middle])))\n",
    "    ssim_noise = ssim(S, N, data_range=np.max([S.max(),N.max()])-np.min([S.min(),N.min()]))\n",
    "    print(ssim_noise)\n",
    "    Num = 2*np.abs(S*mask2[:,:,axial_middle]-N*mask2[:,:,axial_middle])\n",
    "    Den = np.abs(N*mask2[:,:,axial_middle])+np.abs(S*mask2[:,:,axial_middle])\n",
    "    print(np.nanmean(Num/Den))\n",
    "    print('==')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e9e12-717e-45c5-96f8-337da3144797",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d688d-6d34-4dba-b97d-68bffe27b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices7 = [0]+selected_indices\n",
    "\n",
    "bvalsHCP7_1 = bvalsHCP[selected_indices7]\n",
    "bvecsHCP7_1 = bvecsHCP[selected_indices7]\n",
    "\n",
    "i=3\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(14):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx = []\n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "true_indx = selected_indices7+[t+69 for t in true_indx]\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297ace3-8f33-4a4c-ba27-fd10f66968f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIHCPMin.pickle\"):\n",
    "    with open(f\"{network_path}/DKIHCPMin.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(3*13000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(3*13000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(3*26000))   \n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT2,DT3,DT5])\n",
    "    KT = np.vstack([KT2,KT3,KT5])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([3*52000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabHCP7.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabHCP7,S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    if not os.path.exists(f\"{network_path}/DKIHCPMinA2.pickle\"):\n",
    "        with open(f\"{network_path}/DKIHCPMinA2.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n",
    "ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "NoiseEst7 = np.zeros([62, 68 ,22])\n",
    "torch.manual_seed(10)\n",
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            posterior_samples_1 = posteriorFull.sample((100,), x=TestData4D[i,j,axial_middle,true_indx],show_progress_bars=False)\n",
    "            NoiseEst7[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29e4b9-40a8-43dc-ae04-a77948486a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst7[i,j]))),NoiseEst7[i,j,6:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94935529-ca31-41e9-90b1-fbbf4cd5ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK_SBI7  = np.zeros([62, 68])\n",
    "AK_SBI7  = np.zeros([62, 68])\n",
    "RK_SBI7  = np.zeros([62, 68])\n",
    "MKT_SBI7 = np.zeros([62, 68])\n",
    "KFA_SBI7 = np.zeros([62, 68])\n",
    "for i in tqdm.tqdm(range(62)):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "        MK_SBI7[i,j] = Metrics[0]\n",
    "        AK_SBI7[i,j] = Metrics[1]\n",
    "        RK_SBI7[i,j] = Metrics[2]\n",
    "        MKT_SBI7[i,j] = Metrics[3]\n",
    "        KFA_SBI7[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae32349-bb89-4108-93a0-c3f2cf19b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodelNL = dki.DiffusionKurtosisModel(gtabHCP7,fit_method='NLLS')\n",
    "dkifitNL = dkimodelNL.fit(TestData[:,:,true_indx])\n",
    "MK_NL7  = np.zeros([62, 68])\n",
    "AK_NL7  = np.zeros([62, 68])\n",
    "RK_NL7 = np.zeros([62, 68])\n",
    "MKT_NL7 = np.zeros([62, 68])\n",
    "KFA_NL7 = np.zeros([62, 68])\n",
    "for i in range(62):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt)\n",
    "        MK_NL7[i,j] = Metrics[0]\n",
    "        AK_NL7[i,j] = Metrics[1]\n",
    "        RK_NL7[i,j] = Metrics[2]\n",
    "        MKT_NL7[i,j] = Metrics[3]\n",
    "        KFA_NL7[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08637ae-0db4-4c24-b053-7ab5414078ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "KFA_SBIFull[np.isnan(KFA_SBIFull)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e0c80-c2bb-48f3-8532-1377243cf248",
   "metadata": {},
   "outputs": [],
   "source": [
    "KFA_SBI7[np.isnan(KFA_SBI7)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd1310-3161-4b5f-844c-09fea495b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_SBI7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKSBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_SBI7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'AKSBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_SBI7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'RKSBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MKT_SBI7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKTSBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((KFA_SBI7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'KFASBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf037902-4952-4fc0-9fc4-e8e43824303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_NL7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKNL7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_NL7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'AKNL7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_NL7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'RKNL7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MKT_NL7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKTNL7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((KFA_NL7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'KFANL7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f211213a-75b2-4285-a4cd-e10979afa45e",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c3479-4701-4e37-9043-22d1760ac3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = [0,1,2]\n",
    "data = np.abs((MK_SBIFull-MK_SBI7)*mask2[:,:,axial_middle]).T\n",
    "data[~mask2[:,:,axial_middle].T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=1, vmax=2)\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(ticks=ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'MKDiffSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = np.abs((AK_SBIFull-AK_SBI7)*mask2[:,:,axial_middle]).T\n",
    "data[~mask2[:,:,axial_middle].T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.nanmax(data)/2, vmax=np.nanmax(data))\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(ticks=ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'AKDiffSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = np.abs((RK_SBIFull-RK_SBI7)*mask2[:,:,axial_middle]).T\n",
    "data[~mask2[:,:,axial_middle].T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=1,vmax=2)\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'RKDiffSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "ticks = [0,1,2]\n",
    "data = np.abs((MKT_SBIFull-MKT_SBI7)*mask2[:,:,axial_middle]).T\n",
    "data[~mask2[:,:,axial_middle].T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=1, vmax=2)\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()#ticks=ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'MKTDiffSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = np.abs((KFA_SBIFull-KFA_SBI7)*mask2[:,:,axial_middle]).T\n",
    "data[~mask2[:,:,axial_middle].T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.nanmax(data)/2, vmax=np.nanmax(data))\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'KFADiffSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8955987-8d1a-4732-90e4-bfe2588178ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs((MK_NLFull-MK_NL7)*mask2[:,:,axial_middle]).T\n",
    "data[~mask2[:,:,axial_middle].T] = np.nan\n",
    "plt.imshow(data,cmap='Reds',vmin=0,vmax=2)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(ticks=[0,1,2])\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'MKDiffWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "dat = np.abs((AK_SBIFull-AK_SBI7)*mask2[:,:,axial_middle]).T\n",
    "data[~mask2[:,:,axial_middle].T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.nanmax(dat)/2, vmax=np.nanmax(dat))\n",
    "data = np.abs((AK_NLFull-AK_NL7)*mask2[:,:,axial_middle]).T\n",
    "data[~mask2[:,:,axial_middle].T] = np.nan\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()#ticks=ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'AKDiffWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=1,vmax=2)\n",
    "data = np.abs((RK_NLFull-RK_NL7)*mask2[:,:,axial_middle]).T\n",
    "data[~mask2[:,:,axial_middle].T] = np.nan\n",
    "#ticks = [0, np.round(np.max(data),10)]  #Adjust the number of ticks as needed\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'RKDiffWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = np.abs((MKT_NLFull-MKT_NL7)*mask2[:,:,axial_middle]).T\n",
    "data[~mask2[:,:,axial_middle].T] = np.nan\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'MKTDiffWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "dat = np.abs((KFA_SBIFull-KFA_SBI7)*mask2[:,:,axial_middle]).T\n",
    "data[~mask2[:,:,axial_middle].T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.nanmax(dat)/2, vmax=np.nanmax(dat))\n",
    "data = np.abs((KFA_NLFull-KFA_NL7)*mask2[:,:,axial_middle]).T\n",
    "data[~mask2[:,:,axial_middle].T] = np.nan\n",
    "ticks = [0, np.round(np.max(data),10)]  #Adjust the number of ticks as needed\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'KFADiffWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4ce7e-743b-4c48-8e39-90ae590731cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for S,N in zip([MK_SBI7,AK_SBI7,RK_SBI7,MKT_SBI7,KFA_SBI7],[MK_SBIFull,AK_SBIFull,RK_SBIFull,MKT_SBIFull,KFA_SBIFull]):\n",
    "    print('==')\n",
    "    print(np.mean(np.abs(S*mask2[:,:,axial_middle]-N*mask2[:,:,axial_middle])))\n",
    "    ssim_noise = ssim(S, N, data_range=N.max()-N.min())\n",
    "    print(ssim_noise)\n",
    "    Num = 2*np.abs(S*mask2[:,:,axial_middle]-N*mask2[:,:,axial_middle])\n",
    "    Den = np.abs(N*mask2[:,:,axial_middle])+np.abs(S*mask2[:,:,axial_middle])\n",
    "    print(np.nanmean(Num/Den))\n",
    "    print('==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ad512-e8f2-4f3f-87e2-a2f57039f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "for S,N in zip([MK_NL7,AK_NL7,RK_NL7,MKT_NL7,KFA_NL7],[MK_NLFull,AK_NLFull,RK_NLFull,MKT_NLFull,KFA_NLFull]):\n",
    "    print('==')\n",
    "    print(np.mean(np.abs(S*mask2[:,:,axial_middle]-N*mask2[:,:,axial_middle])))\n",
    "    ssim_noise = ssim(S, N, data_range=N.max()-N.min())\n",
    "    print(ssim_noise)\n",
    "    Num = 2*np.abs(S*mask2[:,:,axial_middle]-N*mask2[:,:,axial_middle])\n",
    "    Den = np.abs(N*mask2[:,:,axial_middle])+np.abs(S*mask2[:,:,axial_middle])\n",
    "    print(np.nanmean(Num/Den))\n",
    "    print('==')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84152946-277c-49e9-b554-bfa0ee803742",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ea91b-f3b5-4147-9e82-f9175a04d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrueIndxs = []\n",
    "gTabs7 = []\n",
    "for i in range(1,6):\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [1]\n",
    "    distance_matrix = squareform(pdist(bvecsHCP))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices7 = [0]+selected_indices\n",
    "    \n",
    "    bvalsHCP7_1 = bvalsHCP[selected_indices7]\n",
    "    bvecsHCP7_1 = bvecsHCP[selected_indices7]\n",
    "    \n",
    "    i=3\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    \n",
    "    temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "    temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "    distance_matrix = squareform(pdist(temp_bvecs))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(14):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "    bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "    \n",
    "    gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "    \n",
    "    true_indx = []\n",
    "    for b in bvecsHCP7_3:\n",
    "        true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "    true_indx = selected_indices7+[t+69 for t in true_indx]\n",
    "    TrueIndxs.append(true_indx)\n",
    "    gTabs7.append(gtabHCP7)\n",
    "\n",
    "TrueIndxs20 = []\n",
    "gTabs20 = []\n",
    "for i in range(1,6):\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [1]\n",
    "    distance_matrix = squareform(pdist(bvecsHCP))\n",
    "\n",
    "    temp_bvecs = bvecsHCP[bvalsHCP>0]\n",
    "    temp_bvals = bvalsHCP[bvalsHCP>0]\n",
    "    distance_matrix = squareform(pdist(temp_bvecs))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(18):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices7 = selected_indices\n",
    "    \n",
    "    bvalsHCP7_1 = np.insert(temp_bvals[selected_indices7],0,0)\n",
    "    bvecsHCP7_1 = np.insert(temp_bvecs[selected_indices7],0,[0,0,0],axis=0)\n",
    "    \n",
    "    i=3\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    \n",
    "    temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "    temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "    distance_matrix = squareform(pdist(temp_bvecs))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(27):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "    bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "    \n",
    "    gtabHCP20 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "    \n",
    "    true_indx_one = []\n",
    "    for b in bvecsHCP7_1:\n",
    "        true_indx_one.append(np.linalg.norm(b-bvecsHCP,axis=1).argmin())\n",
    "    true_indx = []        \n",
    "    for b in bvecsHCP7_3:\n",
    "        true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "    true_indx = true_indx_one+[t+69 for t in true_indx]\n",
    "    TrueIndxs20.append(true_indx)\n",
    "    gTabs20.append(gtabHCP20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c27000-4679-40a4-b71f-e0dc25b73405",
   "metadata": {},
   "outputs": [],
   "source": [
    "gTabsE = []\n",
    "\n",
    "for i in tqdm.tqdm(range(1,6)):\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    gTabsE.append(gtabExt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7bdee7-722f-4a25-9aa1-1dc3ecbe0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIMultiHCPFull.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiHCPFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    #DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(500))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(3*16250))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(3*16250))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(3*32500))   \n",
    "    \n",
    "    DT = np.vstack([DT5,DT2,DT3])\n",
    "    KT = np.vstack([KT5,KT2,KT3])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([3*65000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    A  = np.random.choice(5,3*65000)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabsPermE[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabsE[A[i]],S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    A = np.array(A).reshape(len(A),1)\n",
    "    Par = np.hstack([DT,KT,S0])#,A])\n",
    "    Obs = np.hstack([1000*A,Obs])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 50)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiHCPFull.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiHCPFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e26efa-e62f-4611-9ab1-8fc2c7727fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIMultiHCPMid.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiHCPMid.pickle\", \"rb\") as handle:\n",
    "        posterior20 = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    #DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(500))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(3*16250))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(3*16250))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(3*32500))   \n",
    "    \n",
    "    DT = np.vstack([DT5,DT2,DT3])\n",
    "    KT = np.vstack([KT5,KT2,KT3])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([3*65000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    A  = np.random.choice(5,3*65000)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabs20[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabs20[A[i]],S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    A = np.array(A).reshape(len(A),1)\n",
    "    Par = np.hstack([DT,KT,S0])#,A])\n",
    "    Obs = np.hstack([1000*A,Obs])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 50)\n",
    "    posterior20 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiHCPMid.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiHCPMid.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior20, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3fac70-d36e-41e2-8300-01811bab71d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIMultiHCPMin.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiHCPMin.pickle\", \"rb\") as handle:\n",
    "        posterior7 = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    #DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(500))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(3*16250))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(3*16250))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(3*32500))   \n",
    "    \n",
    "    DT = np.vstack([DT5,DT2,DT3])\n",
    "    KT = np.vstack([KT5,KT2,KT3])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([3*65000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    A  = np.random.choice(5,3*65000)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabs7[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabs7[A[i]],S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    A = np.array(A).reshape(len(A),1)\n",
    "    Par = np.hstack([DT,KT,S0])#,A])\n",
    "    Obs = np.hstack([1000*A,Obs])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 50)\n",
    "    posterior7 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiHCPMin.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiHCPMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40301c1-3023-4174-9846-1b1833d2251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullArr = []\n",
    "RKFullArr = []\n",
    "AKFullArr = []\n",
    "MKTFullArr = []\n",
    "KFAFullArr = []\n",
    "\n",
    "MK7Arr = []\n",
    "RK7Arr = []\n",
    "AK7Arr = []\n",
    "MKT7Arr = []\n",
    "KFA7Arr = []\n",
    "for kk in range(5):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk+1)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk+1)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk+1)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    \n",
    "    ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "    NoiseEst = np.zeros(list(ArrShape)+[22])\n",
    "    NoiseEst7 = np.zeros(list(ArrShape)+[22])\n",
    "    torch.manual_seed(10)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                posterior_samples_1 = posteriorFull.sample((InferSamples,), x=np.hstack([kk*1000,TestData4D[i,j,axial_middle,:]]),show_progress_bars=False)\n",
    "                NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "                \n",
    "                posterior_samples_7 = posterior7.sample((InferSamples,), x=np.hstack([kk*1000,TestData4D[i,j,axial_middle,TrueIndxs[kk]]]),show_progress_bars=False)\n",
    "                NoiseEst7[i,j] = np.array([histogram_mode(p) for p in posterior_samples_7.T])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    NoiseEst7_2 =  np.zeros_like(NoiseEst7)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):  \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])\n",
    "            NoiseEst7_2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst7[i,j]))),NoiseEst7[i,j,6:]])\n",
    "    \n",
    "    MK_SBIFull  = np.zeros(ArrShape)\n",
    "    AK_SBIFull  = np.zeros(ArrShape)\n",
    "    RK_SBIFull  = np.zeros(ArrShape)\n",
    "    MKT_SBIFull = np.zeros(ArrShape)\n",
    "    KFA_SBIFull = np.zeros(ArrShape)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK_SBIFull[i,j] = Metrics[0]\n",
    "            AK_SBIFull[i,j] = Metrics[1]\n",
    "            RK_SBIFull[i,j] = Metrics[2]\n",
    "            MKT_SBIFull[i,j] = Metrics[3]\n",
    "            KFA_SBIFull[i,j] = Metrics[4]\n",
    "    MKFullArr.append(MK_SBIFull)\n",
    "    RKFullArr.append(RK_SBIFull)\n",
    "    AKFullArr.append(AK_SBIFull)\n",
    "    MKTFullArr.append(MKT_SBIFull)\n",
    "    KFAFullArr.append(KFA_SBIFull)\n",
    "\n",
    "    MK_SBI7  = np.zeros(ArrShape)\n",
    "    AK_SBI7  = np.zeros(ArrShape)\n",
    "    RK_SBI7  = np.zeros(ArrShape)\n",
    "    MKT_SBI7 = np.zeros(ArrShape)\n",
    "    KFA_SBI7 = np.zeros(ArrShape)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst7_2[i,j][:6],NoiseEst7_2[i,j][6:21])\n",
    "            MK_SBI7[i,j] = Metrics[0]\n",
    "            AK_SBI7[i,j] = Metrics[1]\n",
    "            RK_SBI7[i,j] = Metrics[2]\n",
    "            MKT_SBI7[i,j] = Metrics[3]\n",
    "            KFA_SBI7[i,j] = Metrics[4]\n",
    "    MK7Arr.append(MK_SBI7)\n",
    "    RK7Arr.append(RK_SBI7)\n",
    "    AK7Arr.append(AK_SBI7)\n",
    "    MKT7Arr.append(MKT_SBI7)\n",
    "    KFA7Arr.append(KFA_SBI7)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(AK_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(RK_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56159d39-b4e9-4542-8c3e-91228e80cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK20Arr = []\n",
    "RK20Arr = []\n",
    "AK20Arr = []\n",
    "MKT20Arr = []\n",
    "KFA20Arr = []\n",
    "for kk in range(5):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk+1)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk+1)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk+1)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    \n",
    "    ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "    NoiseEst = np.zeros(list(ArrShape)+[22])\n",
    "    torch.manual_seed(10)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                posterior_samples_1 = posterior20.sample((InferSamples,), x=np.hstack([kk*1000,TestData4D[i,j,axial_middle,TrueIndxs20[kk]]]),show_progress_bars=False)\n",
    "                NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):  \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])\n",
    "    \n",
    "    MK_SBI20  = np.zeros(ArrShape)\n",
    "    AK_SBI20  = np.zeros(ArrShape)\n",
    "    RK_SBI20  = np.zeros(ArrShape)\n",
    "    MKT_SBI20 = np.zeros(ArrShape)\n",
    "    KFA_SBI20 = np.zeros(ArrShape)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK_SBI20[i,j] = Metrics[0]\n",
    "            AK_SBI20[i,j] = Metrics[1]\n",
    "            RK_SBI20[i,j] = Metrics[2]\n",
    "            MKT_SBI20[i,j] = Metrics[3]\n",
    "            KFA_SBI20[i,j] = Metrics[4]\n",
    "    MK20Arr.append(MK_SBI20)\n",
    "    RK20Arr.append(RK_SBI20)\n",
    "    AK20Arr.append(AK_SBI20)\n",
    "    MKT20Arr.append(MKT_SBI20)\n",
    "    KFA20Arr.append(KFA_SBI20)\n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK_SBI20.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(AK_SBI20.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(RK_SBI20.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT_SBI20.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA_SBI20.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730df8a-9b12-44ee-b623-d04cffad0df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK7NLArr = []\n",
    "RK7NLArr = []\n",
    "AK7NLArr = []\n",
    "MKT7NLArr = []\n",
    "KFA7NLArr = []\n",
    "for kk in range(5):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk+1)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk+1)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk+1)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "    gtab7   = gradient_table(gtabExt.bvals[TrueIndxs[kk]],gtabExt.bvecs[TrueIndxs[kk]])\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    \n",
    "    ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gtab7,fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(TestData[:,:,TrueIndxs[kk]])\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7[i,j] = Metrics[0]\n",
    "            AK_NL7[i,j] = Metrics[1]\n",
    "            RK_NL7[i,j] = Metrics[2]\n",
    "            MKT_NL7[i,j] = Metrics[3]\n",
    "            KFA_NL7[i,j] = Metrics[4]\n",
    "    MK7NLArr.append(MK_NL7)\n",
    "    RK7NLArr.append(RK_NL7)\n",
    "    AK7NLArr.append(AK_NL7)\n",
    "    MKT7NLArr.append(MKT_NL7)\n",
    "    KFA7NLArr.append(KFA_NL7)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(AK_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(RK_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA_NL7.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e17e7d9-d8d1-4059-9c43-b1712960f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK20NLArr = []\n",
    "RK20NLArr = []\n",
    "AK20NLArr = []\n",
    "MKT20NLArr = []\n",
    "KFA20NLArr = []\n",
    "for kk in range(5):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk+1)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk+1)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk+1)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "    gtab20   = gradient_table(gtabExt.bvals[TrueIndxs20[kk]],gtabExt.bvecs[TrueIndxs20[kk]])\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    \n",
    "    ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gtab20,fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(TestData[:,:,TrueIndxs20[kk]])\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7[i,j] = Metrics[0]\n",
    "            AK_NL7[i,j] = Metrics[1]\n",
    "            RK_NL7[i,j] = Metrics[2]\n",
    "            MKT_NL7[i,j] = Metrics[3]\n",
    "            KFA_NL7[i,j] = Metrics[4]\n",
    "    MK20NLArr.append(MK_NL7)\n",
    "    RK20NLArr.append(RK_NL7)\n",
    "    AK20NLArr.append(AK_NL7)\n",
    "    MKT20NLArr.append(MKT_NL7)\n",
    "    KFA20NLArr.append(KFA_NL7)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(AK_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(RK_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA_NL7.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d877e-c9e1-41af-829d-91beb770077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullNLArr = []\n",
    "RKFullNLArr = []\n",
    "AKFullNLArr = []\n",
    "MKTFullNLArr = []\n",
    "KFAFullNLArr = []\n",
    "for kk in range(5):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk+1)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk+1)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk+1)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)\n",
    "    \n",
    "    ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gtabExt,fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(TestData)\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "            MK_NL7[i,j] = Metrics[0]\n",
    "            AK_NL7[i,j] = Metrics[1]\n",
    "            RK_NL7[i,j] = Metrics[2]\n",
    "            MKT_NL7[i,j] = Metrics[3]\n",
    "            KFA_NL7[i,j] = Metrics[4]\n",
    "    MKFullNLArr.append(MK_NL7)\n",
    "    RKFullNLArr.append(RK_NL7)\n",
    "    AKFullNLArr.append(AK_NL7)\n",
    "    MKTFullNLArr.append(MKT_NL7)\n",
    "    KFAFullNLArr.append(KFA_NL7)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(AK_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(RK_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA_NL7.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e31fa-19a5-46ac-a52b-f1752b6b5028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCompMets(MG,MT,Ma,WM):\n",
    "    MG[np.isnan(MG)] = 0\n",
    "    MT[np.isnan(MT)] = 0\n",
    "\n",
    "    s = ssim(MG*Ma,MT*Ma,data_range=np.nanmax(MT*Ma)-np.nanmin(MT*Ma))\n",
    "    diff = np.mean(np.abs(MG[Ma]-MT[Ma]))\n",
    "    std  = np.std(MG[WM])\n",
    "    return s,diff,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc67ff8-c7db-4523-9011-43ced7f54486",
   "metadata": {},
   "outputs": [],
   "source": [
    "Masks = []\n",
    "MaskCut = []\n",
    "for kk in range(5):\n",
    "    fdwi = './HCP_data/Pat'+str(kk+1)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(kk+1)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(kk+1)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(kk+1)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(kk+1)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(kk+1)+'/bvecs_3k.txt'\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    _, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    \n",
    "    Masks.append(mask2)\n",
    "    MaskCut.append(mask)\n",
    "WMs = []\n",
    "for kk in range(5):\n",
    "    WM, affine, img = load_nifti('./HCP_data/c2vol0000_'+str(kk+1)+'.nii', return_img=True)\n",
    "    WM, affine = reslice(WM, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    \n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(MaskCut[kk])\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    WMs.append(WM[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e389c90b-5d3b-4ac1-a611-591211badf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e2f09-94b8-4717-ab50-e8e39105bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*10)\n",
    "SSIM_MK_NL7 = []\n",
    "Acc_MK_NL7  = []\n",
    "Perc_MK_NL7 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(MK7NLArr[i],MKFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_MK_NL7.append(x1)\n",
    "    Acc_MK_NL7.append(x2)    \n",
    "    Perc_MK_NL7.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_MK_NL20 = []\n",
    "Acc_MK_NL20  = []\n",
    "Perc_MK_NL20 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(MK20NLArr[i],MKFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_MK_NL20.append(x1)\n",
    "    Acc_MK_NL20.append(x2)    \n",
    "    Perc_MK_NL20.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_MK_20 = []\n",
    "Acc_MK_20  = []\n",
    "Perc_MK_20 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(MK20Arr[i],MKFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_MK_20.append(x1)\n",
    "    Acc_MK_20.append(x2)    \n",
    "    Perc_MK_20.append(x3)  \n",
    "print('-'*10)\n",
    "SSIM_MK_7 = []\n",
    "Acc_MK_7  = []\n",
    "Perc_MK_7 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(MK7Arr[i],MKFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_MK_7.append(x1)\n",
    "    Acc_MK_7.append(x2)    \n",
    "    Perc_MK_7.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_MK_Fulls = []\n",
    "Acc_MK_Fulls  = []\n",
    "Perc_MK_Fulls = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(MKFullNLArr[i],MKFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_MK_Fulls.append(x1)\n",
    "    Acc_MK_Fulls.append(x2)    \n",
    "    Perc_MK_Fulls.append(x3)   \n",
    "print('-'*10)\n",
    "Perc_MK_FullsNL = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(MKFullArr[i],MKFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    Perc_MK_FullsNL.append(x3)   \n",
    "\n",
    "plt.scatter(np.ones(5),SSIM_MK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_MK_Fulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),SSIM_MK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_MK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),SSIM_MK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_MK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),SSIM_MK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_MK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),SSIM_MK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_MK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.axhline(0.66,lw=3,ls='--',c='k')\n",
    "plt.yticks(fontsize=32)\n",
    "plt.ylim([0,1])\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_MK.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.scatter(np.ones(5),Perc_MK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_Fulls),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),widths=0.3)\n",
    "plt.scatter(1.3*np.ones(5),Perc_MK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3],widths=0.3)\n",
    "plt.scatter(1.6*np.ones(5),Perc_MK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.6],widths=0.3)\n",
    "\n",
    "plt.scatter(2.5*np.ones(5),Perc_MK_FullsNL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_FullsNL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.5],widths=0.3)\n",
    "plt.scatter(2.8*np.ones(5),Perc_MK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Perc_MK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.3,1.6,2.5,2.8,3.1],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=32)\n",
    "x = np.arange(0.7,2,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(Perc_MK_Fulls)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(Perc_MK_Fulls)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(2.2,3.5,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(Perc_MK_FullsNL)\n",
    "y2 = np.ones_like(x)*0.9*np.median(Perc_MK_FullsNL)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Prec_MK.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.scatter(np.ones(5),Acc_MK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_MK_Fulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),Acc_MK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_MK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),Acc_MK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_MK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),Acc_MK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_MK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Acc_MK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_MK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.yticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Acc_MK.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57d752-9ad5-432f-bc21-6007e1d76d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "plt.sca(ax1)\n",
    "plt.scatter(np.ones(5),Perc_MK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_Fulls),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),widths=0.3)\n",
    "plt.scatter(1.3*np.ones(5),Perc_MK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3],widths=0.3)\n",
    "plt.scatter(1.6*np.ones(5),Perc_MK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.6],widths=0.3)\n",
    "\n",
    "plt.scatter(2.5*np.ones(5),Perc_MK_FullsNL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_FullsNL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.5],widths=0.3)\n",
    "plt.scatter(2.8*np.ones(5),Perc_MK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Perc_MK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "x = np.arange(0.7,2,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(Perc_MK_Fulls)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(Perc_MK_Fulls)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(2.2,3.5,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(Perc_MK_FullsNL)\n",
    "y2 = np.ones_like(x)*0.9*np.median(Perc_MK_FullsNL)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax1.set_ylim(0.6, 5)\n",
    "#ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax1.yaxis.set_ticks(np.arange(0.0002, 0.0016, 0.0004))\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.sca(ax2)\n",
    "plt.scatter(np.ones(5),Perc_MK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_Fulls),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),widths=0.3)\n",
    "plt.scatter(1.3*np.ones(5),Perc_MK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3],widths=0.3)\n",
    "plt.scatter(1.6*np.ones(5),Perc_MK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.6],widths=0.3)\n",
    "\n",
    "plt.scatter(2.5*np.ones(5),Perc_MK_FullsNL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_FullsNL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.5],widths=0.3)\n",
    "plt.scatter(2.8*np.ones(5),Perc_MK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Perc_MK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "x = np.arange(0.7,2,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(Perc_MK_Fulls)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(Perc_MK_Fulls)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(2.2,3.5,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(Perc_MK_FullsNL)\n",
    "y2 = np.ones_like(x)*0.9*np.median(Perc_MK_FullsNL)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax2.set_ylim(0.15,0.6)\n",
    "#ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "ax2.set_yticks([0.2,0.4])\n",
    "plt.xticks([1,1.3,1.6,2.5,2.8,3.1],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Prec_MK.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb4cd3-d9a2-4549-b6a3-68a7deead187",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*10)\n",
    "SSIM_AK_NL7 = []\n",
    "Acc_AK_NL7  = []\n",
    "Perc_AK_NL7 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(AK7NLArr[i],AKFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_AK_NL7.append(x1)\n",
    "    Acc_AK_NL7.append(x2)    \n",
    "    Perc_AK_NL7.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_AK_NL20 = []\n",
    "Acc_AK_NL20  = []\n",
    "Perc_AK_NL20 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(AK20NLArr[i],AKFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_AK_NL20.append(x1)\n",
    "    Acc_AK_NL20.append(x2)    \n",
    "    Perc_AK_NL20.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_AK_20 = []\n",
    "Acc_AK_20  = []\n",
    "Perc_AK_20 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(AK20Arr[i],AKFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_AK_20.append(x1)\n",
    "    Acc_AK_20.append(x2)    \n",
    "    Perc_AK_20.append(x3)  \n",
    "print('-'*10)\n",
    "SSIM_AK_7 = []\n",
    "Acc_AK_7  = []\n",
    "Perc_AK_7 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(AK7Arr[i],AKFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_AK_7.append(x1)\n",
    "    Acc_AK_7.append(x2)    \n",
    "    Perc_AK_7.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_AK_Fulls = []\n",
    "Acc_AK_Fulls  = []\n",
    "Perc_AK_Fulls = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(AKFullNLArr[i],AKFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_AK_Fulls.append(x1)\n",
    "    Acc_AK_Fulls.append(x2)    \n",
    "    Perc_AK_Fulls.append(x3)   \n",
    "print('-'*10)\n",
    "Perc_AK_FullsNL = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(AKFullArr[i],AKFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    Perc_AK_FullsNL.append(x3)   \n",
    "\n",
    "plt.scatter(np.ones(5),SSIM_AK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_AK_Fulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),SSIM_AK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_AK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),SSIM_AK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_AK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),SSIM_AK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_AK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),SSIM_AK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_AK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.axhline(0.66,lw=3,ls='--',c='k')\n",
    "plt.yticks(fontsize=32)\n",
    "plt.ylim([0,1])\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_AK.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.scatter(np.ones(5),Perc_AK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_Fulls),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),widths=0.3)\n",
    "plt.scatter(1.3*np.ones(5),Perc_AK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3],widths=0.3)\n",
    "plt.scatter(1.6*np.ones(5),Perc_AK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.6],widths=0.3)\n",
    "\n",
    "plt.scatter(2.5*np.ones(5),Perc_AK_FullsNL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_FullsNL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.5],widths=0.3)\n",
    "plt.scatter(2.8*np.ones(5),Perc_AK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Perc_AK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.3,1.6,2.5,2.8,3.1],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=32)\n",
    "\n",
    "x = np.arange(0.7,2,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(Perc_AK_Fulls)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(Perc_AK_Fulls)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(2.2,3.5,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(Perc_AK_FullsNL)\n",
    "y2 = np.ones_like(x)*0.9*np.median(Perc_AK_FullsNL)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Prec_AK.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.scatter(np.ones(5),Acc_AK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_AK_Fulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),Acc_AK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_AK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),Acc_AK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_AK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),Acc_AK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_AK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Acc_AK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_AK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.yticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Acc_AK.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93286673-f84f-462e-9d72-11af0d822af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "plt.sca(ax1)\n",
    "plt.scatter(np.ones(5),Perc_AK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_Fulls),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),widths=0.3)\n",
    "plt.scatter(1.3*np.ones(5),Perc_AK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3],widths=0.3)\n",
    "plt.scatter(1.6*np.ones(5),Perc_AK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.6],widths=0.3)\n",
    "\n",
    "plt.scatter(2.5*np.ones(5),Perc_AK_FullsNL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_FullsNL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.5],widths=0.3)\n",
    "plt.scatter(2.8*np.ones(5),Perc_AK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Perc_AK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "x = np.arange(0.7,2,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(Perc_AK_Fulls)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(Perc_AK_Fulls)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(2.2,3.5,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(Perc_AK_FullsNL)\n",
    "y2 = np.ones_like(x)*0.9*np.median(Perc_AK_FullsNL)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax1.set_ylim(0.85, 1.2)\n",
    "#ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax1.yaxis.set_ticks(np.arange(0.0002, 0.0016, 0.0004))\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.sca(ax2)\n",
    "plt.scatter(np.ones(5),Perc_AK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_Fulls),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),widths=0.3)\n",
    "plt.scatter(1.3*np.ones(5),Perc_AK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3],widths=0.3)\n",
    "plt.scatter(1.6*np.ones(5),Perc_AK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.6],widths=0.3)\n",
    "\n",
    "plt.scatter(2.5*np.ones(5),Perc_AK_FullsNL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_FullsNL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.5],widths=0.3)\n",
    "plt.scatter(2.8*np.ones(5),Perc_AK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Perc_AK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_AK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "x = np.arange(0.7,2,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(Perc_AK_Fulls)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(Perc_AK_Fulls)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(2.2,3.5,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(Perc_AK_FullsNL)\n",
    "y2 = np.ones_like(x)*0.9*np.median(Perc_AK_FullsNL)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "box_plot(np.array(AccM7NL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax2.set_ylim(0.15,0.6)\n",
    "#ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "ax2.set_yticks([0.2,0.4])\n",
    "ax2.set_xticklabels(['Full', 'Mid', 'Min', 'Mid', 'Min'], fontsize=32, rotation=90)\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Prec_AK.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17293cc-1375-4bf6-aa36-d9b7b0065e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*10)\n",
    "SSIM_RK_NL7 = []\n",
    "Acc_RK_NL7  = []\n",
    "Perc_RK_NL7 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(RK7NLArr[i],RKFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_RK_NL7.append(x1)\n",
    "    Acc_RK_NL7.append(x2)    \n",
    "    Perc_RK_NL7.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_RK_NL20 = []\n",
    "Acc_RK_NL20  = []\n",
    "Perc_RK_NL20 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(RK20NLArr[i],RKFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_RK_NL20.append(x1)\n",
    "    Acc_RK_NL20.append(x2)    \n",
    "    Perc_RK_NL20.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_RK_20 = []\n",
    "Acc_RK_20  = []\n",
    "Perc_RK_20 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(RK20Arr[i],RKFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_RK_20.append(x1)\n",
    "    Acc_RK_20.append(x2)    \n",
    "    Perc_RK_20.append(x3)  \n",
    "print('-'*10)\n",
    "SSIM_RK_7 = []\n",
    "Acc_RK_7  = []\n",
    "Perc_RK_7 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(RK7Arr[i],RKFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_RK_7.append(x1)\n",
    "    Acc_RK_7.append(x2)    \n",
    "    Perc_RK_7.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_RK_Fulls = []\n",
    "Acc_RK_Fulls  = []\n",
    "Perc_RK_Fulls = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(RKFullNLArr[i],RKFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_RK_Fulls.append(x1)\n",
    "    Acc_RK_Fulls.append(x2)    \n",
    "    Perc_RK_Fulls.append(x3)   \n",
    "print('-'*10)\n",
    "Perc_RK_FullsNL = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(RKFullArr[i],RKFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    Perc_RK_FullsNL.append(x3)   \n",
    "\n",
    "plt.scatter(np.ones(5),SSIM_RK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_RK_Fulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),SSIM_RK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_RK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),SSIM_RK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_RK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),SSIM_RK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_RK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),SSIM_RK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_RK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.axhline(0.66,lw=3,ls='--',c='k')\n",
    "plt.yticks(fontsize=32)\n",
    "plt.ylim([0,1])\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_RK.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.scatter(np.ones(5),Perc_RK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_Fulls),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),widths=0.3)\n",
    "plt.scatter(1.3*np.ones(5),Perc_RK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3],widths=0.3)\n",
    "plt.scatter(1.6*np.ones(5),Perc_RK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.6],widths=0.3)\n",
    "\n",
    "plt.scatter(2.5*np.ones(5),Perc_RK_FullsNL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_FullsNL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.5],widths=0.3)\n",
    "plt.scatter(2.8*np.ones(5),Perc_RK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Perc_RK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.3,1.6,2.5,2.8,3.1],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=32)\n",
    "\n",
    "x = np.arange(0.7,2,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(Perc_RK_Fulls)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(Perc_RK_Fulls)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(2.2,3.5,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(Perc_RK_FullsNL)\n",
    "y2 = np.ones_like(x)*0.9*np.median(Perc_RK_FullsNL)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Prec_RK.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.scatter(np.ones(5),Acc_RK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_RK_Fulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),Acc_RK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_RK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),Acc_RK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_RK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),Acc_RK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_RK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Acc_RK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_RK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.yticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Acc_RK.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9338bf-4578-4e8f-8d13-5f4a62e1005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "plt.sca(ax1)\n",
    "plt.scatter(np.ones(5),Perc_RK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_Fulls),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),widths=0.3)\n",
    "plt.scatter(1.3*np.ones(5),Perc_RK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3],widths=0.3)\n",
    "plt.scatter(1.6*np.ones(5),Perc_RK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.6],widths=0.3)\n",
    "\n",
    "plt.scatter(2.5*np.ones(5),Perc_RK_FullsNL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_FullsNL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.5],widths=0.3)\n",
    "plt.scatter(2.8*np.ones(5),Perc_RK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Perc_RK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "x = np.arange(0.7,2,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(Perc_RK_Fulls)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(Perc_RK_Fulls)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(2.2,3.5,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(Perc_RK_FullsNL)\n",
    "y2 = np.ones_like(x)*0.9*np.median(Perc_RK_FullsNL)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax1.set_ylim(6, 85)\n",
    "#ax1.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax1.yaxis.set_ticks(np.arange(0.0002, 0.0016, 0.0004))\n",
    "ax1.set_xticks([])\n",
    "\n",
    "plt.sca(ax2)\n",
    "plt.scatter(np.ones(5),Perc_RK_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_Fulls),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),widths=0.3)\n",
    "plt.scatter(1.3*np.ones(5),Perc_RK_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3],widths=0.3)\n",
    "plt.scatter(1.6*np.ones(5),Perc_RK_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.6],widths=0.3)\n",
    "\n",
    "plt.scatter(2.5*np.ones(5),Perc_RK_FullsNL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_FullsNL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.5],widths=0.3)\n",
    "plt.scatter(2.8*np.ones(5),Perc_RK_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Perc_RK_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_RK_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "x = np.arange(0.7,2,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(Perc_RK_Fulls)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(Perc_RK_Fulls)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(2.2,3.5,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(Perc_RK_FullsNL)\n",
    "y2 = np.ones_like(x)*0.9*np.median(Perc_RK_FullsNL)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "#plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-0.5,3))\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "ax2.set_ylim(0.4,1.5)\n",
    "#ax2.ticklabel_format(axis='y', style='sci', scilimits=(-3, -3))\n",
    "#ax2.yaxis.set_ticks(np.arange(0, 0.0001, 0.00004))\n",
    "\n",
    "# Common x-ticks\n",
    "#ax2.set_yticks([0.4,0.6])\n",
    "plt.xticks([1,1.3,1.6,2.5,2.8,3.1],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "# Adding broken axis effect\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12, linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0], [0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0], [1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "# Hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax2.yaxis.offsetText.set_visible(False)  # Hide the \"1e-3\" from ax2\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Prec_RK.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b1026-15dd-4851-b3a0-4dd850745490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bootstrap\n",
    "data = (SSIM_MK_7,)  # samples must be in a sequence\n",
    "res = bootstrap(data, np.mean, confidence_level=0.9,\n",
    "                random_state=0)\n",
    "print(np.mean(data))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54fcfb-c3df-4146-9da9-53bfd77c3c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be413de-89f5-4392-9fef-f2d60ce3ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*10)\n",
    "SSIM_MKT_NL7 = []\n",
    "Acc_MKT_NL7  = []\n",
    "Perc_MKT_NL7 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(MKT7NLArr[i],MKTFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_MKT_NL7.append(x1)\n",
    "    Acc_MKT_NL7.append(x2)    \n",
    "    Perc_MKT_NL7.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_MKT_NL20 = []\n",
    "Acc_MKT_NL20  = []\n",
    "Perc_MKT_NL20 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(MKT20NLArr[i],MKTFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_MKT_NL20.append(x1)\n",
    "    Acc_MKT_NL20.append(x2)    \n",
    "    Perc_MKT_NL20.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_MKT_20 = []\n",
    "Acc_MKT_20  = []\n",
    "Perc_MKT_20 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(MKT20Arr[i],MKTFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_MKT_20.append(x1)\n",
    "    Acc_MKT_20.append(x2)    \n",
    "    Perc_MKT_20.append(x3)  \n",
    "print('-'*10)\n",
    "SSIM_MKT_7 = []\n",
    "Acc_MKT_7  = []\n",
    "Perc_MKT_7 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(MKT7Arr[i],MKTFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_MKT_7.append(x1)\n",
    "    Acc_MKT_7.append(x2)    \n",
    "    Perc_MKT_7.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_MKT_Fulls = []\n",
    "Acc_MKT_Fulls  = []\n",
    "Perc_MKT_Fulls = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(MKTFullNLArr[i],MKTFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_MKT_Fulls.append(x1)\n",
    "    Acc_MKT_Fulls.append(x2)    \n",
    "    Perc_MKT_Fulls.append(x3)   \n",
    "print('-'*10)\n",
    "Perc_MKT_FullsNL = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(MKTFullArr[i],MKTFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    Perc_MKT_FullsNL.append(x3)   \n",
    "\n",
    "plt.scatter(np.ones(5),SSIM_MKT_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_MKT_Fulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),SSIM_MKT_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_MKT_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),SSIM_MKT_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_MKT_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),SSIM_MKT_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_MKT_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),SSIM_MKT_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_MKT_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.axhline(0.66,lw=3,ls='--',c='k')\n",
    "plt.yticks(fontsize=32)\n",
    "plt.ylim([0,1])\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_MKT.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.scatter(np.ones(5),Perc_MKT_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MKT_Fulls),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),widths=0.3)\n",
    "plt.scatter(1.3*np.ones(5),Perc_MKT_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MKT_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3],widths=0.3)\n",
    "plt.scatter(1.6*np.ones(5),Perc_MKT_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MKT_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.6],widths=0.3)\n",
    "\n",
    "plt.scatter(2.5*np.ones(5),Perc_MKT_FullsNL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MKT_FullsNL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.5],widths=0.3)\n",
    "plt.scatter(2.8*np.ones(5),Perc_MKT_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MKT_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Perc_MKT_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_MKT_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.3,1.6,2.5,2.8,3.1],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=32)\n",
    "\n",
    "x = np.arange(0.7,2,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(Perc_MKT_Fulls)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(Perc_MKT_Fulls)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(2.2,3.5,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(Perc_MKT_FullsNL)\n",
    "y2 = np.ones_like(x)*0.9*np.median(Perc_MKT_FullsNL)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Prec_MKT.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.scatter(np.ones(5),Acc_MKT_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_MKT_Fulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),Acc_MKT_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_MKT_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),Acc_MKT_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_MKT_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),Acc_MKT_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_MKT_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Acc_MKT_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_MKT_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.yticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Acc_MKT.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d3809-0072-4660-9057-7a5015efac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*10)\n",
    "SSIM_KFA_NL7 = []\n",
    "Acc_KFA_NL7  = []\n",
    "Perc_KFA_NL7 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(KFA7NLArr[i],KFAFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_KFA_NL7.append(x1)\n",
    "    Acc_KFA_NL7.append(x2)    \n",
    "    Perc_KFA_NL7.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_KFA_NL20 = []\n",
    "Acc_KFA_NL20  = []\n",
    "Perc_KFA_NL20 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(KFA20NLArr[i],KFAFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_KFA_NL20.append(x1)\n",
    "    Acc_KFA_NL20.append(x2)    \n",
    "    Perc_KFA_NL20.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_KFA_20 = []\n",
    "Acc_KFA_20  = []\n",
    "Perc_KFA_20 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(KFA20Arr[i],KFAFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_KFA_20.append(x1)\n",
    "    Acc_KFA_20.append(x2)    \n",
    "    Perc_KFA_20.append(x3)  \n",
    "print('-'*10)\n",
    "SSIM_KFA_7 = []\n",
    "Acc_KFA_7  = []\n",
    "Perc_KFA_7 = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(KFA7Arr[i],KFAFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_KFA_7.append(x1)\n",
    "    Acc_KFA_7.append(x2)    \n",
    "    Perc_KFA_7.append(x3)   \n",
    "print('-'*10)\n",
    "SSIM_KFA_Fulls = []\n",
    "Acc_KFA_Fulls  = []\n",
    "Perc_KFA_Fulls = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(KFAFullNLArr[i],KFAFullArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    SSIM_KFA_Fulls.append(x1)\n",
    "    Acc_KFA_Fulls.append(x2)    \n",
    "    Perc_KFA_Fulls.append(x3)   \n",
    "print('-'*10)\n",
    "Perc_KFA_FullsNL = []\n",
    "for i in range(5):\n",
    "    x1,x2,x3 = GetCompMets(KFAFullArr[i],KFAFullNLArr[i],Masks[i][:,:,AMs[i]],WMs[i][:,:,AMs[i]].astype(bool))\n",
    "    Perc_KFA_FullsNL.append(x3)   \n",
    "\n",
    "plt.scatter(np.ones(5),SSIM_KFA_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_KFA_Fulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),SSIM_KFA_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_KFA_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),SSIM_KFA_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_KFA_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),SSIM_KFA_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_KFA_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),SSIM_KFA_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_KFA_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.axhline(0.66,lw=3,ls='--',c='k')\n",
    "plt.yticks(fontsize=32)\n",
    "plt.ylim([0,1])\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_SSIM_KFA.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.scatter(np.ones(5),Perc_KFA_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_KFA_Fulls),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),widths=0.3)\n",
    "plt.scatter(1.3*np.ones(5),Perc_KFA_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_KFA_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3],widths=0.3)\n",
    "plt.scatter(1.6*np.ones(5),Perc_KFA_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_KFA_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.6],widths=0.3)\n",
    "\n",
    "plt.scatter(2.5*np.ones(5),Perc_KFA_FullsNL,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_KFA_FullsNL),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.5],widths=0.3)\n",
    "plt.scatter(2.8*np.ones(5),Perc_KFA_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_KFA_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Perc_KFA_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Perc_KFA_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.3,1.6,2.5,2.8,3.1],['Full','Mid','Min','Full','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=32)\n",
    "\n",
    "x = np.arange(0.7,2,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(Perc_KFA_Fulls)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(Perc_KFA_Fulls)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(2.2,3.5,0.1)\n",
    "y1 = np.ones_like(x)*1.1*np.median(Perc_KFA_FullsNL)\n",
    "y2 = np.ones_like(x)*0.9*np.median(Perc_KFA_FullsNL)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Prec_KFA.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.scatter(np.ones(5),Acc_KFA_Fulls,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_KFA_Fulls),'black', 'gray',widths=0.3)\n",
    "\n",
    "plt.scatter(1.7*np.ones(5),Acc_KFA_20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_KFA_20),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.7],widths=0.3)\n",
    "plt.scatter(2*np.ones(5),Acc_KFA_7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_KFA_7),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[2],widths=0.3)\n",
    "\n",
    "plt.scatter(2.8*np.ones(5),Acc_KFA_NL20,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_KFA_NL20),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[2.8],widths=0.3)\n",
    "plt.scatter(3.1*np.ones(5),Acc_KFA_NL7,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Acc_KFA_NL7),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[3.1],widths=0.3)\n",
    "\n",
    "plt.xticks([1,1.7,2,2.8,3.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "plt.yticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'DKIHCP_Acc_KFA.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe27a2d-d41e-4720-810d-a570ed7b007a",
   "metadata": {},
   "source": [
    "# Fig 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325bb35b-804c-4dee-bd2d-cf88f1c2a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_6/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a369f7-b1e4-4fd5-a53e-fd330b124634",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6770f60c-15ec-48e8-8a41-a453ae3dcb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSDir = './MS_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e081b12a-311d-4ec9-9a03-1e90705285a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = pmt.read_mat(MSDir+'NMSS_15/data_loaded.mat')\n",
    "affine = np.ones((4,4))\n",
    "\n",
    "data, affine = reslice(F['data'], affine, (2,2,2), (2.5,2.5,2.5))\n",
    "_, maskCut = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=False, dilate=3)\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=True, dilate=3)\n",
    "axial_middle = maskdata.shape[2] // 2\n",
    "\n",
    "bvecs = (F['direction'].T/np.linalg.norm(F['direction'],axis=1)).T\n",
    "bvecs[np.isnan(bvecs)] = 0\n",
    "bvals = F['bval']\n",
    "bvecs2000 = bvecs[bvals==2000]\n",
    "bvecs4000 = bvecs[bvals==4000]\n",
    "\n",
    "gtabFull = gradient_table(bvals, bvecs)\n",
    "\n",
    "bvals2000 = np.array([0] + list(bvals[bvals==2000]))\n",
    "bvecs2000 = np.vstack([[0,0,0],bvecs[bvals==2000]])\n",
    "data2000 = maskdata[:,:,40,np.hstack([0,np.where(bvals==2000)[0]])]\n",
    "gtab2000 = gradient_table(bvals2000,bvecs2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9173d8-4cc7-41d8-99c8-4001f5deeefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdwi = MSDir+'NMSS_15_lesions.nii'\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "lesion, affine = reslice(data, affine, (2,2,2), (2.5,2.5,2.5))\n",
    "\n",
    "fdwi = MSDir+'b0_NMSS_15.nii'\n",
    "\n",
    "b0, affine, img = load_nifti(fdwi, return_img=True)\n",
    "b0, affine = reslice(b0, affine, (2,2,2), (2.5,2.5,2.5))\n",
    "\n",
    "b0_alt = np.swapaxes(b0[::-1,::-1],0,1)\n",
    "lesion_alt = np.swapaxes(lesion[::-1,::-1],0,1)\n",
    "bbox = np.argwhere(maskCut)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = bbox.min(axis=0)\n",
    "max_coords = bbox.max(axis=0)\n",
    "\n",
    "lesion_bin  = lesion_alt[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "b0_alt  = b0_alt[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "lesion_bin = lesion_bin.astype(np.double)\n",
    "lesion_bin[lesion_bin == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa4fdf-4673-4710-bf3d-9b9a898ebe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "distance_matrix = squareform(pdist(bvecs2000))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(6):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "\n",
    "bvalsHCP7 = bvals2000[selected_indices]\n",
    "bvecsHCP7 = bvecs2000[selected_indices]\n",
    "gtabHCP7 = gradient_table(bvalsHCP7, bvecsHCP7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9250e-47f0-4e11-9d96-383b4517b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data2000[:,:,0],cmap='gray')\n",
    "plt.imshow(lesion_bin[:,:,40],alpha=1,cmap='autumn')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'B0Lesions.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994c718-bc70-4fdb-a65b-6733c8acec20",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356197e4-3ba4-427f-a734-46a899762172",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIMSMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMSMin.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(10*TrainingSamples)):\n",
    "        params = priorS0.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        dt = ForceLowFA(dt)\n",
    "        Obs.append(CustomSimulator(dt,gtabHCP7,params[-1],np.random.rand()*30 + 20))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMSMin.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMSMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05674e50-c3dc-4f2c-84e8-083b464a83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArrShape = data2000.shape[:2]\n",
    "NoiseEst = np.zeros(list(data2000.shape[:2])+[7])\n",
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        torch.manual_seed(10)\n",
    "        if(np.sum(data2000[i,j]) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            np.random.seed(1)\n",
    "            torch.manual_seed(1)\n",
    "            posterior_samples_1 = posteriorMin.sample((InferSamples,), x=data2000[i,j,selected_indices],show_progress_bars=False)\n",
    "            NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(ArrShape[0]):\n",
    "    for j in range(ArrShape[1]):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1:]])\n",
    "MD_SBIFull = np.zeros(ArrShape[:2])\n",
    "FA_SBIFull = np.zeros(ArrShape)\n",
    "for i in range(ArrShape[0]):\n",
    "    for j in range(ArrShape[1]): \n",
    "        Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "        MD_SBIFull[i,j] = np.mean(Eigs)\n",
    "        FA_SBIFull[i,j] = FracAni(Eigs,np.mean(Eigs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db6449-90a1-4b01-a169-22d3592caf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenmodel = dti.TensorModel(gtabHCP7,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(data2000[:,:,selected_indices])\n",
    "FAMIN = dti.fractional_anisotropy(tenfit.evals)\n",
    "MDMIN = dti.mean_diffusivity(tenfit.evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa70860-14eb-4425-adcb-99dd31d8bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MDMIN,cmap='gray')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'MD7NL.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ecc98d-2355-44c2-99f3-b5af7eb77136",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MD_SBIFull,cmap='gray')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'MD7SBI.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a624e1-4ee0-4ae6-9a2e-cb4cb7c06b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAMIN[np.sum(data2000,axis=-1) == 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d2cd2-85f3-4bcc-9f16-52add03f0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(FAMIN,cmap='gray')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'FA7NL.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa38b4e-7fb6-48d8-ae29-492efd5d8797",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(FA_SBIFull,cmap='gray')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'FA7SBI.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccea8ab-1c1d-4dd2-9dd3-c1108c6e05d2",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4fcebe-3965-4c11-a870-97d920524a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dats   = []\n",
    "gTabs7 = []\n",
    "gTabsF = []\n",
    "Lesions = []\n",
    "B0s    = []\n",
    "Masks   = []\n",
    "TrueIndxs = []\n",
    "for i in tqdm.tqdm(['11_1year','15','16','18','19']):\n",
    "    MatDir = MSDir+'NMSS_'+i+'/'\n",
    "\n",
    "    F = pmt.read_mat(MatDir+'data_loaded.mat')\n",
    "    affine = np.ones((4,4))\n",
    "    \n",
    "    data, affine = reslice(F['data'], affine, (2,2,2), (2.5,2.5,2.5))\n",
    "    _, maskCut = median_otsu(data, vol_idx=range(10, 50), autocrop=False)\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), autocrop=True)\n",
    "    Masks.append(mask)\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "\n",
    "    bvecs = (F['direction'].T/np.linalg.norm(F['direction'],axis=1)).T\n",
    "    bvecs[np.isnan(bvecs)] = 0\n",
    "    bvals = F['bval']\n",
    "    bvecs2000 = bvecs[bvals==2000]\n",
    "    bvecs4000 = bvecs[bvals==4000]\n",
    "\n",
    "    bvals2000 = np.array([0] + list(bvals[bvals==2000]))\n",
    "    bvecs2000 = np.vstack([[0,0,0],bvecs[bvals==2000]])\n",
    "\n",
    "    Dats.append(maskdata[:,:,:,np.hstack([0,np.where(bvals==2000)[0]])])\n",
    "    \n",
    "    gTabsF.append(gradient_table(bvals2000,bvecs2000))\n",
    "\n",
    "\n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    distance_matrix = squareform(pdist(bvecs2000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(6):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    \n",
    "    bvalsHCP7 = bvals2000[selected_indices]\n",
    "    bvecsHCP7 = bvecs2000[selected_indices]\n",
    "    \n",
    "    gTabs7.append(gradient_table(bvalsHCP7, bvecsHCP7))\n",
    "    TrueIndxs.append(selected_indices)\n",
    "\n",
    "    try:\n",
    "        fdwi = MSDir+'NMSS_'+i+'_lesions.nii.gz'\n",
    "            \n",
    "        data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    except:\n",
    "        fdwi = MSDir+'NMSS_'+i+'_lesions.nii'\n",
    "            \n",
    "        data, affine, img = load_nifti(fdwi, return_img=True)        \n",
    "    lesion, affine = reslice(data, affine, (2,2,2), (2.5,2.5,2.5))\n",
    "    \n",
    "    fdwi = MSDir+'b0_NMSS_'+i+'.nii'\n",
    "    \n",
    "    b0, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    b0, affine = reslice(b0, affine, (2,2,2), (2.5,2.5,2.5))\n",
    "    \n",
    "    b0_alt = np.swapaxes(b0[::-1,::-1],0,1)\n",
    "    lesion_alt = np.swapaxes(lesion[::-1,::-1],0,1)\n",
    "    bbox = np.argwhere(maskCut)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    \n",
    "    lesion_bin  = lesion_alt[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    b0_alt  = b0_alt[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    B0s.append(b0_alt)\n",
    "    Lesions.append(lesion_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536e60e-13c8-4777-bc0d-859643093e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LesionSlices = [np.sum(L,axis=(0,1)).argmax() for L in Lesions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74579248-1360-4f2d-9d93-da465324d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIMultiMSFull.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMultiMSFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(4*TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-2])\n",
    "        dt = ForceLowFA(dt)\n",
    "        cG = gTabsF[int(params[-1])]\n",
    "        Obs.append(np.hstack((1000*params[-1],CustomSimulator(dt,cG,params[-2],np.random.rand()*30 + 20))))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-2]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMultiMSFull.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMultiMSFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8eeddb-3ab5-4416-bf63-674e00c90980",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIMultiMSMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMultiMSMin.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(5*TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-2])\n",
    "        dt = ForceLowFA(dt)\n",
    "        cG = gTabs7[int(params[-1])]\n",
    "        Obs.append(np.hstack((1000*params[-1],CustomSimulator(dt,cG,params[-2],np.random.rand()*30 + 20))))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-2]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMultiMSMin.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMultiMSMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d6b49-4bcd-4b9c-bd25-be12e7fb2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDFullArr = []\n",
    "FAFullArr = []\n",
    "for kk in range(5):\n",
    "    Dat = Dats[kk]\n",
    "    ArrShape = Dat.shape[:2]\n",
    "    NoiseEst = np.zeros(list(ArrShape[:2])+[7])\n",
    "    VarEst   = np.zeros(ArrShape)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            torch.manual_seed(10)\n",
    "            if(np.sum(Dat[i,j,LesionSlices[kk]]) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                np.random.seed(1)\n",
    "                torch.manual_seed(1)\n",
    "                posterior_samples_1 = posteriorFull.sample((InferSamples,), x=np.hstack([kk*1000,Dat[i,j,LesionSlices[kk]]]),show_progress_bars=False)\n",
    "                NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1:]])\n",
    "    MD_SBIFull = np.zeros(ArrShape)\n",
    "    FA_SBIFull = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "            MD_SBIFull[i,j] = np.mean(Eigs)\n",
    "            FA_SBIFull[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "    FA_SBIFull[np.isnan(FA_SBIFull)] = 0\n",
    "    MDFullArr.append(MD_SBIFull)\n",
    "    FAFullArr.append(FA_SBIFull)\n",
    "\n",
    "    lesion_nan = Lesions[kk].astype(np.double)\n",
    "    lesion_nan[lesion_nan==0] = np.nan\n",
    "\n",
    "    fig,ax = plt.subplots(1,3,figsize=(14.4,6.4))\n",
    "    ax[0].imshow(Dat[:,:,LesionSlices[kk],0])\n",
    "    ax[0].imshow(lesion_nan[:,:,LesionSlices[kk]],alpha=1,cmap='autumn')\n",
    "    plt.sca(ax[1])\n",
    "    img = plt.imshow(MD_SBIFull,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.sca(ax[2])\n",
    "    img = plt.imshow(FA_SBIFull,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc2b01c-f47a-4bbe-a222-a6653349558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD7Arr = []\n",
    "FA7Arr = []\n",
    "for kk in range(5):\n",
    "    Dat = Dats[kk]\n",
    "    ArrShape = Dat.shape[:2]\n",
    "    NoiseEst = np.zeros(list(ArrShape[:2])+[7])\n",
    "    VarEst   = np.zeros(ArrShape)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            torch.manual_seed(10)\n",
    "            if(np.sum(Dat[i,j,LesionSlices[kk]]) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                np.random.seed(1)\n",
    "                torch.manual_seed(1)\n",
    "                posterior_samples_1 = posteriorMin.sample((InferSamples,), x=np.hstack([kk*1000,Dat[i,j,LesionSlices[kk],TrueIndxs[kk]]]),show_progress_bars=False)\n",
    "                NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):    \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1:]])\n",
    "    MD_SBI7 = np.zeros(ArrShape)\n",
    "    FA_SBI7 = np.zeros(ArrShape)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]): \n",
    "            Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "            MD_SBI7[i,j] = np.mean(Eigs)\n",
    "            FA_SBI7[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "    FA_SBI7[np.isnan(FA_SBI7)] = 0\n",
    "    MD7Arr.append(MD_SBI7)\n",
    "    FA7Arr.append(FA_SBI7)\n",
    "\n",
    "    lesion_nan = Lesions[kk].astype(np.double)\n",
    "    lesion_nan[lesion_nan==0] = np.nan\n",
    "\n",
    "    fig,ax = plt.subplots(1,3,figsize=(14.4,6.4))\n",
    "    ax[0].imshow(Dat[:,:,LesionSlices[kk],0])\n",
    "    ax[0].imshow(lesion_nan[:,:,LesionSlices[kk]],alpha=1,cmap='autumn')\n",
    "    plt.sca(ax[1])\n",
    "    img = plt.imshow(MD_SBI7,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.sca(ax[2])\n",
    "    img = plt.imshow(FA_SBI7,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08c6e5-c65a-40b2-b26a-474d301117d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MDFullNLArr = []\n",
    "FAFullNLArr = []\n",
    "MD7NLArr = []\n",
    "FA7NLArr = []\n",
    "for kk in range(5):\n",
    "    tenmodel = dti.TensorModel(gTabsF[kk],return_S0_hat = True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(Dats[kk][:,:,LesionSlices[kk]])\n",
    "    FAFull = dti.fractional_anisotropy(tenfit.evals)\n",
    "    MDFull = dti.mean_diffusivity(tenfit.evals)\n",
    "    MDFullNLArr.append(MDFull)\n",
    "    FAFullNLArr.append(FAFull)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,3,figsize=(14.4,6.4))\n",
    "    ax[0].imshow(Dats[kk][:,:,LesionSlices[kk],0])\n",
    "    plt.sca(ax[1])\n",
    "    img = plt.imshow(MDFull,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.sca(ax[2])\n",
    "    img = plt.imshow(FAFull,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.show()\n",
    "    tenmodel = dti.TensorModel(gTabs7[kk],return_S0_hat = True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(Dats[kk][:,:,LesionSlices[kk],TrueIndxs[kk]])\n",
    "    FA7 = dti.fractional_anisotropy(tenfit.evals)\n",
    "    MD7 = dti.mean_diffusivity(tenfit.evals)\n",
    "    MD7NLArr.append(MD7)\n",
    "    FA7NLArr.append(FA7)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,3,figsize=(14.4,6.4))\n",
    "    ax[0].imshow(Dats[kk][:,:,LesionSlices[kk],0])\n",
    "    plt.sca(ax[1])\n",
    "    img = plt.imshow(MD7,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.sca(ax[2])\n",
    "    img = plt.imshow(FA7,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    vmin, vmax = img.get_clim()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c8191-d0e1-4fde-8310-d46ec78a798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            torch.manual_seed(10)\n",
    "            if(np.sum(Dat[i,j,LesionSlices[kk]]) == 0):\n",
    "                FAFull[i,j] = 0\n",
    "                FA7[i,j]    = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dba956-3921-46ec-87e7-401704e4287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Err_Comp_MD = []\n",
    "STD_SBI_MD_Full = []\n",
    "STD_NL_MD_Full = []\n",
    "for ii in range(5):\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = MDFullNLArr[ii][le==1]\n",
    "    Dat2 = MDFullArr[ii][le==1]\n",
    "    Err_Comp_MD.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_NL_MD_Full.append(np.std(Dat1))\n",
    "    STD_SBI_MD_Full.append(np.std(Dat2))\n",
    "    \n",
    "Err_NL_MD_L = []\n",
    "Err_NL_MD = []\n",
    "STD_NL_MD_Min = []\n",
    "for ii in range(5):\n",
    "    ma = Masks[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = MDFullNLArr[ii][ma]\n",
    "    Dat2 = MD7NLArr[ii][ma]\n",
    "    Err_NL_MD.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = MDFullNLArr[ii][le==1]\n",
    "    Dat2 = MD7NLArr[ii][le==1]\n",
    "    Err_NL_MD_L.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_NL_MD_Min.append(np.std(Dat2))\n",
    "    \n",
    "Err_SBI_MD_L = []\n",
    "Err_SBI_MD = []\n",
    "STD_SBI_MD_Min = []\n",
    "for ii in range(5):\n",
    "    ma = Masks[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = MDFullArr[ii]\n",
    "    Dat2 = MD7Arr[ii]\n",
    "    Err_SBI_MD.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = MDFullArr[ii][le==1]\n",
    "    Dat2 = MD7Arr[ii][le==1]\n",
    "    Err_SBI_MD_L.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_SBI_MD_Min.append(np.std(Dat2))\n",
    "\n",
    "Err_Comp_FA = []\n",
    "STD_SBI_FA_Full = []\n",
    "STD_NL_FA_Full = []\n",
    "for ii in range(5):\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = FAFullNLArr[ii][le==1]\n",
    "    Dat2 = FAFullArr[ii][le==1]\n",
    "    Err_Comp_FA.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_NL_FA_Full.append(np.std(Dat1))\n",
    "    STD_SBI_FA_Full.append(np.std(Dat2))\n",
    "    \n",
    "Err_NL_FA_L = []\n",
    "Err_NL_FA = []\n",
    "STD_NL_FA_Min = []\n",
    "for ii in range(5):\n",
    "    ma = Masks[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = FAFullNLArr[ii][ma]\n",
    "    Dat2 = FA7NLArr[ii][ma]\n",
    "    Err_NL_FA.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = FAFullNLArr[ii][le==1]\n",
    "    Dat2 = FA7NLArr[ii][le==1]\n",
    "    Err_NL_FA_L.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_NL_FA_Min.append(np.std(Dat2))\n",
    "    \n",
    "Err_SBI_FA_L = []\n",
    "Err_SBI_FA = []\n",
    "STD_SBI_FA_Min = []\n",
    "for ii in range(5):\n",
    "    ma = Masks[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = FAFullArr[ii]\n",
    "    Dat2 = FA7Arr[ii]\n",
    "    Err_SBI_FA.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = FAFullArr[ii][le==1]\n",
    "    Dat2 = FA7Arr[ii][le==1]\n",
    "    Err_SBI_FA_L.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_SBI_FA_Min.append(np.std(Dat2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f39774-279a-4ac6-97bd-291eccfb0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "plt.scatter(0.6*np.ones(5),Err_Comp_MD,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Err_Comp_MD),'black', 'gray',positions=[0.6])\n",
    "\n",
    "box_plot(np.array([Err_SBI_MD]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1))\n",
    "plt.scatter(np.ones(5),np.array([Err_SBI_MD]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([Err_SBI_MD_L]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.15],hatch='//')\n",
    "plt.scatter(1.15*np.ones(5),np.array([Err_SBI_MD_L]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([Err_NL_MD]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.4])\n",
    "plt.scatter(1.4*np.ones(5),np.array([Err_NL_MD]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([Err_NL_MD_L]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.55],hatch='//')\n",
    "plt.scatter(1.55*np.ones(5),np.array([Err_NL_MD_L]),c='gray',zorder=10,alpha=0.5)\n",
    "plt.xticks([0.6,1.075,1.475],['Full \\n Comp.','SBI','NLLS'],fontsize=32,rotation=90)\n",
    "plt.xlim([0.4,1.7])\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=np.clip(WLSFit+0.2,0,1), edgecolor='k', label='Full Img.',hatch='//'),\n",
    "    mpatches.Patch(facecolor=np.clip(WLSFit+0.2,0,1), edgecolor='k', label='Lesion',),\n",
    "]\n",
    "plt.legend(handles=legend_elements,loc = 'lower left',bbox_to_anchor=(-0.1,0.24),\n",
    "           fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "if Save: plt.savefig(FigLoc+'MDAbsErr.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7c678-02a5-489b-a274-e1355d97d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "plt.scatter(0.6*np.ones(5),Err_Comp_FA,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Err_Comp_FA),'black', 'gray',positions=[0.6])\n",
    "\n",
    "box_plot(np.array([Err_SBI_FA]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1))\n",
    "plt.scatter(np.ones(5),np.array([Err_SBI_FA]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([Err_SBI_FA_L]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.15],hatch='//')\n",
    "plt.scatter(1.15*np.ones(5),np.array([Err_SBI_FA_L]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([Err_NL_FA]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.4])\n",
    "plt.scatter(1.4*np.ones(5),np.array([Err_NL_FA]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([Err_NL_FA_L]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.55],hatch='//')\n",
    "plt.scatter(1.55*np.ones(5),np.array([Err_NL_FA_L]),c='gray',zorder=10,alpha=0.5)\n",
    "plt.xticks([0.6,1.075,1.475],['Full \\n Comp.','SBI','NLLS'],fontsize=32,rotation=90)\n",
    "plt.xlim([0.4,1.7])\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "if Save: plt.savefig(FigLoc+'FAAbsErr.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f24af3b-9e9a-4801-88a7-99e07e9ada0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "box_plot(np.array([STD_SBI_MD_Full]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.7])\n",
    "plt.scatter(0.7*np.ones(5),np.array([STD_SBI_MD_Full]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_SBI_MD_Min]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.9])\n",
    "plt.scatter(0.9*np.ones(5),np.array([STD_SBI_MD_Min]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_NL_MD_Full]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.2])\n",
    "plt.scatter(1.2*np.ones(5),np.array([STD_NL_MD_Full]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_NL_MD_Min]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.4])\n",
    "plt.scatter(1.4*np.ones(5),np.array([STD_NL_MD_Min]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "plt.xticks([0.7,0.9,1.2,1.4],['Full','Min','Full','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "plt.xlim([0.5,1.5])\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "\n",
    "x = np.arange(0.6,1.0,0.01)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(STD_SBI_MD_Full)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(STD_SBI_MD_Full)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(1.1,1.5,0.01)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(STD_NL_MD_Full)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(STD_NL_MD_Full)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "if Save: plt.savefig(FigLoc+'MDSTD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab65ee-f06d-4714-b867-0957134fc6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "box_plot(np.array([STD_SBI_FA_Full]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.7])\n",
    "plt.scatter(0.7*np.ones(5),np.array([STD_SBI_FA_Full]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_SBI_FA_Min]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.9])\n",
    "plt.scatter(0.9*np.ones(5),np.array([STD_SBI_FA_Min]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_NL_FA_Full]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.2])\n",
    "plt.scatter(1.2*np.ones(5),np.array([STD_NL_FA_Full]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_NL_FA_Min]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.4])\n",
    "plt.scatter(1.4*np.ones(5),np.array([STD_NL_FA_Min]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "plt.xticks([0.7,0.9,1.2,1.4],['Full','Min','Full','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "plt.xlim([0.5,1.5])\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "\n",
    "x = np.arange(0.6,1.0,0.01)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(STD_SBI_FA_Full)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(STD_SBI_FA_Full)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(1.1,1.5,0.01)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(STD_NL_FA_Full)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(STD_NL_FA_Full)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "if Save: plt.savefig(FigLoc+'FASTD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf8815-f5d3-43b5-b647-2fa23aefe224",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_Comp_FA = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Lesions[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = FAFullNLArr[ii]#[min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = FAFullArr[ii]#[min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_Comp_FA.append(ssim(Dat1,Dat2,data_range=np.max(Dat1)-np.min(Dat1)))\n",
    "SSIM_Comp_MD = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Lesions[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = MDFullNLArr[ii]#[min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = MDFullArr[ii]#[min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_Comp_MD.append(ssim(Dat1,Dat2,data_range=np.max(Dat1)-np.min(Dat1)))\n",
    "SSIM_NL_FA = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Lesions[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = FAFullNLArr[ii]#[min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = FA7NLArr[ii]#[min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_NL_FA.append(ssim(Dat1,Dat2,data_range=max(np.max(Dat1),np.max(Dat2))-min(np.min(Dat1),np.min(Dat2))))\n",
    "\n",
    "SSIM_NL_MD = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Lesions[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = MDFullNLArr[ii]#[min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = MD7NLArr[ii]#[min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_NL_MD.append(ssim(Dat1,Dat2,data_range=max(np.max(Dat1),np.max(Dat2))-min(np.min(Dat1),np.min(Dat2))))\n",
    "SSIM_SBI_FA = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Lesions[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = FAFullArr[ii]#[min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = FA7Arr[ii]#[min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_SBI_FA.append(ssim(Dat1,Dat2,data_range=max(np.max(Dat1),np.max(Dat2))-min(np.min(Dat1),np.min(Dat2))))\n",
    "\n",
    "SSIM_SBI_MD = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Lesions[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = MDFullArr[ii]#[min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = MD7Arr[ii]#[min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_SBI_MD.append(ssim(Dat1,Dat2,data_range=max(np.max(Dat1),np.max(Dat2))-min(np.min(Dat1),np.min(Dat2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601c770-586b-4652-87c3-f34664d2f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "plt.scatter(0.7*np.ones(5),SSIM_Comp_MD,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_Comp_MD),'black', 'gray',positions=[0.7])\n",
    "\n",
    "box_plot(np.array([SSIM_SBI_MD]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1))\n",
    "plt.scatter(np.ones(5),np.array([SSIM_SBI_MD]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([SSIM_NL_MD]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.3])\n",
    "plt.scatter(1.3*np.ones(5),np.array([SSIM_NL_MD]),c='gray',zorder=10,alpha=0.5)\n",
    "plt.axhline(0.66,c='k',ls='--')\n",
    "plt.ylim([0,1])\n",
    "plt.xticks([0.7,1,1.3],['Full \\n Comp.','SBI','NLLS'],fontsize=32,rotation=90)\n",
    "plt.xlim([0.5,1.5])\n",
    "if Save: plt.savefig(FigLoc+'MDErrors.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b8265-8ac9-496e-87b5-b7c913e2cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "plt.scatter(0.7*np.ones(5),SSIM_Comp_FA,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_Comp_FA),'black', 'gray',positions=[0.7])\n",
    "\n",
    "box_plot(np.array([SSIM_SBI_FA]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1))\n",
    "plt.scatter(np.ones(5),np.array([SSIM_SBI_FA]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([SSIM_NL_FA]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.3])\n",
    "plt.scatter(1.3*np.ones(5),np.array([SSIM_NL_FA]),c='gray',zorder=10,alpha=0.5)\n",
    "plt.axhline(0.66,c='k',ls='--')\n",
    "plt.ylim([0,1])\n",
    "plt.xticks([0.7,1,1.3],['Full \\n Comp.','SBI','NLLS'],fontsize=32,rotation=90)\n",
    "plt.xlim([0.5,1.5])\n",
    "if Save: plt.savefig(FigLoc+'FAErrors.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d8e3a8-bc98-47c5-8f67-720b2a8aa88b",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd3764-590a-4698-8055-42fafd754a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "gTabsF = []\n",
    "Dats   = []\n",
    "\n",
    "gTabs7 = []\n",
    "Indxs7 = []\n",
    "\n",
    "for i in tqdm.tqdm(['11_1year','15','16','18','19']):\n",
    "    MatDir = MSDir+'NMSS_'+i+'/'\n",
    "    #NiiDir = '/Users/maximilianeggl/Downloads/data_for_Max/'\n",
    "\n",
    "    F = pmt.read_mat(MatDir+'data_loaded.mat')\n",
    "    affine = np.ones((4,4))\n",
    "    \n",
    "    data, affine = reslice(F['data'], affine, (2,2,2), (2.5,2.5,2.5))\n",
    "    _, maskCut = median_otsu(data, vol_idx=range(10, 50),autocrop=False)\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50),autocrop=True)\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "\n",
    "    bvecs = (F['direction'].T/np.linalg.norm(F['direction'],axis=1)).T\n",
    "    bvecs[np.isnan(bvecs)] = 0\n",
    "    bvals = F['bval']\n",
    "    bvecs2000 = bvecs[bvals==2000]\n",
    "    bvecs4000 = bvecs[bvals==4000]\n",
    "\n",
    "    bvals2000 = np.array([0] + list(bvals[bvals==2000]))\n",
    "    bvecs2000 = np.vstack([[0,0,0],bvecs[bvals==2000]])\n",
    "\n",
    "    Dats.append(maskdata[:,:,:,:])\n",
    "    \n",
    "    gTabsF.append(gradient_table(bvals,bvecs))\n",
    "\n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    distance_matrix = squareform(pdist(bvecs2000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(6):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices7 = selected_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    distance_matrix = squareform(pdist(bvecs4000))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(14):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices7_2 = selected_indices\n",
    "    gTabs7.append(gradient_table([0]+[2000]*6 + [4000]*15,np.vstack([bvecs2000[selected_indices7],bvecs4000[selected_indices7_2]])))\n",
    "    \n",
    "    Indxs7.append(np.hstack([0,np.where(bvals==2000)[0][np.array(selected_indices7)[1:]-1],np.where(bvals==4000)[0][selected_indices7_2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82209b92-d12c-449c-a4fc-fe4be1df6b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DKIMultiMSMin.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiMSMin.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,200000)\n",
    "    #DT2,KT2 = GenDTKT([DT1_hak,DT2_hak],[x4_lak,R1_lak,x2_lak,R2_lak],12,10000)\n",
    "    \n",
    "    DT = np.vstack([DT1])\n",
    "    KT = np.vstack([KT1])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([200000])\n",
    "    A  = np.random.choice(5,len(S0))\n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabs7[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabs7[A[i]],S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        DT[indxNew],KT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],17+kk,1)\n",
    "        \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    A = np.array(A).reshape(len(A),1)\n",
    "    Par = np.hstack([DT,KT,S0])#,A])\n",
    "    Obs = np.hstack([1000*A,Obs])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiMSMin.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiMSMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b616327a-da51-46db-886f-b38c9d819b2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MK7Arr = []\n",
    "RK7Arr = []\n",
    "AK7Arr = []\n",
    "MKT7Arr = []\n",
    "KFA7Arr = []\n",
    "for kk in range(5):\n",
    "    Dat = Dats[kk]\n",
    "    ArrShape = Dat.shape[:2]\n",
    "    NoiseEst = np.zeros(list(ArrShape[:2])+[22])\n",
    "    VarEst   = np.zeros(ArrShape)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            torch.manual_seed(10)\n",
    "            if(np.sum(Dat[i,j,LesionSlices[kk]]) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                np.random.seed(1)\n",
    "                torch.manual_seed(1)\n",
    "                posterior_samples_1 = posteriorMin.sample((InferSamples,), x=np.hstack([kk*1000,Dat[i,j,LesionSlices[kk],Indxs7[kk]]]),show_progress_bars=False)\n",
    "                NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):  \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])\n",
    "    \n",
    "    MK7  = np.zeros(ArrShape)\n",
    "    AK7  = np.zeros(ArrShape)\n",
    "    RK7  = np.zeros(ArrShape)\n",
    "    MKT7 = np.zeros(ArrShape)\n",
    "    KFA7 = np.zeros(ArrShape)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK7[i,j] = Metrics[0]\n",
    "            AK7[i,j] = Metrics[1]\n",
    "            RK7[i,j] = Metrics[2]\n",
    "            MKT7[i,j] = Metrics[3]\n",
    "            KFA7[i,j] = Metrics[4]\n",
    "    MK7Arr.append(MK7)\n",
    "    RK7Arr.append(RK7)\n",
    "    AK7Arr.append(AK7)\n",
    "    MKT7Arr.append(MKT7)\n",
    "    KFA7Arr.append(KFA7)\n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK7Arr[kk].T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(AK7Arr[kk].T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(RK7Arr[kk].T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT7Arr[kk].T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA7Arr[kk].T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb99edf-c8b4-49e9-bf25-ef6c474cc82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK7NLArr = []\n",
    "RK7NLArr = []\n",
    "AK7NLArr = []\n",
    "MKT7NLArr = []\n",
    "KFA7NLArr = []\n",
    "for kk in range(5):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabs7[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(Dats[kk][:,:,LesionSlices[kk],Indxs7[kk]])\n",
    "    ArrShape = Dats[kk][:,:,LesionSlices[kk],0].shape\n",
    "    MK_NL7  = np.zeros(ArrShape)\n",
    "    AK_NL7  = np.zeros(ArrShape)\n",
    "    RK_NL7 = np.zeros(ArrShape)\n",
    "    MKT_NL7 = np.zeros(ArrShape)\n",
    "    KFA_NL7 = np.zeros(ArrShape)\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            if(np.sum(Dats[kk][i,j,LesionSlices[kk]]) == 0):\n",
    "                pass\n",
    "            else: \n",
    "                Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "                MK_NL7[i,j] = Metrics[0]\n",
    "                AK_NL7[i,j] = Metrics[1]\n",
    "                RK_NL7[i,j] = Metrics[2]\n",
    "                MKT_NL7[i,j] = Metrics[3]\n",
    "                KFA_NL7[i,j] = Metrics[4]\n",
    "    MK7NLArr.append(MK_NL7)\n",
    "    RK7NLArr.append(RK_NL7)\n",
    "    AK7NLArr.append(AK_NL7)\n",
    "    MKT7NLArr.append(MKT_NL7)\n",
    "    KFA7NLArr.append(KFA_NL7)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(AK_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(RK_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT_NL7.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA_NL7.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c97f8-02e8-4372-bf3b-de8ab495f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MK7Arr[1],cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'MK7SBI.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(MK7NLArr[1],cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'MK7NL.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ffd6e-9329-45bb-b0d0-3aed438013b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(AK7Arr[1],cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'AK7SBI.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9281271-26da-45f7-b9c5-69814d6ec8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(AK7NLArr[1],cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'AK7NL.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab1f884-7f39-4143-95a6-c8534dab444e",
   "metadata": {},
   "source": [
    "## h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1922f-56de-4fd6-8fdc-d274939e9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(RK7Arr[1],cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'RK7SBI.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(RK7NLArr[1],cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'RK7NL.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8198409a-9fd2-4f1d-8cc5-98570d544dd5",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98e2b9-241e-412a-8fdd-c4cde3679da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists(f\"{network_path}/DKIMultiMSFull.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiMSFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,200000)\n",
    "    #DT2,KT2 = GenDTKT([DT1_hak,DT2_hak],[x4_lak,R1_lak,x2_lak,R2_lak],12,10000)\n",
    "    \n",
    "    DT = np.vstack([DT1])\n",
    "    KT = np.vstack([KT1])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([200000])\n",
    "    A  = np.random.choice(5,len(S0))\n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabsF[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabsF[A[i]],S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        DT[indxNew],KT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],17+kk,1)\n",
    "        \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    A = np.array(A).reshape(len(A),1)\n",
    "    Par = np.hstack([DT,KT,S0])#,A])\n",
    "    Obs = np.hstack([1000*A,Obs])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiMSFull.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiMSFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f72cf-fd18-487c-9d21-5778462b6715",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullArr = []\n",
    "RKFullArr = []\n",
    "AKFullArr = []\n",
    "MKTFullArr = []\n",
    "KFAFullArr = []\n",
    "for kk in range(5):\n",
    "    Dat = Dats[kk]\n",
    "    ArrShape = Dat.shape[:2]\n",
    "    NoiseEst = np.zeros(list(ArrShape[:2])+[22])\n",
    "    VarEst   = np.zeros(ArrShape)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            torch.manual_seed(10)\n",
    "            if(np.sum(Dat[i,j,LesionSlices[kk]]) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                np.random.seed(1)\n",
    "                torch.manual_seed(1)\n",
    "                posterior_samples_1 = posteriorFull.sample((InferSamples,), x=np.hstack([kk*1000,Dat[i,j,LesionSlices[kk]]]),show_progress_bars=False)\n",
    "                NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):  \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])\n",
    "    \n",
    "    MK_SBIFull  = np.zeros(ArrShape)\n",
    "    AK_SBIFull  = np.zeros(ArrShape)\n",
    "    RK_SBIFull  = np.zeros(ArrShape)\n",
    "    MKT_SBIFull = np.zeros(ArrShape)\n",
    "    KFA_SBIFull = np.zeros(ArrShape)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK_SBIFull[i,j] = Metrics[0]\n",
    "            AK_SBIFull[i,j] = Metrics[1]\n",
    "            RK_SBIFull[i,j] = Metrics[2]\n",
    "            MKT_SBIFull[i,j] = Metrics[3]\n",
    "            KFA_SBIFull[i,j] = Metrics[4]\n",
    "    MKFullArr.append(MK_SBIFull)\n",
    "    RKFullArr.append(RK_SBIFull)\n",
    "    AKFullArr.append(AK_SBIFull)\n",
    "    MKTFullArr.append(MKT_SBIFull)\n",
    "    KFAFullArr.append(KFA_SBIFull)\n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(AK_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(RK_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6753752-5cca-4558-943a-25124e32411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullNLArr = []\n",
    "RKFullNLArr = []\n",
    "AKFullNLArr = []\n",
    "MKTFullNLArr = []\n",
    "KFAFullNLArr = []\n",
    "for kk in range(5):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gTabsF[kk],fit_method='NLLS')\n",
    "    dkifitNL = dkimodelNL.fit(Dats[kk][:,:,LesionSlices[kk],:])\n",
    "    ArrShape = Dats[kk][:,:,LesionSlices[kk],0].shape\n",
    "    MK_NLFull  = np.zeros(ArrShape)\n",
    "    AK_NLFull  = np.zeros(ArrShape)\n",
    "    RK_NLFull = np.zeros(ArrShape)\n",
    "    MKT_NLFull = np.zeros(ArrShape)\n",
    "    KFA_NLFull = np.zeros(ArrShape)\n",
    "    NoiseEst_NL = np.zeros(list(ArrShape)+[21])\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL[i,j] = np.hstack([dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt])\n",
    "    NoiseEst_NL2 =  np.zeros_like(NoiseEst_NL)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            NoiseEst_NL2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst_NL[i,j]))),NoiseEst_NL[i,j,6:]])\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            if(np.sum(Dats[kk][i,j,LesionSlices[kk]]) == 0):\n",
    "                pass\n",
    "            else: \n",
    "                Metrics = DKIMetrics(NoiseEst_NL2[i,j][:6],NoiseEst_NL2[i,j][6:21])\n",
    "                MK_NLFull[i,j] = Metrics[0]\n",
    "                AK_NLFull[i,j] = Metrics[1]\n",
    "                RK_NLFull[i,j] = Metrics[2]\n",
    "                MKT_NLFull[i,j] = Metrics[3]\n",
    "                KFA_NLFull[i,j] = Metrics[4]\n",
    "    MKFullNLArr.append(MK_NLFull)\n",
    "    RKFullNLArr.append(RK_NLFull)\n",
    "    AKFullNLArr.append(AK_NLFull)\n",
    "    MKTFullNLArr.append(MKT_NLFull)\n",
    "    KFAFullNLArr.append(KFA_NLFull)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(AK_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(RK_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA_NLFull.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba4cf00-85bf-42e8-8711-229634dead84",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK7NLArr[2][np.isnan(MK7NLArr[2])] = 0\n",
    "RK7NLArr[2][np.isnan(RK7NLArr[2])] = 0\n",
    "AK7NLArr[2][np.isnan(AK7NLArr[2])] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd900e-aff2-4d6b-8871-31104233ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_Comp_MK = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Masks[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = MKFullNLArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = MKFullArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_Comp_MK.append(ssim(Dat1,Dat2,data_range=np.max(Dat1)-np.min(Dat1)))\n",
    "print(SSIM_Comp_MK)\n",
    "\n",
    "SSIM_Comp_AK = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Masks[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = AKFullNLArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = AKFullArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_Comp_AK.append(ssim(Dat1,Dat2,data_range=np.max(Dat1)-np.min(Dat1)))\n",
    "print(SSIM_Comp_AK)\n",
    "\n",
    "SSIM_Comp_RK = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Masks[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = RKFullNLArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = RKFullArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_Comp_RK.append(ssim(Dat1,Dat2,data_range=np.max(Dat1)-np.min(Dat1)))\n",
    "print(SSIM_Comp_RK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0837b88e-cc00-48ae-a066-f8b1458fa06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_NL_MK = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Masks[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = MKFullNLArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = MK7NLArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_NL_MK.append(ssim(Dat1,Dat2,data_range=np.max(Dat1)-np.min(Dat1)))\n",
    "print(SSIM_NL_MK)\n",
    "\n",
    "SSIM_NL_AK = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Masks[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = AKFullNLArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = AK7NLArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_NL_AK.append(ssim(Dat1,Dat2,data_range=np.max(Dat1)-np.min(Dat1)))\n",
    "print(SSIM_NL_AK)\n",
    "\n",
    "SSIM_NL_RK = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Masks[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = RKFullNLArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = RK7NLArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_NL_RK.append(ssim(Dat1,Dat2,data_range=np.max(Dat1)-np.min(Dat1)))\n",
    "print(SSIM_NL_RK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d9810-2559-4f13-a169-74c4b9192ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_SBI_MK = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Masks[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = MKFullArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = MK7Arr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_SBI_MK.append(ssim(Dat1,Dat2,data_range=np.max(Dat1)-np.min(Dat1)))\n",
    "print(SSIM_SBI_MK)\n",
    "SSIM_SBI_AK = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Masks[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = AKFullArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = AK7Arr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_SBI_AK.append(ssim(Dat1,Dat2,data_range=np.max(Dat1)-np.min(Dat1)))\n",
    "print(SSIM_SBI_AK)\n",
    "SSIM_SBI_RK = []\n",
    "for ii in range(5):\n",
    "    bbox = np.argwhere(Masks[ii][:,:,LesionSlices[ii]])\n",
    "    min_coords = bbox.min(axis=0)\n",
    "    max_coords = bbox.max(axis=0)\n",
    "    Dat1 = RKFullArr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    Dat2 = RK7Arr[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1]]\n",
    "    #Ma   = Masks[ii][min_coords[0]:max_coords[0],min_coords[1]:max_coords[1],LesionSlices[ii]]\n",
    "    SSIM_SBI_RK.append(ssim(Dat1,Dat2,data_range=np.max(Dat1)-np.min(Dat1)))\n",
    "print(SSIM_SBI_RK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69cb4c0-b64d-4732-a6de-675f44eab6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Err_Comp_MK = []\n",
    "STD_SBI_MK_Full = []\n",
    "STD_NL_MK_Full = []\n",
    "for ii in range(5):\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = MKFullNLArr[ii][le==1]\n",
    "    Dat2 = MKFullArr[ii][le==1]\n",
    "    Err_Comp_MK.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_NL_MK_Full.append(np.std(Dat1))\n",
    "    STD_SBI_MK_Full.append(np.std(Dat2))\n",
    "    \n",
    "Err_NL_MK_L = []\n",
    "Err_NL_MK = []\n",
    "STD_NL_MK_Min = []\n",
    "for ii in range(5):\n",
    "    ma = Masks[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = MKFullNLArr[ii][ma]\n",
    "    Dat2 = MK7NLArr[ii][ma]\n",
    "    Err_NL_MK.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = MKFullNLArr[ii][le==1]\n",
    "    Dat2 = MK7NLArr[ii][le==1]\n",
    "    Err_NL_MK_L.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_NL_MK_Min.append(np.std(Dat2))\n",
    "    \n",
    "Err_SBI_MK_L = []\n",
    "Err_SBI_MK = []\n",
    "STD_SBI_MK_Min = []\n",
    "for ii in range(5):\n",
    "    ma = Masks[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = MKFullArr[ii]\n",
    "    Dat2 = MK7Arr[ii]\n",
    "    Err_SBI_MK.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = MKFullArr[ii][le==1]\n",
    "    Dat2 = MK7Arr[ii][le==1]\n",
    "    Err_SBI_MK_L.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_SBI_MK_Min.append(np.std(Dat2))\n",
    "\n",
    "Err_Comp_AK = []\n",
    "STD_SBI_AK_Full = []\n",
    "STD_NL_AK_Full = []\n",
    "for ii in range(5):\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = AKFullNLArr[ii][le==1]\n",
    "    Dat2 = AKFullArr[ii][le==1]\n",
    "    Err_Comp_AK.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_NL_AK_Full.append(np.std(Dat1))\n",
    "    STD_SBI_AK_Full.append(np.std(Dat2))\n",
    "    \n",
    "Err_NL_AK_L = []\n",
    "Err_NL_AK = []\n",
    "STD_NL_AK_Min = []\n",
    "for ii in range(5):\n",
    "    ma = Masks[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = AKFullNLArr[ii][ma]\n",
    "    Dat2 = AK7NLArr[ii][ma]\n",
    "    Err_NL_AK.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = AKFullNLArr[ii][le==1]\n",
    "    Dat2 = AK7NLArr[ii][le==1]\n",
    "    Err_NL_AK_L.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_NL_AK_Min.append(np.std(Dat2))\n",
    "    \n",
    "Err_SBI_AK_L = []\n",
    "Err_SBI_AK = []\n",
    "STD_SBI_AK_Min = []\n",
    "for ii in range(5):\n",
    "    ma = Masks[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = AKFullArr[ii]\n",
    "    Dat2 = AK7Arr[ii]\n",
    "    Err_SBI_AK.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = AKFullArr[ii][le==1]\n",
    "    Dat2 = AK7Arr[ii][le==1]\n",
    "    Err_SBI_AK_L.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_SBI_AK_Min.append(np.std(Dat2))\n",
    "\n",
    "Err_Comp_RK = []\n",
    "STD_SBI_RK_Full = []\n",
    "STD_NL_RK_Full = []\n",
    "for ii in range(5):\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = RKFullNLArr[ii][le==1]\n",
    "    Dat2 = RKFullArr[ii][le==1]\n",
    "    Err_Comp_RK.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_NL_RK_Full.append(np.std(Dat1))\n",
    "    STD_SBI_RK_Full.append(np.std(Dat2))\n",
    "    \n",
    "Err_NL_RK_L = []\n",
    "Err_NL_RK = []\n",
    "STD_NL_RK_Min = []\n",
    "for ii in range(5):\n",
    "    ma = Masks[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = RKFullNLArr[ii][ma]\n",
    "    Dat2 = RK7NLArr[ii][ma]\n",
    "    Err_NL_RK.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = RKFullNLArr[ii][le==1]\n",
    "    Dat2 = RK7NLArr[ii][le==1]\n",
    "    Err_NL_RK_L.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_NL_RK_Min.append(np.std(Dat2))\n",
    "    \n",
    "Err_SBI_RK_L = []\n",
    "Err_SBI_RK = []\n",
    "STD_SBI_RK_Min = []\n",
    "for ii in range(5):\n",
    "    ma = Masks[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = RKFullArr[ii]\n",
    "    Dat2 = RK7Arr[ii]\n",
    "    Err_SBI_RK.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    le = Lesions[ii][:,:,LesionSlices[ii]]\n",
    "    Dat1 = RKFullArr[ii][le==1]\n",
    "    Dat2 = RK7Arr[ii][le==1]\n",
    "    Err_SBI_RK_L.append(np.mean(np.abs(Dat1-Dat2)))\n",
    "    STD_SBI_RK_Min.append(np.std(Dat2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0840c-d80b-48b6-a698-2d10f62e55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "plt.scatter(0.6*np.ones(5),Err_Comp_MK,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Err_Comp_MK),'black', 'gray',positions=[0.6])\n",
    "\n",
    "box_plot(np.array([Err_SBI_MK]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1))\n",
    "plt.scatter(np.ones(5),np.array([Err_SBI_MK]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([Err_SBI_MK_L]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.15],hatch='//')\n",
    "plt.scatter(1.15*np.ones(5),np.array([Err_SBI_MK_L]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([Err_NL_MK]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.4])\n",
    "plt.scatter(1.4*np.ones(5),np.array([Err_NL_MK]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([Err_NL_MK_L]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.55],hatch='//')\n",
    "plt.scatter(1.55*np.ones(5),np.array([Err_NL_MK_L]),c='gray',zorder=10,alpha=0.5)\n",
    "plt.xticks([0.6,1.075,1.475],['Full \\n Comp.','SBI','NLLS'],fontsize=32,rotation=90)\n",
    "plt.xlim([0.4,1.7])\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=np.clip(WLSFit+0.2,0,1), edgecolor='k', label='Full \\n Img.',hatch='//'),\n",
    "    mpatches.Patch(facecolor=np.clip(WLSFit+0.2,0,1), edgecolor='k', label='Lesion',),\n",
    "]\n",
    "plt.legend(handles=legend_elements,loc = 'lower left',bbox_to_anchor=(-0.11,0.5),\n",
    "           fontsize=27,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "if Save: plt.savefig(FigLoc+'MKAbsErr.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84dd995-4aaa-4fc1-a8b4-424ccfca3ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "plt.scatter(0.6*np.ones(5),Err_Comp_AK,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Err_Comp_AK),'black', 'gray',positions=[0.6])\n",
    "\n",
    "box_plot(np.array([Err_SBI_AK]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1))\n",
    "plt.scatter(np.ones(5),np.array([Err_SBI_AK]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([Err_SBI_AK_L]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.15],hatch='//')\n",
    "plt.scatter(1.15*np.ones(5),np.array([Err_SBI_AK_L]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([Err_NL_AK]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.4])\n",
    "plt.scatter(1.4*np.ones(5),np.array([Err_NL_AK]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([Err_NL_AK_L]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.55],hatch='//')\n",
    "plt.scatter(1.55*np.ones(5),np.array([Err_NL_AK_L]),c='gray',zorder=10,alpha=0.5)\n",
    "plt.xticks([0.6,1.075,1.475],['Full \\n Comp.','SBI','NLLS'],fontsize=32,rotation=90)\n",
    "plt.xlim([0.4,1.7])\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "if Save: plt.savefig(FigLoc+'AKAbsErr.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6cabc2-a74a-4916-ab93-a03b9124adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "plt.scatter(0.6*np.ones(5),Err_Comp_RK,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(Err_Comp_RK),'black', 'gray',positions=[0.6])\n",
    "\n",
    "box_plot(np.array([Err_SBI_RK]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1))\n",
    "plt.scatter(np.ones(5),np.array([Err_SBI_RK]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([Err_SBI_RK_L]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.15],hatch='//')\n",
    "plt.scatter(1.15*np.ones(5),np.array([Err_SBI_RK_L]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([Err_NL_RK]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.4])\n",
    "plt.scatter(1.4*np.ones(5),np.array([Err_NL_RK]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([Err_NL_RK_L]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.55],hatch='//')\n",
    "plt.scatter(1.55*np.ones(5),np.array([Err_NL_RK_L]),c='gray',zorder=10,alpha=0.5)\n",
    "plt.xticks([0.6,1.075,1.475],['Full \\n Comp.','SBI','NLLS'],fontsize=32,rotation=90)\n",
    "plt.xlim([0.4,1.7])\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "if Save: plt.savefig(FigLoc+'RKAbsErr.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4104a94b-719f-4caa-ae42-bea276e37cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "plt.scatter(0.7*np.ones(5),SSIM_Comp_MK,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_Comp_MK),'black', 'gray',positions=[0.7])\n",
    "\n",
    "box_plot(np.array([SSIM_SBI_MK]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1))\n",
    "plt.scatter(np.ones(5),np.array([SSIM_SBI_MK]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([SSIM_NL_MK]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.3])\n",
    "plt.scatter(1.3*np.ones(5),np.array([SSIM_NL_MK]),c='gray',zorder=10,alpha=0.5)\n",
    "plt.axhline(0.66,c='k',ls='--')\n",
    "plt.ylim([0,1])\n",
    "plt.xticks([0.7,1,1.3],['Full \\n Comp.','SBI','NLLS'],fontsize=32,rotation=90)\n",
    "plt.xlim([0.5,1.5])\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.savefig(FigLoc+'MKErrors.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3d4e3-81bd-4ed9-871e-7b0887d06313",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "plt.scatter(0.7*np.ones(5),SSIM_Comp_AK,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_Comp_AK),'black', 'gray',positions=[0.7])\n",
    "\n",
    "box_plot(np.array([SSIM_SBI_AK]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1))\n",
    "plt.scatter(np.ones(5),np.array([SSIM_SBI_AK]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([SSIM_NL_AK]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.3])\n",
    "plt.scatter(1.3*np.ones(5),np.array([SSIM_NL_AK]),c='gray',zorder=10,alpha=0.5)\n",
    "plt.axhline(0.66,c='k',ls='--')\n",
    "plt.ylim([0,1])\n",
    "plt.xticks([0.7,1,1.3],['Full \\n Comp.','SBI','NLLS'],fontsize=32,rotation=90)\n",
    "plt.xlim([0.5,1.5])\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.savefig(FigLoc+'AKErrors.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962a3fb-90ea-483e-889e-5dbe3cb3ad80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "plt.scatter(0.7*np.ones(5),SSIM_Comp_RK,c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array(SSIM_Comp_RK),'black', 'gray',positions=[0.7])\n",
    "\n",
    "box_plot(np.array([SSIM_SBI_RK]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1))\n",
    "plt.scatter(np.ones(5),np.array([SSIM_SBI_RK]),c='gray',zorder=10,alpha=0.5)\n",
    "box_plot(np.array([SSIM_NL_RK]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.3])\n",
    "plt.scatter(1.3*np.ones(5),np.array([SSIM_NL_RK]),c='gray',zorder=10,alpha=0.5)\n",
    "plt.axhline(0.66,c='k',ls='--')\n",
    "plt.ylim([0,1])\n",
    "plt.xticks([0.7,1,1.3],['Full \\n Comp.','SBI','NLLS'],fontsize=32,rotation=90)\n",
    "plt.xlim([0.5,1.5])\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.savefig(FigLoc+'RKErrors.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cc8fa-034a-40b0-8b11-2e1297cc7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "box_plot(np.array([STD_SBI_MK_Full]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.7])\n",
    "plt.scatter(0.7*np.ones(5),np.array([STD_SBI_MK_Full]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_SBI_MK_Min]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.9])\n",
    "plt.scatter(0.9*np.ones(5),np.array([STD_SBI_MK_Min]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_NL_MK_Full]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.2])\n",
    "plt.scatter(1.2*np.ones(5),np.array([STD_NL_MK_Full]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_NL_MK_Min]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.4])\n",
    "plt.scatter(1.4*np.ones(5),np.array([STD_NL_MK_Min]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "plt.xticks([0.7,0.9,1.2,1.4],['Full','Min','Full','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "plt.xlim([0.5,1.5])\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "\n",
    "x = np.arange(0.6,1.0,0.01)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(STD_SBI_MK_Full)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(STD_SBI_MK_Full)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(1.1,1.5,0.01)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(STD_NL_MK_Full)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(STD_NL_MK_Full)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "plt.savefig(FigLoc+'MKSTD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c54a4-2a0b-4139-aef4-d5016615c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "box_plot(np.array([STD_SBI_RK_Full]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.7])\n",
    "plt.scatter(0.7*np.ones(5),np.array([STD_SBI_RK_Full]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_SBI_RK_Min]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.9])\n",
    "plt.scatter(0.9*np.ones(5),np.array([STD_SBI_RK_Min]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_NL_RK_Full]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.2])\n",
    "plt.scatter(1.2*np.ones(5),np.array([STD_NL_RK_Full]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_NL_RK_Min]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.4])\n",
    "plt.scatter(1.4*np.ones(5),np.array([STD_NL_RK_Min]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "plt.xticks([0.7,0.9,1.2,1.4],['Full','Min','Full','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "plt.xlim([0.5,1.5])\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "\n",
    "x = np.arange(0.6,1.0,0.01)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(STD_SBI_RK_Full)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(STD_SBI_RK_Full)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(1.1,1.5,0.01)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(STD_NL_RK_Full)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(STD_NL_RK_Full)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "plt.savefig(FigLoc+'RKSTD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12500bc8-7e2b-4b26-bb0a-f4ddcf799a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(3.2,4.8))\n",
    "box_plot(np.array([STD_SBI_AK_Full]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.7])\n",
    "plt.scatter(0.7*np.ones(5),np.array([STD_SBI_AK_Full]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_SBI_AK_Min]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.9])\n",
    "plt.scatter(0.9*np.ones(5),np.array([STD_SBI_AK_Min]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_NL_AK_Full]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.2])\n",
    "plt.scatter(1.2*np.ones(5),np.array([STD_NL_AK_Full]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "box_plot(np.array([STD_NL_AK_Min]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),positions=[1.4])\n",
    "plt.scatter(1.4*np.ones(5),np.array([STD_NL_AK_Min]),c='gray',zorder=10,alpha=0.5)\n",
    "\n",
    "plt.xticks([0.7,0.9,1.2,1.4],['Full','Min','Full','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "plt.xlim([0.5,1.5])\n",
    "plt.yticks([0.05,0.07,0.09,0.11,0.13,0.15])\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "\n",
    "x = np.arange(0.6,1.0,0.01)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(STD_SBI_AK_Full)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(STD_SBI_AK_Full)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=SBIFit,zorder=10,alpha=0.5,hatch='//')\n",
    "\n",
    "x = np.arange(1.1,1.5,0.01)\n",
    "y1 = np.ones_like(x)*1.1*np.mean(STD_NL_AK_Full)\n",
    "y2 = np.ones_like(x)*0.9*np.mean(STD_NL_AK_Full)\n",
    "\n",
    "plt.fill_between(x,y1,y2,color=WLSFit,zorder=10,alpha=0.5,hatch='//')\n",
    "plt.savefig(FigLoc+'AKSTD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e18650-1b9f-4183-968b-37a2bb03cba5",
   "metadata": {},
   "source": [
    "# Supplemental Fig 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3801716-05b9-4af7-8a5d-f1d45e89ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S1/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b68c1b-38ba-4a9e-9a55-0d142227b712",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dcf0ce-e7ae-4e56-8971-42d000ca782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial = HemiSphere(xyz=bvecs[1:])\n",
    "hsph_initial20 = HemiSphere(xyz=bvecs[1:20])\n",
    "hsph_initial7 = HemiSphere(xyz=bvecs[1:7])\n",
    "hsph_updated,potentials = disperse_charges(hsph_initial,5000)\n",
    "hsph_updated20,potentials = disperse_charges(hsph_initial20,5000)\n",
    "hsph_updated7,potentials = disperse_charges(hsph_initial7,5000)\n",
    "\n",
    "gtabSimF = gradient_table(np.array([0]+[1000]*64).squeeze(), np.vstack([[0,0,0],hsph_updated.vertices]))\n",
    "gtabSim20 = gradient_table(np.array([0]+[1000]*19).squeeze(), np.vstack([[0,0,0],hsph_updated20.vertices]))\n",
    "gtabSim7 = gradient_table(np.array([0]+[1000]*6).squeeze(), np.vstack([[0,0,0],hsph_updated7.vertices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b3bc9-5186-45de-9167-fe7986b01ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FullDat = []\n",
    "S0Full  = []\n",
    "DTIFull = []\n",
    "for i in tqdm.tqdm(range(1,6)):\n",
    "    fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    print('maskdata.shape (%d, %d, %d, %d)' % maskdata.shape)\n",
    "    \n",
    "    TestData = maskdata[:, :, axial_middle, :]\n",
    "    FlatTD = TestData.reshape(maskdata.shape[0]*maskdata.shape[1],69)\n",
    "    FlatTD = FlatTD[FlatTD.sum(axis=-1)>0]\n",
    "    FlatTD = FlatTD[~np.array(FlatTD<0).any(axis=-1)]\n",
    "    FullDat.append(FlatTD)\n",
    "    # Fit the tensor model to the DWI data with return_S0_hat=True\n",
    "    tenmodel = dti.TensorModel(gtabHCP, return_S0_hat=True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(FlatTD)\n",
    "    DTIHCP = tenfit.quadratic_form\n",
    "    DTIFull.append(DTIHCP)\n",
    "    # Get the estimated S0_hat values\n",
    "    S0HCP = tenfit.S0_hat\n",
    "    S0Full.append(S0HCP)\n",
    "DTIFull = np.concatenate(DTIFull)\n",
    "FullDat = np.concatenate(FullDat)\n",
    "S0Full = np.hstack(S0Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa5ef6-a06f-45be-9f2b-d8826c2b1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "Samples  = []\n",
    "DTISim = []\n",
    "S0Sim    = []\n",
    "# Define the lower and upper bounds\n",
    "\n",
    "lower_abs,upper_abs = -0.07,0.07\n",
    "lower_rest,upper_rest = -0.015,0.015\n",
    "lower_S0 = 25\n",
    "upper_S0 = 2000\n",
    "\n",
    "custom_prior = DTIPriorS0(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0)\n",
    "prior, *_ = process_prior(custom_prior) \n",
    "\n",
    "params = prior.sample([5000])\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim.append(dt)\n",
    "    S0Sim.append(params[i,-1])\n",
    "    Samples.append([CustomSimulator(dt,gtabSimF, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples = np.array(Samples).squeeze()\n",
    "Samples = np.moveaxis(Samples, 0, -1)\n",
    "\n",
    "DTISim = np.array(DTISim)\n",
    "\n",
    "MDSim = [np.mean(np.linalg.eigh(B)[0]) for B in DTISim]\n",
    "MDHCP = [np.mean(np.linalg.eigh(B)[0]) for B in DTIFull]\n",
    "\n",
    "FASim = [FracAni(np.linalg.eigh(B)[0],m) for B,m in zip(DTISim,MDSim)]\n",
    "FAHCP = [FracAni(np.linalg.eigh(B)[0],m) for B,m in zip(DTIFull,MDHCP)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d5b6b-418d-400d-9dd3-2f6ee6f55064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a7176c-c965-4336-8219-30b36f8a14f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(S0Sim,density=True,stacked=True,alpha=0.75,label='Simulated',color=SBIFit)\n",
    "plt.hist(S0Full,density=True,stacked=True,alpha=0.75,label='HCP',color='gray')\n",
    "plt.legend(fontsize=32,loc=1,bbox_to_anchor=(1,1))\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=32)\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'S0Dist.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589eb927-dca9-4964-bb7d-978e6ca9845d",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932280ec-76ac-4ca5-ae9e-fabb83cd6ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(MDSim,density=True,stacked=True,label='Simulated samples',color=SBIFit)\n",
    "plt.hist(MDHCP,density=True,stacked=True,alpha=0.75,label='HPC subset',color='gray')\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=32)\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'MDDist.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74040569-c9e1-4a56-a304-1cd6f998845f",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221d6e5-0a0d-4803-ac35-a250ca3ec7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(FASim,density=True,label='Simulated samples',color=SBIFit)\n",
    "plt.hist(FAHCP,density=True,alpha=0.75,label='HPC subset',color='gray')\n",
    "plt.yticks(fontsize=32)\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'FADist.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82722113-e886-4bb9-b8fb-fc72f92755f3",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af6507-b9a6-4949-b45f-7c368fb7b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(3,3,figsize=(12,12))\n",
    "ax = axs.ravel()\n",
    "ax[0].hist(DTISim[:,0,0],density=True,color=SBIFit)\n",
    "ax[1].hist(DTISim[:,0,1],density=True,color=SBIFit)\n",
    "ax[2].hist(DTISim[:,0,2],density=True,color=SBIFit)\n",
    "ax[4].hist(DTISim[:,1,1],density=True,color=SBIFit)\n",
    "ax[5].hist(DTISim[:,1,2],density=True,color=SBIFit)\n",
    "ax[-1].hist(DTISim[:,2,2],density=True,color=SBIFit)\n",
    "\n",
    "\n",
    "ax[0].hist(DTIFull[:,0,0],density=True,alpha=0.75,color='gray')\n",
    "ax[1].hist(DTIFull[:,0,1],density=True,alpha=0.75,color='gray')\n",
    "ax[2].hist(DTIFull[:,0,2],density=True,alpha=0.75,color='gray')\n",
    "ax[4].hist(DTIFull[:,1,1],density=True,alpha=0.75,color='gray')\n",
    "ax[5].hist(DTIFull[:,1,2],density=True,alpha=0.75,color='gray')\n",
    "ax[-1].hist(DTIFull[:,2,2],density=True,alpha=0.75,color='gray')\n",
    "ax[3].axis('off')\n",
    "ax[-2].axis('off')\n",
    "ax[-3].axis('off')\n",
    "\n",
    "for a in ax:\n",
    "    a.tick_params(axis='x', labelsize=32)\n",
    "    a.tick_params(axis='y', labelsize=32)\n",
    "    a.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    a.ticklabel_format(axis='x',style='sci',scilimits=(-1,1))\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'DTDist.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d809d11-fe9d-4629-90f7-ddc62701f3cd",
   "metadata": {},
   "source": [
    "# Supplemental Fig 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c47464-effd-40d0-8afa-902556f457ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S2/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b497e-ab78-43ba-b6c2-0c0605f25a07",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0e5e7-6a95-4381-a620-ef4d7bf2759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FullDat = []\n",
    "S0Full  = []\n",
    "DKIFull = []\n",
    "DTIFull = []\n",
    "for i in tqdm.tqdm(range(1,6)):\n",
    "    fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    \n",
    "    fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "    bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    FlatTD = TestData.reshape(maskdata.shape[0]*maskdata.shape[1],138)\n",
    "    FlatTD = FlatTD[FlatTD[:,:69].sum(axis=-1)>0]\n",
    "    FlatTD = FlatTD[~np.array(FlatTD<0).any(axis=-1)]\n",
    "    FullDat.append(FlatTD)\n",
    "    \n",
    "    dkimodel = dki.DiffusionKurtosisModel(gtabExt)\n",
    "    tenfit = dkimodel.fit(FlatTD)\n",
    "    DKIHCP = tenfit.kt\n",
    "    DTIHCP = tenfit.lower_triangular()\n",
    "    DTIFull.append(DTIHCP)\n",
    "    DKIFull.append(DKIHCP)\n",
    "    # Get the estimated S0_hat values\n",
    "    S0HCP = tenfit.S0_hat\n",
    "    S0Full.append(S0HCP)\n",
    "DKIFull = np.concatenate(DKIFull)\n",
    "DTIFull = np.concatenate(DTIFull)\n",
    "\n",
    "DTIFilt_all = DTIFull[(abs(DKIFull)<10).all(axis=1)]\n",
    "DKIFilt_all = DKIFull[(abs(DKIFull)<10).all(axis=1)]\n",
    "DTIFilt_all = DTIFilt_all[(DKIFilt_all>-3/7).all(axis=1)]\n",
    "DKIFilt_all = DKIFilt_all[(DKIFilt_all>-3/7).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcedcb4-f51e-4bba-9b2f-5cd37235c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(DTIFilt[:,5],bins=30,density=True,color=WLSFit,alpha=0.5,label='1 HCP Indv.')\n",
    "plt.hist(DTIFilt_all[:,5],bins=30,density=True,color=SBIFit,alpha=0.5,label='All HCP')\n",
    "plt.legend(fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.3,loc=1)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Comp1.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90235f98-375a-4945-b689-22c165cd1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(DTIFilt[:,1],bins=30,density=True,color=WLSFit,alpha=0.5)\n",
    "plt.hist(DTIFilt_all[:,1],bins=30,density=True,color=SBIFit,alpha=0.5)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=24)\n",
    "if Save: plt.savefig(FigLoc+'Comp2.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e157a-114e-45e0-8566-664cf4622938",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(DKIFilt[:,0],bins=30,density=True,color=WLSFit,alpha=0.5)\n",
    "plt.hist(DKIFilt_all[:,0],bins=30,density=True,color=SBIFit,alpha=0.5)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Comp3.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b628d-4d15-4780-ae5e-1a7ef6e75681",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(DKIFilt[:,3],bins=30,density=True,color=WLSFit,alpha=0.5)\n",
    "plt.hist(DKIFilt_all[:,3],bins=30,density=True,color=SBIFit,alpha=0.5)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Comp4.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d135e4-accf-455f-92f3-442bef8dfa12",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95554ec-627e-4329-8817-6a782ba3b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTISim = np.array(DTISim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330daecb-a855-4ada-9b5f-9c5381dcfe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DTIFilt[:,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[:,5],bins=30,density=True,color=WLSFit)\n",
    "plt.hist(np.array(DTISim)[:,0,0],bins=30,density=True,alpha=0.5,color='gray')\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.0014,600,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Normal1.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "#DT_rest\n",
    "data = DTIFilt[:,1]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti2_fitted = stats.norm(loc=loc, scale=scale)\n",
    "\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.norm(loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[:,1],bins=30,density=True,color=WLSFit,label='HCP data')\n",
    "plt.hist(DTISim[:,1,0],bins=30,density=True,alpha=0.5,color='gray',label='DTI prior')\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit,label='stat. fit')\n",
    "plt.text(0.00011,2000,\"Normal, \\n mean = {:.2f},\\n S.D. = {:.2e} \\n\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=24)\n",
    "plt.legend(fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1,loc=1,bbox_to_anchor=(0.52,1))\n",
    "if Save: plt.savefig(FigLoc+'Normal2.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "data = DKIFilt[:,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "x4_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DKIFilt[:,0],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,x4_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(1,0.8,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Normal3.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Fitting R1\n",
    "data = DKIFilt[:,3]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "R1_fitted = stats.norm(loc,scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "plt.hist(DKIFilt[:,3],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,R1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.1,3,\"Normal, \\n mean = {:.2f},\\n S.D. = {:.2e} \\n\".format(shape,loc),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Normal4.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fbe689-69d0-4824-9f53-9b5b88f5b079",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d4187-d809-48f6-824b-0a58ee501c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = TrueMets[:,-1]<0.3\n",
    "data = DTIFilt[mask,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[mask,5],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.0014,600,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'lowFA1.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "#DT_rest\n",
    "data = DTIFilt[mask,1]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti2_fitted = stats.norm(loc=loc, scale=scale)\n",
    "\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.norm(loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[mask,1],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.00011,2000,\"Normal, \\n mean = {:.2f},\\n S.D. = {:.2e} \\n\".format(shape,loc,scale),\n",
    "        fontsize=24)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'lowFA2.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = DKIFilt[mask,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "x4_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DKIFilt[mask,0],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,x4_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(1,0.8,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'lowFA3.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Fitting R1\n",
    "data = DKIFilt[mask,3]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "R1_fitted = stats.norm(loc,scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "plt.hist(DKIFilt[mask,3],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,R1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.05,3,\"Normal, \\n mean = {:.2f},\\n S.D. = {:.2e} \\n\".format(shape,loc),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'lowFA4.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b054f-6dda-48f3-9861-dc1057947a96",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8aa79-feec-4b85-8b63-48e5c04c0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = TrueMets[:,-1]>0.7\n",
    "data = DTIFilt[mask,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[mask,0],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.0008,1400,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=24)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'HighFA1.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "#DT_rest\n",
    "data = DTIFilt[mask,1]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti2_fitted = stats.norm(loc=loc, scale=scale)\n",
    "\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.norm(loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[mask,1],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.00013,1600,\"Normal, \\n mean = {:.2f},\\n S.D. = {:.2e} \\n\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'HighFA2.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = DKIFilt[mask,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "x4_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DKIFilt[mask,0],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,x4_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(1.3,0.5,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=24)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'HighFA3.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Fitting R1\n",
    "data = DKIFilt[mask,3]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "R1_fitted = stats.norm(loc,scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "plt.hist(DKIFilt[mask,3],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,R1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.3,1,\"Normal, \\n mean = {:.2f},\\n S.D. = {:.2e} \\n\".format(shape,loc),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'HighFA4.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af576c3-5642-463e-b549-ee4fa7a7e76e",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b85a0-6c6f-453b-9f39-31df9d6842d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j=0,0\n",
    "mask = (TrueMets[:,-1]<0.7)*(TrueMets[:,-1]>0.3)\n",
    "plt.scatter(DTIFilt[mask,i],DKIFilt[mask,j],color=np.clip(WLSFit-0.5,0,1),label='HCP data')\n",
    "mask = TrueMets[:,-1]>0.7\n",
    "plt.scatter(DTIFilt[mask,i],DKIFilt[mask,j],color=np.clip(WLSFit,0,1),marker='v'\n",
    "            ,label='HCP data (KFA$>$0.7)')\n",
    "mask = TrueMets[:,-1]<0.3\n",
    "plt.scatter(DTIFilt[mask,i],DKIFilt[mask,j],color=np.clip(WLSFit-0.3,0,1),marker='^'\n",
    "            ,label='HCP data (KFA$<$0.3)')\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.legend(fontsize=20,loc=1,bbox_to_anchor=(1,1),handlelength=0.4,handletextpad=0.4,markerscale=2)\n",
    "if Save: plt.savefig(FigLoc+'Scatter1Dat.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98346248-95b7-4747-aa72-d0d1817fade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(4*3000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(4*1000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(4*3000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(4*3000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(4*3000))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796e8c26-f229-4820-bc24-8413fb46caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,300)\n",
    "DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,100)\n",
    "DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,300)\n",
    "DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,300)\n",
    "DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,300)\n",
    "\n",
    "\n",
    "DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT1,KT1)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest1 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT2,KT2)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest2 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT3,KT3)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest3 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT4,KT4)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest4 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT5,KT5)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest5 = np.array(ParMets)\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT,KT)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest = np.array(ParMets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9bd5a-3942-4b2d-a664-e710af07b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j=0,0\n",
    "mask = (ParTest[:,-1]<0.7)*(ParTest[:,-1]>0.3)\n",
    "plt.scatter(DT[mask,i],KT[mask,j],color=np.clip(SBIFit-0.5,0,1),label='Sim. data')\n",
    "mask = ParTest[:,-1]>0.7\n",
    "plt.scatter(DT[mask,i],KT[mask,j],color=np.clip(SBIFit+0.2,0,1),marker='v',label='Sim. data (KFA$>$0.7)')\n",
    "mask = ParTest[:,-1]<0.3\n",
    "plt.scatter(DT[mask,i],KT[mask,j],color=np.clip(SBIFit-0.3,0,1),marker='^',label='Sim. data (KFA$>$0.7)')\n",
    "plt.xlim((2.6217407288099334e-05, 0.004342572522996818))\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.legend(fontsize=20,loc=1,bbox_to_anchor=(1,1),handlelength=0.4,handletextpad=0.4,markerscale=2)\n",
    "if Save: plt.savefig(FigLoc+'Scatter1Sim.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b61b59-167b-41c7-bbfc-df12ce3aa23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j=9,0\n",
    "mask = (TrueMets[:,-1]<0.7)*(TrueMets[:,-1]>0.3)\n",
    "plt.scatter(DKIFilt[mask,i],DKIFilt[mask,j],color=np.clip(WLSFit-0.5,0,1))\n",
    "mask = TrueMets[:,-1]>0.7\n",
    "plt.scatter(DKIFilt[mask,i],DKIFilt[mask,j],color=np.clip(WLSFit+0.2,0,1),marker='v')\n",
    "mask = TrueMets[:,-1]<0.3\n",
    "plt.scatter(DKIFilt[mask,i],DKIFilt[mask,j],color=np.clip(WLSFit-0.3,0,1),marker='^')\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "if Save: plt.savefig(FigLoc+'Scatter2Dat.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ab6a6-b013-4a25-8a6c-cbd5fc28e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j=9,0\n",
    "mask = (ParTest[:,-1]<0.7)*(ParTest[:,-1]>0.3)\n",
    "plt.scatter(KT[mask,i],KT[mask,j],color=np.clip(SBIFit-0.5,0,1))\n",
    "mask = ParTest[:,-1]>0.7\n",
    "plt.scatter(KT[mask,i],KT[mask,j],color=np.clip(SBIFit+0.2,0,1),marker='v')\n",
    "mask = ParTest[:,-1]<0.3\n",
    "plt.scatter(KT[mask,i],KT[mask,j],color=np.clip(SBIFit-0.3,0,1),marker='^')\n",
    "plt.xlim((-0.07662077262433849, 1.0282445059644276))\n",
    "plt.ylim((-0.5425279356172327, 4.178757181686705))\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "if Save: plt.savefig(FigLoc+'Scatter2Sim.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a2ca0-6bcc-45a2-a7ac-87e1e7ec456b",
   "metadata": {},
   "source": [
    "# Supplemental Fig 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81ee19-bcee-4743-b5b7-343339e3c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S3/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fece82-06d8-42d8-93b1-5d9b6db2f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimMid.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimMid.pickle\", \"rb\") as handle:\n",
    "        posterior20 = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSim20,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior20 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTISimMid.pickle\"):\n",
    "        with open(f\"{network_path}/DTISimMid.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior20, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc27b3-b17e-48f6-b74e-f5185445fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "Error20 = []\n",
    "NoiseApprox20 = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim20[i])\n",
    "        tObs = Samples20[k,:,i]\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSim20, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posterior20.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSim20,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApprox20.append(ENoise)\n",
    "    Error20.append(ErrorN2)\n",
    "\n",
    "NoiseApprox20 = np.array(NoiseApprox20)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15f5fc-86fd-4f28-8576-34f49cedd22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k,gtab,Samps,DTIS = 20,gtabSim20,Samples20,DTISim20\n",
    "tenmodel = dti.TensorModel(gtab,fit_method='NLLS')\n",
    "Error_n = []\n",
    "for S,Noise in zip(Samps,NoiseLevels):\n",
    "    Error = []\n",
    "    for i in range(500):\n",
    "        tenfit = tenmodel.fit(S[:,i])\n",
    "        tensor_vals = dti.lower_triangular(tenfit.quadratic_form)\n",
    "        DT_test = vals_to_mat(tensor_vals)\n",
    "        Error.append(Errors(DT_test,DTIS[i],gtab,Samps[0][:,i],S[:,i]))\n",
    "    Error_n.append(Error)\n",
    "Error_n = np.array(Error_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e604e-b904-44b4-87cb-c68ea28e15c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,4,figsize=(18,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(Error20).T,Errors_name)):\n",
    "    plt.sca(a) \n",
    "    bp = box_plot(E[:,1:].T,SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3,2.3,3.3,4.3],showfliers=False,widths=0.3)\n",
    "    bp2 = box_plot(Error_n[1:,:,ll],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    plt.sca(a)\n",
    "    if(ll>3):\n",
    "        plt.xlabel('SNR', fontsize=24)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    plt.yticks(fontsize=32)\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'SimDatDTIErrors1_20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "\n",
    "fig,axs = plt.subplots(1,4,figsize=(18,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(Error20).T[4:],Errors_name[4:])):\n",
    "    plt.sca(a) \n",
    "    bp = box_plot(E[:,1:].T,SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3,2.3,3.3,4.3],showfliers=False,widths=0.3)\n",
    "    bp2 = box_plot(Error_n[1:,:,ll+4],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    plt.sca(a)\n",
    "    if(ll>3):\n",
    "        plt.xlabel('SNR', fontsize=24)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    plt.yticks(fontsize=32)\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'SimDatDTIErrors2_20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63518d1-daa7-449b-824a-10a049e0584f",
   "metadata": {},
   "source": [
    "# Supplemental Fig 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc566e78-1e2c-4a4c-8150-f53533f9fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S4/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbcbd1-a293-4dba-9e03-402954b86243",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93adef-ef95-4c78-9484-bc22e83e58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "Samples  = []\n",
    "DTISim = []\n",
    "S0Sim    = []\n",
    "\n",
    "params = priorS0.sample([5000])\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim.append(dt)\n",
    "    S0Sim.append(params[i,-1])\n",
    "    Samples.append([CustomSimulator(dt,gtabSimF, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples = np.array(Samples).squeeze()\n",
    "Samples = np.moveaxis(Samples, 0, -1)\n",
    "\n",
    "Samples20  = []\n",
    "DTISim20 = []\n",
    "S0Sim20    = []\n",
    "\n",
    "params = priorS0.sample([5000])\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim20.append(dt)\n",
    "    S0Sim20.append(params[i,-1])\n",
    "    Samples20.append([CustomSimulator(dt,gtabSim20, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples20 = np.array(Samples20).squeeze()\n",
    "Samples20 = np.moveaxis(Samples20, 0, -1)\n",
    "\n",
    "Samples7  = []\n",
    "DTISim7 = []\n",
    "S0Sim7    = []\n",
    "\n",
    "params = priorS0.sample([5000])\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim7.append(dt)\n",
    "    S0Sim7.append(params[i,-1])\n",
    "    Samples7.append([CustomSimulator(dt,gtabSim7, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples7 = np.array(Samples7).squeeze()\n",
    "Samples7 = np.moveaxis(Samples7, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60dfef-1fc0-47df-8b59-c6f2d27f73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimFull.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSimF,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{save_path}/DTISimFull.pickle\"):\n",
    "        with open(f\"{save_path}/DTISimFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d536c-98f3-417c-8cb1-94ae931fc728",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "ErrorFull = []\n",
    "NoiseApproxFull = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim[i])\n",
    "        tObs = Samples[k,:,i]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSimF, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        mat_guess = clip_negative_eigenvalues(mat_guess)\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSimF,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApproxFull.append(ENoise)\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "NoiseApproxFull = np.array(NoiseApproxFull)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f91f0-535f-4a43-98ba-d0754cbfa186",
   "metadata": {},
   "outputs": [],
   "source": [
    "k,gtab,Samps,DTIS = 65,gtabSimF,Samples,DTISim\n",
    "tenmodel = dti.TensorModel(gtab,fit_method='NLLS')\n",
    "Error_n = []\n",
    "for S,Noise in zip(Samps,NoiseLevels):\n",
    "    Error = []\n",
    "    for i in range(500):\n",
    "        tenfit = tenmodel.fit(S[:,i])\n",
    "        tensor_vals = dti.lower_triangular(tenfit.quadratic_form)\n",
    "        DT_test = vals_to_mat(tensor_vals)\n",
    "        Error.append(Errors(DT_test,DTIS[i],gtab,Samps[0][:,i],S[:,i]))\n",
    "    Error_n.append(Error)\n",
    "Error_n = np.array(Error_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0850f-ab5a-48fb-a86f-b7f73dc57a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,6,figsize=(27,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(ErrorFull).T[2:],Errors_name[2:])):\n",
    "    plt.sca(a) \n",
    "    bp = box_plot(E[:,1:].T,SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3,2.3,3.3,4.3],showfliers=False,widths=0.3)\n",
    "    bp2 = box_plot(Error_n[1:,:,ll+2],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    plt.sca(a)\n",
    "    if(ll>3):\n",
    "        plt.xlabel('SNR', fontsize=24)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    plt.yticks(fontsize=32)\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    if(ll==1):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.05),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(ll==0):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=WLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'SimDatDTIErrors2.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337fb9d6-fe66-468d-9d77-260bf06836da",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08490819-b1bd-49af-b904-2125038bd4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimMin.pickle\", \"rb\") as handle:\n",
    "        posterior7 = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSim7,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior7 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTISimMin.pickle\"):\n",
    "        with open(f\"{network_path}/DTISimMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f27ab-7be6-415d-864e-b1f927cc1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "Error7 = []\n",
    "NoiseApprox7 = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim7[i])\n",
    "        tObs = Samples7[k,:,i]\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSim7, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posterior7.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSim7,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApprox7.append(ENoise)\n",
    "    Error7.append(ErrorN2)\n",
    "\n",
    "NoiseApprox7 = np.array(NoiseApprox7)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7911f26-40c8-4651-8596-8fd01ecc6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "k,gtab,Samps,DTIS = 7,gtabSim7,Samples7,DTISim7\n",
    "tenmodel = dti.TensorModel(gtab,fit_method='NLLS')\n",
    "Error_n = []\n",
    "for S,Noise in zip(Samps,NoiseLevels):\n",
    "    Error = []\n",
    "    for i in range(500):\n",
    "        tenfit = tenmodel.fit(S[:,i])\n",
    "        tensor_vals = dti.lower_triangular(tenfit.quadratic_form)\n",
    "        DT_test = vals_to_mat(tensor_vals)\n",
    "        Error.append(Errors(DT_test,DTIS[i],gtab,Samps[0][:,i],S[:,i]))\n",
    "    Error_n.append(Error)\n",
    "Error_n = np.array(Error_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d072e4c-cd58-40ed-a3de-1b40bfcf3ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,6,figsize=(27,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(Error7).T[2:],Errors_name[2:])):\n",
    "    plt.sca(a) \n",
    "    bp = box_plot(E[:,1:].T,SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3,2.3,3.3,4.3],showfliers=False,widths=0.3)\n",
    "    bp2 = box_plot(Error_n[1:,:,ll+2],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    plt.sca(a)\n",
    "    if(ll>3):\n",
    "        plt.xlabel('SNR', fontsize=24)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    plt.yticks(fontsize=32)\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    if(ll==1):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.25),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(ll==0):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=WLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(-0.1,1.25),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'SimDatDTIErrors2_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4492f464-2463-40f0-8141-f670e0a8f56c",
   "metadata": {},
   "source": [
    "# Supplemental Fig 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb1dec-8bbc-457e-b3ac-0ac242b083dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import lognorm,norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5f5c98-c5a8-4bd0-bcd9-b95d89f6a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S5/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a0720-ced2-40b0-930c-a85fd854d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT1_hak,DT2_hak = FitDT(DTIFilt[TrueMets[:,1]>0.99,:],1)\n",
    "x4_hak,R1_hak,x2_hak,R2_hak = FitKT(DKIFilt[TrueMets[:,1]>0.99,:],1)\n",
    "DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,300)\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT5,KT5)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest5 = np.array(ParMets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02eeb3-29e9-42e6-8ce1-35ee70fdb735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full fit\n",
    "DT1_full,DT2_full = FitDT(DTIFilt,1)\n",
    "x4_full,R1_full,x2_full,R2_full = FitKT(DKIFilt,1)\n",
    "\n",
    "# LowFA Fit\n",
    "DT1_lfa,DT2_lfa = FitDT(DTIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "x4_lfa,R1_lfa,x2_lfa,R2_lfa = FitKT(DKIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "\n",
    "# HighFA Fit\n",
    "DT1_hfa,DT2_hfa = FitDT(DTIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "x4_hfa,R1_hfa,x2_hfa,R2_hfa = FitKT(DKIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "\n",
    "# UltraLowFA Fit\n",
    "DT1_ulfa,DT2_ulfa = FitDT(DTIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa = FitKT(DKIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "\n",
    "# HigherAK Fit\n",
    "DT1_hak,DT2_hak = FitDT(DTIFilt[TrueMets[:,1]>0.8,:],1)\n",
    "x4_hak,R1_hak,x2_hak,R2_hak = FitKT(DKIFilt[TrueMets[:,1]>0.8,:],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c51a25-e6be-41cb-a904-e39ddb0785b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,300)\n",
    "DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,100)\n",
    "DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,300)\n",
    "DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,300)\n",
    "DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,600)\n",
    "\n",
    "\n",
    "DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT1,KT1)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest1 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT2,KT2)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest2 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT3,KT3)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest3 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT4,KT4)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest4 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT5,KT5)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest5 = np.array(ParMets)\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT,KT)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest = np.array(ParMets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410c919-729e-4474-a493-71b4c798e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.hist(ParTest[:,i],density=True,range=[0,1],color=SBIFit,label='Simulated')\n",
    "    plt.hist(TrueMets[:,i],alpha=0.8,density=True,range=[0,1],color='gray',label='HCP')\n",
    "    if(i==0):\n",
    "        plt.legend(fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "    plt.xticks(fontsize=32)\n",
    "    plt.yticks(fontsize=32)\n",
    "    if Save: plt.savefig(FigLoc+'EgMetricDKI_'+str(i)+'.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45214278-3a58-405f-a6ac-3743a7f03665",
   "metadata": {},
   "source": [
    "# Supplemental Fig 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd6427-9be2-43ce-997e-ac324752b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S6/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee080db6-0437-4e50-a1d7-4cc6660949ed",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3d8db-e493-4a26-91b9-cda52ec00823",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdwi = './HCP_data/Pat'+str(1)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(1)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(1)+'/bvecs_1k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "axial_middle = data.shape[2] // 2\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(19):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "\n",
    "bvalsHCP20 = bvalsHCP[selected_indices]\n",
    "bvecsHCP20 = bvecsHCP[selected_indices]\n",
    "gtabHCP20 = gradient_table(bvalsHCP20, bvecsHCP20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccffaf0-68ca-4ad4-bcca-646298249eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIHCPMid.pickle\"):\n",
    "    with open(f\"{network_path}/DTIHCPMid.pickle\", \"rb\") as handle:\n",
    "        posterior20 = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    bvals = gtabHCP.bvals\n",
    "    bvecs = gtabHCP.bvecs\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorS0.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        dt = ForceLowFA(dt)\n",
    "        Obs.append(CustomSimulator(dt,gtabHCP20,params[-1],np.random.rand()*30 + 20))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior20= inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIHCPMid.pickle\"):\n",
    "        with open(f\"{network_path}/DTIHCPMid.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior20, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892a341-8f16-456d-99fc-5ef32bf1ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArrShape = maskdata[:,:,axial_middle,0].shape\n",
    "NoiseEst = np.zeros([55,64,7])\n",
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        torch.manual_seed(10)\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            posterior_samples_1 = posterior20.sample((InferSamples,), x=maskdata[i,j,axial_middle,selected_indices],show_progress_bars=False)\n",
    "            NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea7a4f-917d-4769-ab65-b4ce221c550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(55):\n",
    "    for j in range(64):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1]])\n",
    "MD_SBI20 = np.zeros([55,64])\n",
    "FA_SBI20 = np.zeros([55,64])\n",
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "        MD_SBI20[i,j] = np.mean(Eigs)\n",
    "        FA_SBI20[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "FA_SBI20[np.isnan(FA_SBI20)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceb0db2-8684-4368-bc34-cd5c132a88d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtab2 = gtabHCP20\n",
    "tenmodel = dti.TensorModel(gtab2,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(maskdata[:,:,axial_middle,selected_indices])\n",
    "FA20 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD20 = dti.mean_diffusivity(tenfit.evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db4950-9e80-4d4e-bd95-ab6f3df23030",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(MD_SBI20.T,cmap='gray')\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_MD_20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3751b8-2e70-470c-9b09-2c6c8c666bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MD20.T,cmap='gray',vmin=vmin, vmax=vmax)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_MD_20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5b0ea3-cd73-4c15-b226-ac4da30da9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MD_SBI20.T-MD20.T\n",
    "data[~mask[:,:,axial_middle].T] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969bf42-24fd-4b83-a036-4565fff8dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MD_SBI20.T-MD20.T\n",
    "data[~mask[:,:,axial_middle].T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=np.nanmin(data), vcenter=0, vmax=np.nanmax(data))\n",
    "plt.imshow(data,cmap='seismic',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "ticks = [np.nanmin(data), 0, np.nanmax(data)]  # Adjust the number of ticks as needed\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'HCP_MD_Diff_20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30ef8a-510d-4040-b54b-e5a45b86a1d2",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49481fbb-6020-4155-92ff-ef5dd6925de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            FA20[i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f67417-30ef-4ae5-9e16-38f82474a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(FA_SBI20.T,cmap='gray',vmin=0, vmax=1)\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_FA_20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "plt.imshow(FA20.T,cmap='gray',vmin=0, vmax=1)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_FA_20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d5c22-ca1b-4a97-9116-96149e3924a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import TwoSlopeNorm\n",
    "data = FA_SBI20.T-FA20.T\n",
    "data[~mask[:,:,axial_middle].T] = np.nan\n",
    "norm = TwoSlopeNorm(vmin=np.nanmin(data),vcenter=0, vmax=np.nanmax(data))\n",
    "plt.imshow(data,cmap='seismic',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "ticks = [np.nanmin(data), 0, np.nanmax(data)]  # Adjust the number of ticks as needed\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'HCP_FA_Diff_20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565e84a-3ffb-4333-9c73-ed4ecff202ad",
   "metadata": {},
   "source": [
    "# Supplemental Fig 7\n",
    "This figure is created at the same time as Figure 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa34f3-e36a-4980-b8b9-a47b99065d3a",
   "metadata": {},
   "source": [
    "# Supplemental Fig 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52169c-f600-40a8-be70-20e63a46317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S8/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40953b36-523c-4d29-ab4a-2299740a7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial28 = HemiSphere(xyz=bvecs[1:29])\n",
    "hsph_initial20 = HemiSphere(xyz=bvecs[1:20])\n",
    "hsph_updated28,_ = disperse_charges(hsph_initial28,5000)\n",
    "hsph_updated20,_ = disperse_charges(hsph_initial20,5000)\n",
    "gtabSimSub = gradient_table(np.array([0]+[1000]*19+[3000]*28).squeeze(), np.vstack([[0,0,0],hsph_updated20.vertices,hsph_updated28.vertices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9def9-d7d4-472a-921f-8c52b5f57bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DKISimMid.pickle\"):\n",
    "    with open(f\"{network_path}/DKISimMid.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*60000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*20000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*60000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*60000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(2.5*60000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabSimSub.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabSimSub,200,np.random.rand()*30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>800).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKISimMid.pickle\"):\n",
    "        with open(f\"{network_path}/DKISimMid.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d9c2b4-85a8-4f17-a581-20d56900acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "j = 1\n",
    "vL = torch.tensor([0.2*j])\n",
    "vS = torch.tensor([0.01*j])  \n",
    "\n",
    "kk = np.random.randint(0,4)\n",
    "if(kk==0):\n",
    "    DT,KT = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],2,1)\n",
    "elif(kk==1):\n",
    "    DT,KT = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],2,1)\n",
    "elif(kk==2):\n",
    "    DT,KT = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],2,1)\n",
    "elif(kk==3):\n",
    "    DT,KT = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],2,1)\n",
    "\n",
    "tObs = CustomDKISimulator(np.squeeze(DT),np.squeeze(KT),gtabSimSub,200,20)\n",
    "tTrue = CustomDKISimulator(np.squeeze(DT),np.squeeze(KT),gtabSim,200,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea594d1a-e8b8-46d6-9b28-90c7dc7ac6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c578a3-d3fd-44bb-ac28-7284c8167d7e",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7423528-c4bf-44ef-8bfa-5837f2a9e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GuessDKI = posterior_samples_1.mean(axis=0)\n",
    "GuessSig = CustomDKISimulator(GuessDKI[:6],GuessDKI[6:],gtabSim,200)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k',label='True signal')\n",
    "plt.plot(GuessSig,lw=2,c=SBIFit,ls='--',label='SBI Recon.')\n",
    "plt.axis('off')\n",
    "legend = plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1.09,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "plt.fill_betweenx(np.arange(0,500,50),0*np.ones(10),20*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.fill_betweenx(np.arange(0,500,50),64*np.ones(10),92*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.ylim(-9.996985449425491, 209.99985644997255)\n",
    "for handle in legend.get_lines():\n",
    "    handle.set_linewidth(6)  # Set desired linewidth\n",
    "plt.savefig(FigLoc+'20ReconSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01d2b1-eabd-41a0-8faa-8919e8b1c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "tenfit = dkimodel.fit(tObs)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k')\n",
    "plt.plot(tenfit.predict(gtabSim,200),lw=2,c=WLSFit,ls='--',label='NLLS Recon.')\n",
    "plt.axis('off')\n",
    "legend = plt.legend(ncols=2,loc=1,bbox_to_anchor =  (0.9,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "plt.fill_betweenx(np.arange(0,500,50),0*np.ones(10),20*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.fill_betweenx(np.arange(0,500,50),64*np.ones(10),92*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.ylim(-9.996985449425491, 209.99985644997255)\n",
    "for handle in legend.get_lines():\n",
    "    handle.set_linewidth(6)  # Set desired linewidth\n",
    "plt.savefig(FigLoc+'20ReconWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f93c9c-b4d2-40cd-8102-b9e2bd0541b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "Mets = []\n",
    "MetsSBI = []\n",
    "for i in tqdm.tqdm([20,10,5,2]):\n",
    "    m = []\n",
    "    m2 = []\n",
    "    for k in range(50):\n",
    "        tObs = CustomDKISimulator(np.squeeze(DT), np.squeeze(KT),gtabSimSub, S0=200, snr=i)#\n",
    "        dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        m.append(DKIMetrics(tenfit.lower_triangular(),tenfit.kt,False))\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessDKI = posterior_samples_1.mean(axis=0)\n",
    "        m2.append(DKIMetrics(GuessDKI[:6],GuessDKI[6:],False))\n",
    "    Mets.append(m)\n",
    "    MetsSBI.append(m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1294fe-2ec8-49ea-a0d5-3d4351ee9aaa",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33ae9f-a4bd-4121-82ec-2c6822545f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mets = np.array(Mets)\n",
    "MetsSBI = np.array(MetsSBI)\n",
    "fig,ax = plt.subplots(1,5,figsize=(22.5,3))\n",
    "for i in range(5):\n",
    "    plt.sca(ax[i])\n",
    "    viol_plot(Mets[:,:,i].T,WLSFit,)\n",
    "    viol_plot(MetsSBI[:,:,i].T,SBIFit,widths=0.3,positions=[1.3,2.3,3.3,4.3])\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.axhline(DKIMetrics(np.squeeze(DT),np.squeeze(KT),False)[i],lw=3,ls='--',c='k')\n",
    "    plt.yticks(fontsize=32)\n",
    "    if(i==2):\n",
    "        plt.yticks([0,5])\n",
    "    if(i==4):\n",
    "        plt.yticks([0,1])\n",
    "plt.savefig(FigLoc+'EgSigMetrics20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496d2ad-395d-4948-b9ab-1bf8be4e8356",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e91dc49-1daf-4e96-bbf3-22ab6c226425",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],1,40)\n",
    "DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],1,40)\n",
    "DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],1,40)\n",
    "DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],1,40)\n",
    "DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,40)\n",
    "\n",
    "SampsDT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "SampsKT = np.vstack([KT1,KT2,KT3,KT4,KT5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8df768-566a-4dc7-be32-e0dbf9202d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "Samples20  = []\n",
    "\n",
    "for Sd,Sk in zip(SampsDT,SampsKT):\n",
    "    Samples20.append([CustomDKISimulator(Sd,Sk,gtabSimSub, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "\n",
    "Samples20 = np.array(Samples20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720048c-bd32-42b5-8a25-229928a85092",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "ErrorFull = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples20[i,k,:]\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessSBI = posterior_samples_1.mean(axis=0)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(GuessSBI[:6],GuessSBI[6:],SampsDT[i],SampsKT[i]))\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "Error_s = []\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples20[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(tenfit.lower_triangular(),tenfit.kt,SampsDT[i],SampsKT[i]))\n",
    "    Error_s.append(ErrorN2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f6fbef-7d99-4e79-8a38-20401710df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorFull = np.array(ErrorFull)\n",
    "Error_s = np.array(Error_s)\n",
    "ErrorNames = ['MK Error', 'AK Error', 'RK Error', 'MKT Error', 'KFA Error']\n",
    "fig,ax = plt.subplots(1,5,figsize=(22.5,3))\n",
    "for i in range(5):\n",
    "    plt.sca(ax[i])\n",
    "    box_plot(Error_s[1:,:,i],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    box_plot(ErrorFull[1:,:,i],SBIFit-0.2, np.clip(SBIFit+0.2,0,1),showfliers=False,widths=0.3,positions=[1.3,2.3,3.3,4.3])\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    plt.yticks(fontsize=32)\n",
    "plt.savefig(FigLoc+'Errors20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f33129-d4d5-421d-a278-621ee3313ded",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d382e-e8f3-44c4-8d25-4deb5dfeb5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "\n",
    "temp_bvecs = bvecsHCP[bvalsHCP>0]\n",
    "temp_bvals = bvalsHCP[bvalsHCP>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(18):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices7 = selected_indices\n",
    "\n",
    "bvalsHCP7_1 = np.insert(temp_bvals[selected_indices7],0,0)\n",
    "bvecsHCP7_1 = np.insert(temp_bvecs[selected_indices7],0,[0,0,0],axis=0)\n",
    "\n",
    "i=3\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(27):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP20 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx_one = []\n",
    "for b in bvecsHCP7_1:\n",
    "    true_indx_one.append(np.linalg.norm(b-bvecsHCP,axis=1).argmin())\n",
    "true_indx20 = []        \n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx20.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "true_indx20 = true_indx_one+[t+69 for t in true_indx20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459cbae-182c-4812-a658-13865f8fb228",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIHCPMid.pickle\"):\n",
    "    with open(f\"{network_path}/DKIHCPMid.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*60000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*20000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*60000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*60000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(2.5*60000))\n",
    "        \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([650000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabHCP20.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabHCP20,S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKIHCPMid.pickle\"):\n",
    "        with open(f\"{network_path}/DKIHCPMid.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af7d639-5c8f-475f-8cd1-0be159b61650",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "_, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "\n",
    "\n",
    "data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "# Get the indices of True values\n",
    "true_indices = np.argwhere(mask)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = true_indices.min(axis=0)\n",
    "max_coords = true_indices.max(axis=0)\n",
    "\n",
    "maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "axial_middle = maskdata.shape[2] // 2\n",
    "maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0507825a-2fc7-45d4-b6a1-3d3a041c0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "NoiseEst = np.zeros([62, 68 ,22])\n",
    "torch.manual_seed(10)\n",
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            posterior_samples_1 = posteriorFull.sample((InferSamples,), x=TestData4D[i,j,axial_middle,true_indx20],show_progress_bars=False)\n",
    "            NoiseEst[i,j] = posterior_samples_1.mean(axis=0)\n",
    "os.system('say \"Finished sampling.\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a2b65-a2b2-4b90-94c7-bc121734148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb70632-ab92-4871-8f5b-264535210a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MK_SBIFull  = np.zeros([62, 68])\n",
    "AK_SBIFull  = np.zeros([62, 68])\n",
    "RK_SBIFull  = np.zeros([62, 68])\n",
    "MKT_SBIFull = np.zeros([62, 68])\n",
    "KFA_SBIFull = np.zeros([62, 68])\n",
    "for i in tqdm.tqdm(range(62)):\n",
    "    for j in range(68):\n",
    "        if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK_SBIFull[i,j] = Metrics[0]\n",
    "            AK_SBIFull[i,j] = Metrics[1]\n",
    "            RK_SBIFull[i,j] = Metrics[2]\n",
    "            MKT_SBIFull[i,j] = Metrics[3]\n",
    "            KFA_SBIFull[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471bc6b8-d661-479c-a505-c97518051288",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodelNL = dki.DiffusionKurtosisModel(gtabHCP20,fit_method='NLLS')\n",
    "dkifitNL = dkimodelNL.fit(TestData[:,:,true_indx20])\n",
    "MK_NLFull  = np.zeros([62, 68])\n",
    "AK_NLFull  = np.zeros([62, 68])\n",
    "RK_NLFull  = np.zeros([62, 68])\n",
    "MKT_NLFull = np.zeros([62, 68])\n",
    "KFA_NLFull = np.zeros([62, 68])\n",
    "for i in range(62):\n",
    "    for j in range(68):\n",
    "        if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            Metrics = DKIMetrics(dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt)\n",
    "            MK_NLFull[i,j] = Metrics[0]\n",
    "            AK_NLFull[i,j] = Metrics[1]\n",
    "            RK_NLFull[i,j] = Metrics[2]\n",
    "            MKT_NLFull[i,j] = Metrics[3]\n",
    "            KFA_NLFull[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3adcd9-19f1-451d-8168-cfe65c04a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "KFA_SBIFull[np.isnan(KFA_SBIFull)] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c72c8e-2405-49e9-8757-277e344ef67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKSBI20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'AKSBI20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'RKSBI20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MKT_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKTSBI20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((KFA_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'KFASBI20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575af5de-88e9-41e5-b0aa-59d38c37f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKNL20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'AKNL20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'RKNL20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MKT_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKTNL20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((KFA_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'KFANL20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6409ebca-90dd-454d-b0c4-8c5276953161",
   "metadata": {},
   "source": [
    "# Supplemental Fig 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd4c6e-bd37-4e92-8f5e-699c5ce8c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S9/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08653673-7d46-43fc-a59e-cab8758204a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = pmt.read_mat(MSDir+'NMSS_15/data_loaded.mat')\n",
    "affine = np.ones((4,4))\n",
    "\n",
    "data, affine = reslice(F['data'], affine, (2,2,2), (2.5,2.5,2.5))\n",
    "_, maskCut = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=False, dilate=3)\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=True, dilate=3)\n",
    "axial_middle = maskdata.shape[2] // 2\n",
    "\n",
    "fdwi = MSDir+'NMSS_15_lesions.nii'\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "lesion, affine = reslice(data, affine, (2,2,2), (2.5,2.5,2.5))\n",
    "\n",
    "fdwi = MSDir+'b0_NMSS_15.nii'\n",
    "\n",
    "b0, affine, img = load_nifti(fdwi, return_img=True)\n",
    "b0, affine = reslice(b0, affine, (2,2,2), (2.5,2.5,2.5))\n",
    "\n",
    "b0_alt = np.swapaxes(b0[::-1,::-1],0,1)\n",
    "lesion_alt = np.swapaxes(lesion[::-1,::-1],0,1)\n",
    "bbox = np.argwhere(maskCut)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = bbox.min(axis=0)\n",
    "max_coords = bbox.max(axis=0)\n",
    "\n",
    "lesion_bin  = lesion_alt[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "b0_alt  = b0_alt[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "bvecs = (F['direction'].T/np.linalg.norm(F['direction'],axis=1)).T\n",
    "bvecs[np.isnan(bvecs)] = 0\n",
    "bvals = F['bval']\n",
    "bvecs2000 = bvecs[bvals==2000]\n",
    "\n",
    "bvals2000 = np.array([0] + list(bvals[bvals==2000]))\n",
    "bvecs2000 = np.vstack([[0,0,0],bvecs[bvals==2000]])\n",
    "data2000 = maskdata[:,:,40,np.hstack([0,np.where(bvals==2000)[0]])]\n",
    "gtab2000 = gradient_table(bvals2000,bvecs2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6709f1b7-a842-4f9b-b8a7-62317403138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIMSFull.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMSFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorS0.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        dt = ForceLowFA(dt)\n",
    "        Obs.append(CustomSimulator(dt,gtab2000,params[-1],np.random.rand()*30 + 20))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMSFull.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMSFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0f79b4-ce3d-42a4-8482-cb88d6fe6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArrShape = data2000.shape[:2]\n",
    "NoiseEst = np.zeros(list(data2000.shape[:2])+[7])\n",
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        torch.manual_seed(10)\n",
    "        if(np.sum(data2000[i,j]) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            np.random.seed(1)\n",
    "            torch.manual_seed(1)\n",
    "            posterior_samples_1 = posteriorFull.sample((InferSamples,), x=data2000[i,j,:],show_progress_bars=False)\n",
    "            NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(ArrShape[0]):\n",
    "    for j in range(ArrShape[1]):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1:]])\n",
    "MD_SBIFull = np.zeros(ArrShape[:2])\n",
    "FA_SBIFull = np.zeros(ArrShape)\n",
    "for i in range(ArrShape[0]):\n",
    "    for j in range(ArrShape[1]): \n",
    "        Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "        MD_SBIFull[i,j] = np.mean(Eigs)\n",
    "        FA_SBIFull[i,j] = FracAni(Eigs,np.mean(Eigs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b5376-ed4f-45a4-b648-e268023f36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenmodel = dti.TensorModel(gtab2000,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(data2000[:,:,:])\n",
    "FAFull = dti.fractional_anisotropy(tenfit.evals)\n",
    "MDFull = dti.mean_diffusivity(tenfit.evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e88a9e-1619-42bd-b2ad-4cc2dfa941df",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1637c-2df3-4b22-b4c1-82301b224a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MD_SBIFull,cmap='gray')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'MDFullSBI.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "\n",
    "plt.imshow(MDFull,cmap='gray')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'MDFullNL.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e83df-cebb-499e-8a20-f5c9bcc15f06",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b90b1-4231-4159-877f-4170db3f7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(FAFull,cmap='gray')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'FAFullNL.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "\n",
    "plt.imshow(FA_SBIFull,cmap='gray')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'FAFullSBI.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de4fdc-7786-4715-9bf0-91927309bf4d",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104f392-add2-46e8-a0bc-410575051228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists(f\"{network_path}/DKIMultiMSFull.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiMSFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,200000)\n",
    "    #DT2,KT2 = GenDTKT([DT1_hak,DT2_hak],[x4_lak,R1_lak,x2_lak,R2_lak],12,10000)\n",
    "    \n",
    "    DT = np.vstack([DT1])\n",
    "    KT = np.vstack([KT1])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([200000])\n",
    "    A  = np.random.choice(5,len(S0))\n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabsF[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabsF[A[i]],S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        DT[indxNew],KT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],17+kk,1)\n",
    "        \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    A = np.array(A).reshape(len(A),1)\n",
    "    Par = np.hstack([DT,KT,S0])#,A])\n",
    "    Obs = np.hstack([1000*A,Obs])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiMSFull.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiMSFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf1935-1fad-45bc-a252-8cabf491f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MKFullArr = []\n",
    "RKFullArr = []\n",
    "AKFullArr = []\n",
    "MKTFullArr = []\n",
    "KFAFullArr = []\n",
    "for kk in range(5):\n",
    "    Dat = Dats[kk]\n",
    "    ArrShape = Dat.shape[:2]\n",
    "    NoiseEst = np.zeros(list(ArrShape[:2])+[22])\n",
    "    VarEst   = np.zeros(ArrShape)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            torch.manual_seed(10)\n",
    "            if(np.sum(Dat[i,j,LesionSlices[kk]]) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                np.random.seed(1)\n",
    "                torch.manual_seed(1)\n",
    "                posterior_samples_1 = posteriorFull.sample((InferSamples,), x=np.hstack([kk*1000,Dat[i,j,LesionSlices[kk]]]),show_progress_bars=False)\n",
    "                NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "    NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "    for i in range(ArrShape[0]):\n",
    "        for j in range(ArrShape[1]):  \n",
    "            NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])\n",
    "    \n",
    "    MK_SBIFull  = np.zeros(ArrShape)\n",
    "    AK_SBIFull  = np.zeros(ArrShape)\n",
    "    RK_SBIFull  = np.zeros(ArrShape)\n",
    "    MKT_SBIFull = np.zeros(ArrShape)\n",
    "    KFA_SBIFull = np.zeros(ArrShape)\n",
    "    for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "        for j in range(ArrShape[1]):\n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK_SBIFull[i,j] = Metrics[0]\n",
    "            AK_SBIFull[i,j] = Metrics[1]\n",
    "            RK_SBIFull[i,j] = Metrics[2]\n",
    "            MKT_SBIFull[i,j] = Metrics[3]\n",
    "            KFA_SBIFull[i,j] = Metrics[4]\n",
    "    MKFullArr.append(MK_SBIFull)\n",
    "    RKFullArr.append(RK_SBIFull)\n",
    "    AKFullArr.append(AK_SBIFull)\n",
    "    MKTFullArr.append(MKT_SBIFull)\n",
    "    KFAFullArr.append(KFA_SBIFull)\n",
    "    fig,ax = plt.subplots(1,5,figsize=(24,6.4))\n",
    "    plt.sca(ax[0])\n",
    "    plt.imshow(MK_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[1])\n",
    "    plt.imshow(AK_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[2])\n",
    "    plt.imshow(RK_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[3])\n",
    "    plt.imshow(MKT_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.sca(ax[4])\n",
    "    plt.imshow(KFA_SBIFull.T,vmin=0,vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950888e9-80bc-4391-95db-1f848f25977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MKFullArr[1],cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'MKFullSBI.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "\n",
    "plt.imshow(MKFullNLArr[1],cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'MKFullNL.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba138f-46fd-439f-96b4-095e6c053947",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d324229-f867-4908-8417-f0a59aeafe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(AKFullArr[1],cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'AKFullSBI.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "\n",
    "plt.imshow(AKFullNLArr[1],cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'AKFullNL.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb5f4a-d9c9-47c3-be3a-5eb80b084be7",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0d130-f4b1-450e-b798-21c0ed817f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(RKFullArr[1],cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'RKFullSBI.pdf',format='PDF',transparent=True,bbox_inches='tight')\n",
    "\n",
    "plt.imshow(RKFullNLArr[1],cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'RKFullNL.pdf',format='PDF',transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9df7d-ee3c-4378-aeb9-e0966f90828f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
