{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f961d685-737e-464b-bb40-baf5ae2b7a38",
   "metadata": {},
   "source": [
    "# Front matter and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb91541-4dc6-47fb-b4c4-6828168b5553",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e3d45-362e-4672-8e43-9875ca81d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dill as pickle\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import lognorm,norm\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 14}\n",
    "plt.rc('font', **font)\n",
    "plt.rc('ytick', labelsize=24)\n",
    "plt.rc('xtick', labelsize=24)\n",
    "plt.rc('text.latex', preamble=r'\\usepackage{color}')\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "\n",
    "from sbi import analysis as analysis\n",
    "from sbi import utils as utils\n",
    "from sbi.inference import SNPE, simulate_for_sbi\n",
    "from sbi.inference.potentials.posterior_based_potential import posterior_estimator_based_potential\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")\n",
    "from sbi.utils import process_prior,BoxUniform\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Categorical,Normal\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "from dipy.sims.voxel import single_tensor\n",
    "from dipy.data import get_fnames\n",
    "from dipy.io.gradients import read_bvals_bvecs\n",
    "from dipy.core.gradients import gradient_table\n",
    "from dipy.reconst.dti import (decompose_tensor, from_lower_triangular)\n",
    "from dipy.io.image import load_nifti\n",
    "from dipy.segment.mask import median_otsu\n",
    "import dipy.reconst.dti as dti\n",
    "import dipy.reconst.dki as dki\n",
    "from dipy.align.reslice import reslice\n",
    "from dipy.core.sphere import disperse_charges, Sphere, HemiSphere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d0e97f-d8b3-49e1-b1dd-7a0e1e844384",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717fb08-d502-465a-a830-70812a9846f7",
   "metadata": {},
   "source": [
    "### DTI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b35f2-0c20-45ec-b58d-0c25f3410e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vals_to_mat(dt):\n",
    "    DTI = np.zeros((3,3))\n",
    "    DTI[0,0] = dt[0]\n",
    "    DTI[0,1],DTI[1,0] =  dt[1],dt[1]\n",
    "    DTI[1,1] =  dt[2]\n",
    "    DTI[0,2],DTI[2,0] =  dt[3],dt[3]\n",
    "    DTI[1,2],DTI[2,1] =  dt[4],dt[4]\n",
    "    DTI[2,2] =  dt[5]\n",
    "    return DTI\n",
    "\n",
    "def mat_to_vals(DTI):\n",
    "    dt = np.zeros(6)\n",
    "    dt[0] = DTI[0,0]\n",
    "    dt[1] = DTI[0,1]\n",
    "    dt[2] = DTI[1,1]\n",
    "    dt[3] = DTI[0,2]\n",
    "    dt[4] = DTI[1,2]\n",
    "    dt[5] = DTI[2,2]\n",
    "    return dt\n",
    "\n",
    "def fill_lower_diag(a):\n",
    "    b = [a[0],a[3],a[1],a[4],a[5],a[2]]\n",
    "    n = 3\n",
    "    mask = np.tri(n,dtype=bool) \n",
    "    out = np.zeros((n,n),dtype=float)\n",
    "    out[mask] = b\n",
    "    return out\n",
    "\n",
    "def ComputeDTI(params):\n",
    "    L = fill_lower_diag(params)\n",
    "    \n",
    "    np.fill_diagonal(L, np.abs(np.diagonal(L)))\n",
    "\n",
    "    A = L @ L.T\n",
    "    return A\n",
    "\n",
    "def ForceLowFA(dt):\n",
    "    # Modify the matrix to ensure low FA (more isotropic)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(dt)\n",
    "    \n",
    "    # Make the eigenvalues more similar to enforce low FA\n",
    "    mean_eigenvalue = np.mean(eigenvalues)\n",
    "\n",
    "    adjusted_eigenvalues = np.clip(eigenvalues, mean_eigenvalue * np.random.rand(), mean_eigenvalue * 1.0)\n",
    "    \n",
    "    # Reconstruct the matrix with the adjusted eigenvalues\n",
    "    dt_low_fa = eigenvectors @ np.diag(adjusted_eigenvalues) @ eigenvectors.T\n",
    "    \n",
    "    return dt_low_fa\n",
    "    \n",
    "def FracAni(evals,MD):\n",
    "    numerator = np.sqrt(3 * np.sum((evals - MD) ** 2))\n",
    "    denominator = np.sqrt(2) * np.sqrt(np.sum(evals ** 2))\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "def clip_negative_eigenvalues(matrix):\n",
    "    # Perform eigenvalue decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "    \n",
    "    # Clip negative eigenvalues to 0\n",
    "    clipped_eigenvalues = np.maximum(eigenvalues, 1e-5)\n",
    "    \n",
    "    # Reconstruct the matrix with the clipped eigenvalues\n",
    "    clipped_matrix = eigenvectors @ np.diag(clipped_eigenvalues) @ np.linalg.inv(eigenvectors)\n",
    "    \n",
    "    return clipped_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea42bca-723a-4a92-821e-2a1b5c2beed4",
   "metadata": {},
   "source": [
    "### DKI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09482812-1b86-42b0-b203-ff4b5911ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FitDT(Dat,seed=1):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    # DT_abc\n",
    "    data = Dat[:,0]\n",
    "    shape,loc,scale = lognorm.fit(data)\n",
    "    \n",
    "    dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "\n",
    "    #DT_rest\n",
    "    data = Dat[:,1]\n",
    "    loc,scale = norm.fit(data)\n",
    "    \n",
    "    # Compute the fitted PDF\n",
    "    dti2_fitted = stats.norm(loc=loc, scale=scale)\n",
    "\n",
    "    return dti1_fitted,dti2_fitted\n",
    "\n",
    "def FitKT(Dat,seed=1):\n",
    "    np.random.seed(seed)    \n",
    "    # Fitting x4\n",
    "    data = Dat[:,0]\n",
    "    shape,loc,scale = lognorm.fit(data)\n",
    "    x4_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "    \n",
    "    # Fitting R1\n",
    "    data = Dat[:,3]\n",
    "    loc,scale = norm.fit(data)\n",
    "    R1_fitted = norm(loc,scale)\n",
    "    \n",
    "    # Fitting x2\n",
    "    data = Dat[:,9]\n",
    "    shape,loc,scale = lognorm.fit(data)\n",
    "    x2_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "\n",
    "    # Fitting R2\n",
    "    data = Dat[:,12]\n",
    "    loc,scale = norm.fit(data)\n",
    "    R2_fitted = norm(loc,scale)\n",
    "\n",
    "\n",
    "    return x4_fitted,R1_fitted,x2_fitted,R2_fitted\n",
    "\n",
    "def GenDTKT(DT_Fits,KT_Fits,seed,size):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    DT = np.zeros([size,6])\n",
    "    KT = np.zeros([size,15])\n",
    "\n",
    "    DT[:,0] = DT_Fits[0].rvs(size)\n",
    "    DT[:,2] = DT_Fits[0].rvs(size)\n",
    "    DT[:,5] = DT_Fits[0].rvs(size)\n",
    "\n",
    "    DT[:,1] = DT_Fits[1].rvs(size)\n",
    "    DT[:,3] = DT_Fits[1].rvs(size)\n",
    "    DT[:,4] = DT_Fits[1].rvs(size)\n",
    "\n",
    "    for k in range(3):\n",
    "        KT[:,k] = KT_Fits[0].rvs(size)\n",
    "    for k in range(3,9):\n",
    "        KT[:,k] = KT_Fits[1].rvs(size)\n",
    "    for k in range(9,12):\n",
    "        KT[:,k] = KT_Fits[2].rvs(size)\n",
    "    for k in range(12,15):\n",
    "        KT[:,k] = KT_Fits[3].rvs(size)\n",
    "\n",
    "    return DT,KT\n",
    "    \n",
    "def DKIMetrics(dt,kt,analytical=True):\n",
    "    if(dt.ndim == 1):\n",
    "        dt = vals_to_mat(dt)\n",
    "    evals,evecs = np.linalg.eigh(dt)\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evals = evals[idx]\n",
    "    evecs = evecs[:, idx]\n",
    "    \n",
    "    params = np.concatenate([evals,np.hstack(evecs),kt])\n",
    "    params2 = np.concatenate([evals,np.hstack(evecs),-kt])\n",
    "\n",
    "    mk = dki.mean_kurtosis(params,analytical=analytical,min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    ak = dki.axial_kurtosis(params,analytical=analytical,min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    rk = dki.radial_kurtosis(params,analytical=analytical,min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    mkt = dki.mean_kurtosis_tensor(params, min_kurtosis=-3.0 / 7, max_kurtosis=np.inf)\n",
    "\n",
    "    kfa = kurtosis_fractional_anisotropy_test(params)\n",
    "\n",
    "    return mk,ak,rk,mkt,kfa\n",
    "    \n",
    "def kurtosis_fractional_anisotropy_test(dki_params):\n",
    "    r\"\"\"Compute the anisotropy of the kurtosis tensor (KFA).\n",
    "\n",
    "    See :footcite:p:`Glenn2015` and :footcite:p:`NetoHenriques2021` for further\n",
    "    details about the method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dki_params : ndarray (x, y, z, 27) or (n, 27)\n",
    "        All parameters estimated from the diffusion kurtosis model.\n",
    "        Parameters are ordered as follows:\n",
    "            1) Three diffusion tensor's eigenvalues\n",
    "            2) Three lines of the eigenvector matrix each containing the first,\n",
    "                second and third coordinates of the eigenvector\n",
    "            3) Fifteen elements of the kurtosis tensor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kfa : array\n",
    "        Calculated mean kurtosis tensor.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The KFA is defined as :footcite:p:`Glenn2015`:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "         KFA \\equiv\n",
    "         \\frac{||\\mathbf{W} - MKT \\mathbf{I}^{(4)}||_F}{||\\mathbf{W}||_F}\n",
    "\n",
    "    where $W$ is the kurtosis tensor, MKT the kurtosis tensor mean, $I^{(4)}$ is\n",
    "    the fully symmetric rank 2 isotropic tensor and $||...||_F$ is the tensor's\n",
    "    Frobenius norm :footcite:p:`Glenn2015`.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. footbibliography::\n",
    "\n",
    "    \"\"\"\n",
    "    Wxxxx = dki_params[..., 12]\n",
    "    Wyyyy = dki_params[..., 13]\n",
    "    Wzzzz = dki_params[..., 14]\n",
    "    Wxxxy = dki_params[..., 15]\n",
    "    Wxxxz = dki_params[..., 16]\n",
    "    Wxyyy = dki_params[..., 17]\n",
    "    Wyyyz = dki_params[..., 18]\n",
    "    Wxzzz = dki_params[..., 19]\n",
    "    Wyzzz = dki_params[..., 20]\n",
    "    Wxxyy = dki_params[..., 21]\n",
    "    Wxxzz = dki_params[..., 22]\n",
    "    Wyyzz = dki_params[..., 23]\n",
    "    Wxxyz = dki_params[..., 24]\n",
    "    Wxyyz = dki_params[..., 25]\n",
    "    Wxyzz = dki_params[..., 26]\n",
    "\n",
    "\n",
    "    W = 1.0 / 5.0 * (Wxxxx + Wyyyy + Wzzzz + 2 * Wxxyy + 2 * Wxxzz + 2 * Wyyzz)\n",
    "    # Compute's equation numerator\n",
    "    A = (\n",
    "        (Wxxxx - W) ** 2\n",
    "        + (Wyyyy - W) ** 2\n",
    "        + (Wzzzz - W) ** 2\n",
    "        + 4 * (Wxxxy**2 + Wxxxz**2 + Wxyyy**2 + Wyyyz**2 + Wxzzz**2 + Wyzzz**2)\n",
    "        + 6 * ((Wxxyy - W / 3) ** 2 + (Wxxzz - W / 3) ** 2 + (Wyyzz - W / 3) ** 2)\n",
    "        + 12 * (Wxxyz**2 + Wxyyz**2 + Wxyzz**2)\n",
    "    )\n",
    "    # Compute's equation denominator\n",
    "    B = (\n",
    "        Wxxxx**2\n",
    "        + Wyyyy**2\n",
    "        + Wzzzz**2\n",
    "        + 4 * (Wxxxy**2 + Wxxxz**2 + Wxyyy**2 + Wyyyz**2 + Wxzzz**2 + Wyzzz**2)\n",
    "        + 6 * (Wxxyy**2 + Wxxzz**2 + Wyyzz**2)\n",
    "        + 12 * (Wxxyz**2 + Wxyyz**2 + Wxyzz**2)\n",
    "    )\n",
    "\n",
    "    # Compute KFA\n",
    "    KFA = np.zeros(A.shape)\n",
    "    KFA = np.sqrt(A / B)\n",
    "\n",
    "    return KFA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd64dcc-b517-435f-ba95-d9e6a4496319",
   "metadata": {},
   "source": [
    "### Simulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e445080b-782a-4aad-a29b-f1a19ba283fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomSimulator(Mat,gtab,S0,snr=None):\n",
    "    evals,evecs = np.linalg.eigh(Mat)\n",
    "    signal = single_tensor(gtab, S0=S0, evals=evals, evecs=evecs)\n",
    "    if(snr is None):\n",
    "        return signal\n",
    "    else:\n",
    "        return AddNoise(signal,S0,snr)\n",
    "\n",
    "def Simulator(bvals,bvecs,S0,params,SNR):\n",
    "\n",
    "    dt = ComputeDTI(params)\n",
    "    signal_dti = CustomSimulator(dt,gradient_table(bvals, bvecs),S0,SNR)\n",
    "    \n",
    "    return signal_dti\n",
    "\n",
    "\n",
    "def GenRicciNoise(signal,S0,snr):\n",
    "\n",
    "    size = signal.shape\n",
    "    sigma = S0 / snr\n",
    "    noise1 = np.random.normal(0, sigma, size=size)\n",
    "    noise2 = np.random.normal(0, sigma, size=size)\n",
    "\n",
    "    return np.sqrt((signal+noise1) ** 2 + noise2 ** 2)\n",
    "\n",
    "\n",
    "def AddNoise(signal,S0,snr):\n",
    "    \n",
    "    return GenRicciNoise(signal,S0,snr)\n",
    "\n",
    "def CustomDKISimulator(dt,kt,gtab,S0,snr=None):\n",
    "    if(dt.ndim == 1):\n",
    "        dt = vals_to_mat(dt)\n",
    "    evals,evecs = np.linalg.eigh(dt)\n",
    "    params = np.concatenate([evals,np.hstack(evecs),kt])\n",
    "    signal = dki.dki_prediction(params,gtab,S0)\n",
    "    if(snr is None):\n",
    "        return signal\n",
    "    else:\n",
    "        return AddNoise(signal,S0,snr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287aaa10-3d36-4b90-9266-02914c612c3a",
   "metadata": {},
   "source": [
    "### SBI Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79408e28-c4fa-45d3-b60e-5c3102c1e2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIPrior:\n",
    "    def __init__(self, lower_abs : Tensor, upper_abs : Tensor, \n",
    "                       lower_rest: Tensor, upper_rest: Tensor,\n",
    "                        return_numpy: bool = False):\n",
    "\n",
    "        self.dist_abs = BoxUniform(low= lower_abs* torch.ones(3), high=upper_abs * torch.ones(3))\n",
    "        self.dist_rest = BoxUniform(low=lower_rest * torch.ones(3), high=upper_rest *torch.ones(3))\n",
    "        self.return_numpy = return_numpy\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        \n",
    "        abc  = self.dist_abs.sample(sample_shape)\n",
    "        rest = self.dist_rest.sample(sample_shape)\n",
    "        \n",
    "        if self.return_numpy:   \n",
    "            params = np.hstack([abc,rest]) \n",
    "        else:\n",
    "            params = torch.hstack([abc,rest])\n",
    "\n",
    "        return params\n",
    "        \n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        \n",
    "        abc  = values[:,:3]\n",
    "        rest = values[:,3:]\n",
    "\n",
    "        log_prob_abc  = self.dist_abs.log_prob(abc)\n",
    "        log_prob_rest = self.dist_rest.log_prob(rest)\n",
    "        return log_prob_abc+log_prob_rest\n",
    "\n",
    "class DTIPriorS0:\n",
    "    def __init__(self, lower_abs : Tensor, upper_abs : Tensor, \n",
    "                       lower_rest: Tensor, upper_rest: Tensor,\n",
    "                       lower_S0: Tensor, upper_S0: Tensor,\n",
    "                        return_numpy: bool = False):\n",
    "\n",
    "        self.dist_abs = BoxUniform(low= lower_abs* torch.ones(3), high=upper_abs * torch.ones(3))\n",
    "        self.dist_rest = BoxUniform(low=lower_rest * torch.ones(3), high=upper_rest *torch.ones(3))\n",
    "        self.dist_S0 = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "        self.return_numpy = return_numpy\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        \n",
    "        abc  = self.dist_abs.sample(sample_shape)\n",
    "        rest = self.dist_rest.sample(sample_shape)\n",
    "        S0   = self.dist_S0.sample(sample_shape)\n",
    "        \n",
    "        if self.return_numpy:   \n",
    "            params = np.hstack([abc,rest,S0]) \n",
    "        else:\n",
    "            params = torch.hstack([abc,rest,S0])\n",
    "\n",
    "        return params\n",
    "        \n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        \n",
    "        abc  = values[:,:3]\n",
    "        rest = values[:,3:-1]\n",
    "        S0   = values[:,-1]\n",
    "\n",
    "        log_prob_abc  = self.dist_abs.log_prob(abc)\n",
    "        log_prob_rest = self.dist_rest.log_prob(rest)\n",
    "        log_prob_S0 = self.dist_S0.log_prob(S0)\n",
    "        return log_prob_abc+log_prob_rest+log_prob_S0\n",
    "\n",
    "class DTIPriorS0Direc:\n",
    "    def __init__(self, lower_abs : Tensor, upper_abs : Tensor, \n",
    "                       lower_rest: Tensor, upper_rest: Tensor,\n",
    "                       lower_S0: Tensor, upper_S0: Tensor,\n",
    "                        return_numpy: bool = False):\n",
    "\n",
    "        self.dist_abs = BoxUniform(low= lower_abs* torch.ones(3), high=upper_abs * torch.ones(3))\n",
    "        self.dist_rest = BoxUniform(low=lower_rest * torch.ones(3), high=upper_rest *torch.ones(3))\n",
    "        self.dist_S0 = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "        self.direction_choice = Categorical(probs=torch.ones(1, 5))\n",
    "        self.return_numpy = return_numpy\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        \n",
    "        abc  = self.dist_abs.sample(sample_shape)\n",
    "        rest = self.dist_rest.sample(sample_shape)\n",
    "        S0   = self.dist_S0.sample(sample_shape)\n",
    "        direc = self.direction_choice.sample(sample_shape)       \n",
    "        \n",
    "        if self.return_numpy:   \n",
    "            params = np.hstack([abc,rest,S0,direc]) \n",
    "        else:\n",
    "            params = torch.hstack([abc,rest,S0,direc])\n",
    "\n",
    "        return params\n",
    "        \n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        \n",
    "        abc  = values[:,:3]\n",
    "        rest = values[:,3:-2]\n",
    "        S0   = values[:,-2]\n",
    "        direc   = values[:,-1]\n",
    "\n",
    "        log_prob_abc   = self.dist_abs.log_prob(abc)\n",
    "        log_prob_rest  = self.dist_rest.log_prob(rest)\n",
    "        log_prob_S0    = self.dist_S0.log_prob(S0)\n",
    "        log_prob_direc =  self.direction_choice.log_prob(direc)\n",
    "        return log_prob_abc+log_prob_rest+log_prob_S0+log_prob_direc\n",
    "\n",
    "class DTIPriorS0Noise:\n",
    "    def __init__(self, lower_abs : Tensor, upper_abs : Tensor, \n",
    "                       lower_rest: Tensor, upper_rest: Tensor,\n",
    "                       lower_S0: Tensor, upper_S0: Tensor,\n",
    "                       lower_noise: Tensor, upper_noise: Tensor,\n",
    "                        return_numpy: bool = False):\n",
    "\n",
    "        self.dist_abs = BoxUniform(low= lower_abs* torch.ones(3), high=upper_abs * torch.ones(3))\n",
    "        self.dist_rest = BoxUniform(low=lower_rest * torch.ones(3), high=upper_rest *torch.ones(3))\n",
    "        self.dist_S0 = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "        self.dist_noise = BoxUniform(low=torch.tensor([lower_noise]), high=torch.tensor([upper_noise]))\n",
    "        self.return_numpy = return_numpy\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        \n",
    "        abc     = self.dist_abs.sample(sample_shape)\n",
    "        rest    = self.dist_rest.sample(sample_shape)\n",
    "        S0      = self.dist_S0.sample(sample_shape)\n",
    "        noise   = self.dist_noise.sample(sample_shape)\n",
    "        \n",
    "        if self.return_numpy:   \n",
    "            params = np.hstack([abc,rest,S0,noise]) \n",
    "        else:\n",
    "            params = torch.hstack([abc,rest,S0,noise])\n",
    "\n",
    "        return params\n",
    "        \n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        \n",
    "        abc     = values[:,:3]\n",
    "        rest    = values[:,3:-2]\n",
    "        S0      = values[:,-2]\n",
    "        noise   = values[:,-1]\n",
    "\n",
    "        log_prob_abc  = self.dist_abs.log_prob(abc)\n",
    "        log_prob_rest = self.dist_rest.log_prob(rest)\n",
    "        log_prob_S0 = self.dist_S0.log_prob(S0)\n",
    "        log_prob_noise = self.dist_noise.log_prob(noise)\n",
    "        return log_prob_abc+log_prob_rest+log_prob_S0+log_prob_noise\n",
    "\n",
    "def histogram_mode(data, bins=50):\n",
    "    # Calculate the histogram\n",
    "    counts, bin_edges = np.histogram(data, bins=bins)\n",
    "    \n",
    "    # Find the bin with the maximum count (highest frequency)\n",
    "    max_bin_index = np.argmax(counts)\n",
    "    \n",
    "    # Calculate the mode as the midpoint of the bin with the highest count\n",
    "    mode = (bin_edges[max_bin_index] + bin_edges[max_bin_index + 1]) / 2\n",
    "    \n",
    "    return mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fc460-912e-409a-af59-c18f71ef2b05",
   "metadata": {},
   "source": [
    "### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff72b7da-8284-4f11-b85c-92ab12af1795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Errors(Guess,Truth,gtab,signal_true,signal_provided,S0Guess=200):\n",
    "    # Eigenvalue error\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess)\n",
    "    evals_guess = np.sort(evals_guess_raw)\n",
    "    evals_true_raw,evecs_true = np.linalg.eigh(Truth)\n",
    "    evals_true = np.sort(evals_true_raw)\n",
    "    \n",
    "    EigError = np.linalg.norm(evals_guess-evals_true)\n",
    "\n",
    "    # Mean diffusivitiy\n",
    "    mean_true = np.mean(evals_true)\n",
    "    mean_guess = np.mean(evals_guess)\n",
    "    MD = abs(mean_true-mean_guess)\n",
    "\n",
    "    # Fractional Anisotropy\n",
    "    FA_true  = FracAni(evals_true,mean_true)\n",
    "    FA_guess = FracAni(evals_guess,mean_guess)\n",
    "    FA = abs(FA_true-FA_guess)                                        \n",
    "\n",
    "    # Frobenius error\n",
    "    Frob =  np.linalg.norm(Guess-Truth, 'fro')\n",
    "\n",
    "    # Signal error\n",
    "    signal_guess = single_tensor(gtab, S0=S0Guess, evals=evals_guess_raw, evecs=evecs_guess)\n",
    "    Err  = np.linalg.norm(signal_true-signal_guess)/len(signal_true)\n",
    "    Corr = np.corrcoef(signal_true,signal_guess)[0,1]\n",
    "    \n",
    "    Err2  = np.linalg.norm(signal_provided-signal_guess[:len(signal_provided)])/len(signal_provided)\n",
    "    Corr2 = np.corrcoef(signal_provided,signal_guess[:len(signal_provided)])[0,1]\n",
    "    \n",
    "    return MD,FA,EigError,Frob,Err,Corr,Err2,Corr2\n",
    "\n",
    "def ErrorsMDFA(Guess,Truth):\n",
    "    # Eigenvalue error\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess)\n",
    "    evals_guess = np.sort(evals_guess_raw)\n",
    "    evals_true_raw,evecs_true = np.linalg.eigh(Truth)\n",
    "    evals_true = np.sort(evals_true_raw)\n",
    "\n",
    "    # Mean diffusivitiy\n",
    "    mean_true = np.mean(evals_true)\n",
    "    mean_guess = np.mean(evals_guess)\n",
    "    if(not mean_true == 0):\n",
    "        MD = abs(mean_true-mean_guess)\n",
    "    else:\n",
    "        MD = abs(mean_true-mean_guess)\n",
    "\n",
    "    # Fractional Anisotropy\n",
    "    FA_true  = FracAni(evals_true,mean_true)\n",
    "    FA_guess = FracAni(evals_guess,mean_guess)\n",
    "    if(not FA_true == 0):\n",
    "        FA = abs(FA_true-FA_guess)\n",
    "    else:\n",
    "        FA = abs(FA_true-FA_guess)\n",
    "                                    \n",
    "    \n",
    "    return MD,FA\n",
    "    \n",
    "def PercsMDFA(Guess,Truth):\n",
    "    # Eigenvalue error\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess)\n",
    "    evals_guess = np.sort(evals_guess_raw)\n",
    "    evals_true_raw,evecs_true = np.linalg.eigh(Truth)\n",
    "    evals_true = np.sort(evals_true_raw)\n",
    "\n",
    "    # Mean diffusivitiy\n",
    "    mean_true = np.mean(evals_true)\n",
    "    mean_guess = np.mean(evals_guess)\n",
    "    if(not mean_true == 0):\n",
    "        MD = abs(mean_true-mean_guess)/mean_true\n",
    "    else:\n",
    "        MD = abs(mean_true-mean_guess)/mean_true\n",
    "\n",
    "    # Fractional Anisotropy\n",
    "    FA_true  = FracAni(evals_true,mean_true)\n",
    "    FA_guess = FracAni(evals_guess,mean_guess)\n",
    "    if(not FA_true == 0):\n",
    "        FA = abs(FA_true-FA_guess)/FA_true\n",
    "    else:\n",
    "        FA = abs(FA_true-FA_guess)/FA_true\n",
    "                                    \n",
    "    \n",
    "    return MD,FA\n",
    "\n",
    "\n",
    "def DKIErrors(GuessDT,GuessKT,TruthDT,TruthKT):\n",
    "    guess = DKIMetrics(GuessDT,GuessKT,False)\n",
    "    truth = DKIMetrics(TruthDT,TruthKT,False)\n",
    "\n",
    "    #mk diff\n",
    "    mk = abs(guess[0]-truth[0])\n",
    "    ak = abs(guess[1]-truth[1])\n",
    "    rk = abs(guess[2]-truth[2])\n",
    "    mkt = abs(guess[3]-truth[3])\n",
    "    kfa = abs(guess[4]-truth[4])\n",
    "\n",
    "    return mk,ak,rk,mkt,kfa\n",
    "\n",
    "def Percs(GuessDT,GuessKT,TruthDT,TruthKT):\n",
    "    guess = DKIMetrics(GuessDT,GuessKT,False)\n",
    "    truth = DKIMetrics(TruthDT,TruthKT,False)\n",
    "    \n",
    "    #mk diff\n",
    "    mk = abs(guess[0]-truth[0])/abs(truth[0])\n",
    "    ak = abs(guess[1]-truth[1])/abs(truth[1])\n",
    "    rk = abs(guess[2]-truth[2])/abs(truth[2])\n",
    "    mkt = abs(guess[3]-truth[3])/abs(truth[3])\n",
    "    kfa = abs(guess[4]-truth[4])/abs(truth[4])\n",
    "    \n",
    "    return mk,ak,rk,mkt,kfa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae18af9a-8a6a-41f3-bc95-33001a251354",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21026b37-3475-4748-9de6-89a6d72a53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viol_plot(A,col,hatch=False,**kwargs):\n",
    "    A_T = np.transpose(A)\n",
    "    filtered_A = []\n",
    "    for column in A_T:\n",
    "        # Remove NaNs\n",
    "        column = column[~np.isnan(column)]\n",
    "        # Identify outliers using Z-score\n",
    "        z_scores = stats.zscore(column)\n",
    "        abs_z_scores = np.abs(z_scores)\n",
    "        # Filter data within 3 standard deviations\n",
    "        filtered_entries = (abs_z_scores < 1000)\n",
    "        filtered_column = column[filtered_entries]\n",
    "        filtered_A.append(filtered_column)\n",
    "    \n",
    "    vp = plt.violinplot(filtered_A,showmeans=True,**kwargs)  \n",
    "    for v in vp['bodies']:\n",
    "        v.set_facecolor(col)\n",
    "    vp['cbars'].set_color(col)\n",
    "    vp['cmins'].set_color(col)\n",
    "    vp['cmaxes'].set_color(col)\n",
    "    vp['cmeans'].set_color('black')\n",
    "    if(hatch):\n",
    "        vp['bodies'][0].set_hatch('//')\n",
    "\n",
    "def box_plot(data, edge_color, fill_color, hatch=None, linewidth=1.5, **kwargs):\n",
    "    # Clean data to remove NaNs column-wise\n",
    "    if(np.ndim(data) == 1):\n",
    "        cleaned_data = data[~np.isnan(data)]\n",
    "    else:\n",
    "        cleaned_data = [d[~np.isnan(d)] for d in data]\n",
    "    # Create the box plot with cleaned data\n",
    "    bp = plt.boxplot(cleaned_data, patch_artist=True, **kwargs)\n",
    "    \n",
    "    for element in ['boxes', 'whiskers', 'means', 'medians', 'caps']:\n",
    "        plt.setp(bp[element], color=edge_color, linewidth=linewidth)\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set(facecolor=fill_color, linewidth=linewidth)      \n",
    "        if hatch is not None:\n",
    "            patch.set(hatch=hatch)\n",
    "\n",
    "    return bp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d49c87e-9d2b-4549-beee-b41f4af56a86",
   "metadata": {},
   "source": [
    "## Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b627ef6-dc96-4125-be0a-b41beb7ffe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_path = './Networks/'\n",
    "image_path   = './Images/'\n",
    "if not os.path.exists(image_path):\n",
    "    os.makedirs(image_path)\n",
    "NoiseLevels = [None,20,10,5,2]\n",
    "\n",
    "TrainingSamples = 50000\n",
    "InferSamples    = 100\n",
    "\n",
    "lower_abs,upper_abs = -0.07,0.07\n",
    "lower_rest,upper_rest = -0.015,0.015\n",
    "lower_S0 = 25\n",
    "upper_S0 = 2000\n",
    "Save = True\n",
    "\n",
    "TrueCol  = 'k'\n",
    "NoisyCol = 'k'\n",
    "WLSFit   = np.array([225,190,106])/255\n",
    "SBIFit   = np.array([64,176,166])/255\n",
    "\n",
    "Errors_name = ['MD comparison','FA comparison','eig. comparison','Frobenius','Signal comparison','Correlation','Signal comparison','Correlation2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01707d9a-eca9-48ac-8ae1-d830dbaa35c6",
   "metadata": {},
   "source": [
    "## DKI Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579daa3-3e8e-48a5-a203-1c5366e6f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 1\n",
    "fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "\n",
    "data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "# Get the indices of True values\n",
    "true_indices = np.argwhere(mask)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = true_indices.min(axis=0)\n",
    "max_coords = true_indices.max(axis=0)\n",
    "\n",
    "maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "FlatTD = TestData.reshape(maskdata.shape[0]*maskdata.shape[1],138)\n",
    "FlatTD = FlatTD[FlatTD[:,:69].sum(axis=-1)>0]\n",
    "FlatTD = FlatTD[~np.array(FlatTD<0).any(axis=-1)]\n",
    "\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabExt)\n",
    "tenfit = dkimodel.fit(FlatTD)\n",
    "DKIHCP = tenfit.kt\n",
    "DTIHCP = tenfit.lower_triangular()\n",
    "DKIFull = np.array(DKIHCP)\n",
    "DTIFull = np.array(DTIHCP)\n",
    "\n",
    "\n",
    "DTIFilt1 = DTIFull[(abs(DKIFull)<10).all(axis=1)]\n",
    "DKIFilt1 = DKIFull[(abs(DKIFull)<10).all(axis=1)]\n",
    "DTIFilt = DTIFilt1[(DKIFilt1>-3/7).all(axis=1)]\n",
    "DKIFilt = DKIFilt1[(DKIFilt1>-3/7).all(axis=1)]\n",
    "\n",
    "TrueMets = []\n",
    "FA       = []\n",
    "for (dt,kt) in tqdm.tqdm(zip(DTIFilt,DKIFilt)):\n",
    "    TrueMets.append(DKIMetrics(dt,kt))\n",
    "    FA.append(FracAni(np.linalg.eigh(vals_to_mat(dt))[0],np.mean(np.linalg.eigh(vals_to_mat(dt))[0])))\n",
    "TrueMets = np.array(TrueMets)\n",
    "TrueFA = np.array(FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeab163-c71c-4fd2-b94b-5d57afd1ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full fit\n",
    "DT1_full,DT2_full = FitDT(DTIFilt,1)\n",
    "x4_full,R1_full,x2_full,R2_full = FitKT(DKIFilt,1)\n",
    "\n",
    "# LowFA Fit\n",
    "DT1_lfa,DT2_lfa = FitDT(DTIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "x4_lfa,R1_lfa,x2_lfa,R2_lfa = FitKT(DKIFilt[TrueMets[:,-1]<0.3,:],1)\n",
    "\n",
    "# HighFA Fit\n",
    "DT1_hfa,DT2_hfa = FitDT(DTIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "x4_hfa,R1_hfa,x2_hfa,R2_hfa = FitKT(DKIFilt[TrueMets[:,-1]>0.7,:],1)\n",
    "\n",
    "# UltraLowFA Fit\n",
    "DT1_ulfa,DT2_ulfa = FitDT(DTIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa = FitKT(DKIFilt[TrueMets[:,-1]<0.1,:],1)\n",
    "\n",
    "# HigherAK Fit\n",
    "DT1_hak,DT2_hak = FitDT(DTIFilt[TrueMets[:,1]>0.7,:],1)\n",
    "x4_hak,R1_hak,x2_hak,R2_hak = FitKT(DKIFilt[TrueMets[:,1]>0.9,:],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2175941-5649-49c1-884f-2677ff2e0632",
   "metadata": {},
   "source": [
    "# Fig 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f084d3-4650-48e2-a7dc-002eb5244cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_1/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393315f-c34b-49fe-befc-d5133da1d9f6",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6918a4-e713-4f38-9377-74f5f165c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6492be8-3855-4f42-a7f2-4a9229fa67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(6):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices_20 = [0]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(19):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices_20))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices_20], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices_20.append(next_index)\n",
    "\n",
    "selected_indices_20 = np.array(selected_indices_20)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices3 = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP3))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(15):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP3))) - set(selected_indices3))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices3], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices3.append(next_index)\n",
    "\n",
    "selected_indices3 = np.array(selected_indices3)\n",
    "selected_indices3 = selected_indices3[selected_indices3>0]\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices3_48 = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP3))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(28):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP3))) - set(selected_indices3_48))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices3_48], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices3_48.append(next_index)\n",
    "\n",
    "selected_indices3_48= np.array(selected_indices3_48)\n",
    "selected_indices3_48= selected_indices3_48[selected_indices3_48>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be2c830-99f3-4aea-ba3d-59fcadd618dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.linspace(0, 2 * np.pi, 100)\n",
    "v = np.linspace(0, np.pi, 100)\n",
    "\n",
    "x = 4 * np.outer(np.cos(u), np.sin(v))\n",
    "y = 4 * np.outer(np.sin(u), np.sin(v))\n",
    "z = 4 * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "\n",
    "x1 = 2 * np.outer(np.cos(u), np.sin(v))\n",
    "y1 = 2 * np.outer(np.sin(u), np.sin(v))\n",
    "z1 = 2 * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "#for i in range(2):\n",
    "#    ax.plot_surface(x+random.randint(-5,5), y+random.randint(-5,5), z+random.randint(-5,5),  rstride=4, cstride=4, color='b', linewidth=0, alpha=0.5)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(0,0,0,s=50,color='k',label=r'$B_0$')\n",
    "\n",
    "ax.plot_surface(x1, y1, z1,  rstride=4, cstride=4, color=WLSFit, linewidth=0, alpha=0.25)\n",
    "ax.scatter(2*bvecsHCP[np.sum(bvecsHCP,axis=1)!=0][:,0],2*bvecsHCP[np.sum(bvecsHCP,axis=1)!=0][:,1],2*bvecsHCP[np.sum(bvecsHCP,axis=1)!=0][:,2],s=50,\n",
    "           color=WLSFit-0.2,label=r'$B = 1000$')\n",
    "\n",
    "ax.plot_surface(x, y, z,  rstride=4, cstride=4, color=SBIFit, linewidth=0, alpha=0.25)\n",
    "ax.scatter(4*bvecsHCP3[np.sum(bvecsHCP3,axis=1)!=0][:,0],4*bvecsHCP3[np.sum(bvecsHCP3,axis=1)!=0][:,1],4*bvecsHCP3[np.sum(bvecsHCP3,axis=1)!=0][:,2],s=50,\n",
    "           color=SBIFit-0.2,label=r'$B = 3000$')\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.legend(ncols=3,fontsize=18,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,loc=2,bbox_to_anchor=(0.05,0.3))\n",
    "plt.title('Full Set \\n 69 measurements',y=0.85,fontsize=24)\n",
    "if Save: plt.savefig(FigLoc+'MultiShell.pdf',format='pdf',bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1e4fe-7420-404c-a284-91c564e5a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.linspace(0, 2 * np.pi, 100)\n",
    "v = np.linspace(0, np.pi, 100)\n",
    "\n",
    "x = 4 * np.outer(np.cos(u), np.sin(v))\n",
    "y = 4 * np.outer(np.sin(u), np.sin(v))\n",
    "z = 4 * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "\n",
    "x1 = 2 * np.outer(np.cos(u), np.sin(v))\n",
    "y1 = 2 * np.outer(np.sin(u), np.sin(v))\n",
    "z1 = 2 * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "#for i in range(2):\n",
    "#    ax.plot_surface(x+random.randint(-5,5), y+random.randint(-5,5), z+random.randint(-5,5),  rstride=4, cstride=4, color='b', linewidth=0, alpha=0.5)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(0,0,0,s=50,color='k',label=r'$B_0$')\n",
    "\n",
    "ax.plot_surface(x1, y1, z1,  rstride=4, cstride=4, color=WLSFit, linewidth=0, alpha=0.25)\n",
    "ax.scatter(2*bvecsHCP[selected_indices,0],2*bvecsHCP[selected_indices,1],2*bvecsHCP[selected_indices,2],s=50,\n",
    "           color=WLSFit-0.2,label=r'$B = 1000$')\n",
    "\n",
    "ax.plot_surface(x, y, z,  rstride=4, cstride=4, color=SBIFit, linewidth=0, alpha=0.25)\n",
    "ax.scatter(4*bvecsHCP3[selected_indices3,0],4*bvecsHCP3[selected_indices3,1],4*bvecsHCP3[selected_indices3,2],s=50,\n",
    "           color=SBIFit-0.2,label=r'$B = 3000$')\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('Minimum Set \\n 22 measurements',y=0.85,fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'MiniSet.pdf',format='pdf',bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81764283-d6cb-45e5-83b9-0f805b978347",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.linspace(0, 2 * np.pi, 100)\n",
    "v = np.linspace(0, np.pi, 100)\n",
    "\n",
    "x = 4 * np.outer(np.cos(u), np.sin(v))\n",
    "y = 4 * np.outer(np.sin(u), np.sin(v))\n",
    "z = 4 * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "\n",
    "x1 = 2 * np.outer(np.cos(u), np.sin(v))\n",
    "y1 = 2 * np.outer(np.sin(u), np.sin(v))\n",
    "z1 = 2 * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "#for i in range(2):\n",
    "#    ax.plot_surface(x+random.randint(-5,5), y+random.randint(-5,5), z+random.randint(-5,5),  rstride=4, cstride=4, color='b', linewidth=0, alpha=0.5)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(0,0,0,s=50,color='k',label=r'$B_0$')\n",
    "\n",
    "ax.plot_surface(x1, y1, z1,  rstride=4, cstride=4, color=WLSFit, linewidth=0, alpha=0.25)\n",
    "ax.scatter(2*bvecsHCP[selected_indices_20,0],2*bvecsHCP[selected_indices_20,1],2*bvecsHCP[selected_indices_20,2],s=50,\n",
    "           color=WLSFit-0.2,label=r'$B = 1000$')\n",
    "\n",
    "ax.plot_surface(x, y, z,  rstride=4, cstride=4, color=SBIFit, linewidth=0, alpha=0.25)\n",
    "ax.scatter(4*bvecsHCP3[selected_indices3_48,0],4*bvecsHCP3[selected_indices3_48,1],4*bvecsHCP3[selected_indices3_48,2],s=50,\n",
    "           color=SBIFit-0.2,label=r'$B = 3000$')\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('Medium Set \\n 48 measurements',y=0.85,fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'MedSet.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4b46b-8d88-4cc6-881a-3412631e9c4b",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b4be9-a46d-45ea-b1db-2eaf50c62041",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial = HemiSphere(xyz=bvecs[1:])\n",
    "hsph_initial20 = HemiSphere(xyz=bvecs[1:20])\n",
    "hsph_initial7 = HemiSphere(xyz=bvecs[1:7])\n",
    "hsph_updated,potentials = disperse_charges(hsph_initial,5000)\n",
    "hsph_updated20,potentials = disperse_charges(hsph_initial20,5000)\n",
    "hsph_updated7,potentials = disperse_charges(hsph_initial7,5000)\n",
    "\n",
    "gtabSimF = gradient_table(np.array([0]+[1000]*64).squeeze(), np.vstack([[0,0,0],hsph_updated.vertices]))\n",
    "gtabSim20 = gradient_table(np.array([0]+[1000]*19).squeeze(), np.vstack([[0,0,0],hsph_updated20.vertices]))\n",
    "gtabSim7 = gradient_table(np.array([0]+[1000]*6).squeeze(), np.vstack([[0,0,0],hsph_updated7.vertices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8cc0e1-dd6e-4da5-b4ff-b655654a4e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIPriorDirec:\n",
    "    def __init__(self, lower_abs : Tensor, upper_abs : Tensor, \n",
    "                       lower_rest: Tensor, upper_rest: Tensor,\n",
    "                        return_numpy: bool = False):\n",
    "\n",
    "        self.dist_abs = BoxUniform(low= lower_abs* torch.ones(3), high=upper_abs * torch.ones(3))\n",
    "        self.dist_rest = BoxUniform(low=lower_rest * torch.ones(3), high=upper_rest *torch.ones(3))\n",
    "        self.direction_choice = Categorical(probs=torch.ones(1, 5))\n",
    "        self.return_numpy = return_numpy\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        \n",
    "        abc  = self.dist_abs.sample(sample_shape)\n",
    "        rest = self.dist_rest.sample(sample_shape)\n",
    "        direc = self.direction_choice.sample(sample_shape)\n",
    "        \n",
    "        if self.return_numpy:   \n",
    "            params = np.hstack([abc,rest,direc]) \n",
    "        else:\n",
    "            params = torch.hstack([abc,rest,direc])\n",
    "\n",
    "        return params\n",
    "        \n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        \n",
    "        abc   = values[:,:3]\n",
    "        rest  = values[:,3:-1]\n",
    "        direc = values[:,-1]\n",
    "\n",
    "        log_prob_abc  = self.dist_abs.log_prob(abc)\n",
    "        log_prob_rest = self.dist_rest.log_prob(rest)\n",
    "        log_prob_direc =  self.direction_choice.log_prob(direc)\n",
    "        return log_prob_abc+log_prob_rest+log_prob_direc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d86fc-69e9-4751-99b5-b9d8f4bf54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prior = DTIPriorS0(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0)\n",
    "priorS0, *_ = process_prior(custom_prior) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c262be14-5ff5-4a2d-bb0e-15cb73e81e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "gTabs = [gtabSimF]\n",
    "for _ in range(4):\n",
    "    x = np.random.permutation(np.arange(65))\n",
    "    bvecs_shuffle = gtabSimF.bvecs[x]\n",
    "    bvals_shuffle = gtabSimF.bvals[x]\n",
    "    \n",
    "    gTabs.append(gradient_table(bvals_shuffle, bvecs_shuffle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa8745f-894e-4e6c-8f89-e18d3f894aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/EgPosterior.pickle\"):\n",
    "    with open(f\"{network_path}/EgPosterior.pickle\", \"rb\") as handle:\n",
    "        posterior = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        if(np.random.rand()<0.8):\n",
    "            dt = ForceLowFA(dt)\n",
    "        cG = gTabs[int(params[-1])]\n",
    "        Obs.append(CustomSimulator(dt,cG,200,None))\n",
    "        Par.append(np.append(mat_to_vals(dt),params[-1]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/EgPosterior.pickle\"):\n",
    "        with open(f\"{network_path}/EgPosterior.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f51a66-f808-4ffa-927d-46ecaaf8c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "Samples  = []\n",
    "DTISim = []\n",
    "S0Sim    = []\n",
    "\n",
    "params = priorS0.sample([5000])\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim.append(dt)\n",
    "    S0Sim.append(params[i,-1])\n",
    "    Samples.append([CustomSimulator(dt,gtabSimF, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples = np.array(Samples).squeeze()\n",
    "Samples = np.moveaxis(Samples, 0, -1)\n",
    "\n",
    "Samples20  = []\n",
    "DTISim20 = []\n",
    "S0Sim20    = []\n",
    "\n",
    "params = priorS0.sample([5000])\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim20.append(dt)\n",
    "    S0Sim20.append(params[i,-1])\n",
    "    Samples20.append([CustomSimulator(dt,gtabSim20, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples20 = np.array(Samples20).squeeze()\n",
    "Samples20 = np.moveaxis(Samples20, 0, -1)\n",
    "\n",
    "Samples7  = []\n",
    "DTISim7 = []\n",
    "S0Sim7    = []\n",
    "\n",
    "params = priorS0.sample([5000])\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim7.append(dt)\n",
    "    S0Sim7.append(params[i,-1])\n",
    "    Samples7.append([CustomSimulator(dt,gtabSim7, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples7 = np.array(Samples7).squeeze()\n",
    "Samples7 = np.moveaxis(Samples7, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9bbfb-bc71-44fe-a570-349eeb15cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(5)\n",
    "j = np.random.choice(64)\n",
    "gT = gTabs[i]\n",
    "dT = DTISim[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75958b-13c0-4217-8e57-f1b1dccd8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "tObs = CustomSimulator(dT,gT,200,None)\n",
    "posterior_samples_1 = posterior.sample((InferSamples,), x=tObs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413ea88-ffee-44fb-a585-2505058da58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.subplots(figsize=(6,1))\n",
    "    plt.plot(Samples[0][:,i],c=SBIFit,lw=3)\n",
    "    plt.axis('off')\n",
    "    if Save: plt.savefig(FigLoc+'EgSamples'+str(i)+'.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a33e9-f290-4b8e-a729-872eb61139e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32987d63-c3d4-4171-83ef-67e8fdaa0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "j = 20\n",
    "gT = gtabSimF\n",
    "dT = DTISim[j]\n",
    "tObs = CustomSimulator(dT,gT,200,None)\n",
    "posterior_samples_1 = posterior.sample((InferSamples,), x=tObs)\n",
    "\n",
    "signal_dti = CustomSimulator(vals_to_mat([histogram_mode(p) for p in posterior_samples_1.T][:-1]),gT,200,None)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tObs,lw=3,c='k')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'EgInfPre.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tObs,lw=3,c='k',label='true signal')\n",
    "plt.plot(signal_dti,lw=2,c=SBIFit,ls='--',label='Recon. signal')\n",
    "plt.axis('off')\n",
    "plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1.09,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "if Save: plt.savefig(FigLoc+'EgInfPost.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c189484-1ff0-421b-86b2-850d6a1c634b",
   "metadata": {},
   "source": [
    "# Fig 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a57a7-839f-4ed8-a6db-44916d4b110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_2/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3230ec7-6c56-4214-be21-ceb4e2661dcd",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f6316-182c-494c-a0be-b0f8e187f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "params = priorS0.sample()\n",
    "dtTruth = ComputeDTI(params)\n",
    "dtTruth = ForceLowFA(dtTruth)\n",
    "Truth = CustomSimulator(dtTruth,gtabSimF,S0=200,snr=None)\n",
    "\n",
    "    \n",
    "dt_evals,dt_evecs = np.linalg.eigh(dtTruth)\n",
    "\n",
    "SNR = [CustomSimulator(dtTruth,gtabSimF, S0=200,snr=scale) for scale in NoiseLevels[1:]]\n",
    "    \n",
    "SNR = np.array(SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d1be1-6712-420c-bb24-451577aa6ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(Truth,'k',lw=2,label='True signal')\n",
    "plt.plot(SNR[0],'gray',lw=2,ls='--',label='Noisy signal')\n",
    "plt.axis('off')\n",
    "plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1.09,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "if Save: plt.savefig(FigLoc+'EgSig20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(Truth,'k',lw=2)\n",
    "plt.plot(SNR[1],'gray',lw=2,ls='--')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'EgSig10.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(Truth,'k',lw=2)\n",
    "plt.plot(SNR[2],'gray',lw=2,ls='--')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'EgSig5.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(Truth,'k',lw=2)\n",
    "plt.plot(SNR[3],'gray',lw=2,ls='--')\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'EgSig2.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578625a-ed1c-447b-98f4-70552aa250e0",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d951cf-2e45-4e22-bced-6c2fb895099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNR20 = np.vstack([CustomSimulator(dtTruth,gtabSimF, S0=200,snr=20) for k in range(100)])\n",
    "SNR10 = np.vstack([CustomSimulator(dtTruth,gtabSimF, S0=200,snr=10) for k in range(100)])\n",
    "SNR5 = np.vstack([CustomSimulator(dtTruth,gtabSimF, S0=200,snr=5) for k in range(100)])\n",
    "SNR2 = np.vstack([CustomSimulator(dtTruth,gtabSimF, S0=200,snr=2) for k in range(100)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55acd95-e235-4a98-a9a7-d93dff2be89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenmodel = dti.TensorModel(gtabSimF,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(SNR20)\n",
    "FA20 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD20 = dti.mean_diffusivity(tenfit.evals)\n",
    "tenfit = tenmodel.fit(SNR10)\n",
    "FA10 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD10 = dti.mean_diffusivity(tenfit.evals)\n",
    "tenfit = tenmodel.fit(SNR5)\n",
    "FA5 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD5 = dti.mean_diffusivity(tenfit.evals)\n",
    "tenfit = tenmodel.fit(SNR2)\n",
    "FA2 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD2 = dti.mean_diffusivity(tenfit.evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a487b83-34c5-4576-9ac2-fc5cb29047b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(6.4,2.4))\n",
    "viol_plot(np.array([FA20,FA10,FA5,FA2]).T,WLSFit)\n",
    "\n",
    "l = plt.axhline(FracAni(dt_evals,np.mean(dt_evals)),c='k',lw=3,ls='--',label='True FA')\n",
    "plt.xticks([1,2,3,4],[20,10,5,2],fontsize=28)\n",
    "plt.xticks(fontsize=28)\n",
    "plt.xlabel('SNR',fontsize=32)\n",
    "plt.ylabel('FA',fontsize=32)\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=WLSFit, edgecolor='k', label='Fit FA'),\n",
    "    Line2D([0], [0], color='k', lw=3, ls='--', label='True FA')\n",
    "]\n",
    "plt.legend(handles=legend_elements,loc = 'lower left',bbox_to_anchor=(0.05,0.5),\n",
    "           fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,ncols=2)\n",
    "plt.yticks([0,0.5,1])\n",
    "if Save: plt.savefig(FigLoc+'EgNoiseFA.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "\n",
    "plt.subplots(figsize=(6.4,2.4))\n",
    "viol_plot(np.array([MD20,MD10,MD5,MD2]).T,WLSFit)\n",
    "\n",
    "l = plt.axhline(np.mean(dt_evals),c='k',lw=3,ls='--',label='True MD')\n",
    "plt.xticks([1,2,3,4],[20,10,5,2],fontsize=28)\n",
    "plt.xticks(fontsize=28)\n",
    "plt.xlabel('SNR',fontsize=32)\n",
    "plt.ylabel('MD',fontsize=32)\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=WLSFit, edgecolor='k', label='Fit MD'),\n",
    "    Line2D([0], [0], color='k', lw=3, ls='--', label='True MD')\n",
    "]\n",
    "plt.legend(handles=legend_elements,loc = 'lower left',bbox_to_anchor=(0.05,0.5),\n",
    "           fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,ncols=2)\n",
    "plt.yticks([0,0.001,0.002])\n",
    "plt.ylim((-7.687787458229293e-05, 0.0025))\n",
    "if Save: plt.savefig(FigLoc+'EgNoiseMD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf61bde5-4c65-4c7a-adf8-a27c93e0b604",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0278a7c-6b55-4134-82f1-e4b67836db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prior = DTIPriorS0(lower_abs,upper_abs,lower_rest,upper_rest,0,30)\n",
    "priorNoise, *_ = process_prior(custom_prior) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b48034-d829-43ab-a616-126fd0c9d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimFull.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSimF,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTISimFull.pickle\"):\n",
    "        with open(f\"{network_path}/DTISimFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc261ac-4601-4747-b84a-a590e9a0f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "MD20 = []\n",
    "FA20 = []\n",
    "for S in tqdm.tqdm(SNR20):\n",
    "    posterior_samples_1 = posteriorFull.sample((InferSamples,), x=S,show_progress_bars=False)\n",
    "    CustomSimulator(vals_to_mat([histogram_mode(p) for p in posterior_samples_1.T][:-1]),gT,200,None)\n",
    "    Guess = vals_to_mat(posterior_samples_1.mean(axis=0))\n",
    "    Guess_clean = clip_negative_eigenvalues(Guess)\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess_clean)\n",
    "    MD20.append(np.mean(evals_guess_raw))\n",
    "    FA20.append(FracAni(evals_guess_raw,MD20[-1]))\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "MD10 = []\n",
    "FA10 = []\n",
    "for S in tqdm.tqdm(SNR10):\n",
    "    posterior_samples_1 = posteriorFull.sample((InferSamples,), x=S,show_progress_bars=False)\n",
    "    Guess = vals_to_mat(posterior_samples_1.mean(axis=0))\n",
    "    Guess_clean = clip_negative_eigenvalues(Guess)\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess_clean)\n",
    "    if((evals_guess_raw<0).any()): print(True)\n",
    "    MD10.append(np.mean(evals_guess_raw))\n",
    "    FA10.append(FracAni(evals_guess_raw,MD10[-1]))\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "MD5 = []\n",
    "\n",
    "\n",
    "FA5 = []\n",
    "for S in tqdm.tqdm(SNR5):\n",
    "    posterior_samples_1 = posteriorFull.sample((InferSamples,), x=S,show_progress_bars=False)\n",
    "    Guess = vals_to_mat(posterior_samples_1.mean(axis=0))\n",
    "    Guess_clean = clip_negative_eigenvalues(Guess)\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess_clean)\n",
    "    if((evals_guess_raw<0).any()): print(True)\n",
    "    MD5.append(np.mean(evals_guess_raw))\n",
    "    FA5.append(FracAni(evals_guess_raw,MD5[-1]))\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "MD2 = []\n",
    "FA2 = []\n",
    "for S in tqdm.tqdm(SNR2):\n",
    "    posterior_samples_1 = posteriorFull.sample((InferSamples,), x=S,show_progress_bars=False)\n",
    "    Guess = vals_to_mat(posterior_samples_1.mean(axis=0))\n",
    "    Guess_clean = clip_negative_eigenvalues(Guess)\n",
    "    evals_guess_raw,evecs_guess = np.linalg.eigh(Guess_clean)\n",
    "    if((evals_guess_raw<0).any()): print(True)\n",
    "    MD2.append(np.mean(evals_guess_raw))\n",
    "    FA2.append(FracAni(evals_guess_raw,MD2[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219a61e-45e2-493a-af27-689924cabe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(6.4,2.4))\n",
    "viol_plot(np.array([FA20,FA10,FA5,FA2]).T,SBIFit)\n",
    "\n",
    "l = plt.axhline(FracAni(dt_evals,np.mean(dt_evals)),c='k',lw=3,ls='--',label='True FA')\n",
    "plt.xticks([1,2,3,4],[20,10,5,2],fontsize=28)\n",
    "plt.xticks(fontsize=28)\n",
    "plt.xlabel('SNR',fontsize=32)\n",
    "plt.ylabel('FA',fontsize=32)\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=SBIFit, edgecolor='k', label='Fit FA'),\n",
    "    Line2D([0], [0], color='k', lw=3, ls='--', label='True FA')\n",
    "]\n",
    "plt.legend(handles=legend_elements,loc = 'lower left',bbox_to_anchor=(0.05,0.5),\n",
    "           fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,ncols=2)\n",
    "plt.yticks([0,0.5,1])\n",
    "if Save: plt.savefig(FigLoc+'EgNoiseFA_SBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "    \n",
    "plt.subplots(figsize=(6.4,2.4))\n",
    "viol_plot(np.array([MD20,MD10,MD5,MD2]).T,SBIFit)\n",
    "\n",
    "l = plt.axhline(np.mean(dt_evals),c='k',lw=3,ls='--',label='True MD')\n",
    "plt.xticks([1,2,3,4],[20,10,5,2],fontsize=28)\n",
    "plt.xticks(fontsize=28)\n",
    "plt.xlabel('SNR',fontsize=32)\n",
    "plt.ylabel('MD',fontsize=32)\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=SBIFit, edgecolor='k', label='Fit MD'),\n",
    "    Line2D([0], [0], color='k', lw=3, ls='--', label='True MD')\n",
    "]\n",
    "plt.legend(handles=legend_elements,loc = 'lower left',bbox_to_anchor=(-0.05,-0.05),\n",
    "           fontsize=32,columnspacing=0.3,handlelength=0.6,handletextpad=0.3,ncols=2)\n",
    "plt.yticks([0,0.001,0.002])\n",
    "plt.ylim((-7.687787458229293e-05, 0.0025))\n",
    "if Save: plt.savefig(FigLoc+'EgNoiseMD_SBI.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e9d50e-f8c7-40a0-ae50-4f372badfd75",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a93c59-3bc7-41d7-8b40-cc8004e27262",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "ErrorFull = []\n",
    "NoiseApproxFull = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim[i])\n",
    "        tObs = Samples[k,:,i]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSimF, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "        mat_guess = clip_negative_eigenvalues(mat_guess)\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSimF,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApproxFull.append(ENoise)\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "NoiseApproxFull = np.array(NoiseApproxFull)    \n",
    "\n",
    "Error_s = []\n",
    "for k,gtab,Samps,DTIS in zip([65,20,7],[gtabSimF,gtabSim20,gtabSim7],[Samples,Samples20,Samples7],[DTISim,DTISim20,DTISim7]):\n",
    "    tenmodel = dti.TensorModel(gtab,fit_method='NLLS')\n",
    "    Error_n = []\n",
    "    for S,Noise in zip(Samps,NoiseLevels):\n",
    "        Error = []\n",
    "        for i in range(500):\n",
    "            tenfit = tenmodel.fit(S[:,i])\n",
    "            tensor_vals = dti.lower_triangular(tenfit.quadratic_form)\n",
    "            DT_test = vals_to_mat(tensor_vals)\n",
    "            Error.append(Errors(DT_test,DTIS[i],gtab,Samps[0][:,i],S[:,i]))\n",
    "        Error_n.append(Error)\n",
    "    Error_s.append(Error_n)\n",
    "Error_s = np.array(Error_s)\n",
    "Error_s = np.swapaxes(Error_s,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df5aec-f06e-4c1c-b5eb-7b2df1ce243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(9,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(ErrorFull).T,Errors_name)):\n",
    "    plt.sca(a) \n",
    "    bp = box_plot(E[:,1:].T,SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3,2.3,3.3,4.3],showfliers=False,widths=0.3)\n",
    "    bp2 = box_plot(Error_s[1:,0,:,ll],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    plt.sca(a)\n",
    "    if(ll>3):\n",
    "        plt.xlabel('SNR', fontsize=24)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:])\n",
    "    ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.yticks(fontsize=32)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "\n",
    "    if(ll==1):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(ll==0):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=WLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "\n",
    "    plt.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'SimDatDTIErrors1.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef0fe1-bab4-4e2c-9017-ca63926bbbef",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb01e4-7c08-4e0d-ba26-83a413114af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimMin.pickle\", \"rb\") as handle:\n",
    "        posterior7 = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSim7,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior7 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTISimMin.pickle\"):\n",
    "        with open(f\"{network_path}/DTISimMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c3560-9de6-4037-a85d-13253dd668b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "Error7 = []\n",
    "NoiseApprox7 = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim7[i])\n",
    "        tObs = Samples7[k,:,i]\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSim7, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posterior7.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSim7,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApprox7.append(ENoise)\n",
    "    Error7.append(ErrorN2)\n",
    "\n",
    "NoiseApprox7 = np.array(NoiseApprox7)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243f9b28-cfb2-4d77-b1d8-f1a2abd2bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(9,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(Error7).T,Errors_name)):\n",
    "    plt.sca(a) \n",
    "    bp = box_plot(E[:,1:].T,SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3,2.3,3.3,4.3],showfliers=False,widths=0.3)\n",
    "    bp2 = box_plot(Error_s[1:,-1,:,ll],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    plt.sca(a)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.yticks(fontsize=32)\n",
    "    if(ll==1):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        #plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.05),\n",
    "        #           fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(ll==2):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=WLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        #plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "        #           fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    #PlotSig(1,1.3,ymax,ydiff2=ymax*0.01,ydiff1=ymax*0.1)\n",
    "    plt.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'SimDatDTIErrors1_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0923c0-67be-4f86-9b1c-191654ce68f9",
   "metadata": {},
   "source": [
    "# Fig 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29467140-7c3d-4864-98ba-3707b1d34c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_3/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ff76e-544b-4ac4-9d09-4a16812ad61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial = HemiSphere(xyz=bvecs[1:])\n",
    "hsph_updated,_ = disperse_charges(hsph_initial,5000)\n",
    "bvecs = np.vstack([[0,0,0],hsph_updated.vertices])\n",
    "bvalsExt = np.hstack([bvals, 3000*np.ones_like(bvals)])\n",
    "bvecsExt = np.vstack([bvecs, bvecs])\n",
    "bvalsExt[65] = 0\n",
    "gtabSim = gradient_table(bvalsExt, bvecsExt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a12387a-fdfa-449b-a1b2-c138d9092053",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6deaca7-7ee4-45c2-9ff0-8aa01c95c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DKISimFull.pickle\"):\n",
    "    with open(f\"{network_path}/DKISimFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*6000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*2000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*6000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*6000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(2.5*6000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabSim.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabSim,200,np.random.rand()*30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>800).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKISimFull.pickle\"):\n",
    "        with open(f\"{network_path}/DKISimFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98490cad-3984-48db-a92a-8cfbfcc6ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "j = 1\n",
    "vL = torch.tensor([0.2*j])\n",
    "vS = torch.tensor([0.01*j])  \n",
    "\n",
    "kk = np.random.randint(0,4)\n",
    "if(kk==0):\n",
    "    DT,KT = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],2,1)\n",
    "elif(kk==1):\n",
    "    DT,KT = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],2,1)\n",
    "elif(kk==2):\n",
    "    DT,KT = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],2,1)\n",
    "elif(kk==3):\n",
    "    DT,KT = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],2,1)\n",
    "\n",
    "tObs = CustomDKISimulator(DT.squeeze(),KT.squeeze(),gtabSim,200,20)\n",
    "tTrue = CustomDKISimulator(DT.squeeze(),KT.squeeze(),gtabSim,200,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4279dc0-f2d5-4b85-9804-d13ac4882440",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=True)\n",
    "GuessDKI = posterior_samples_1.mean(axis=0)\n",
    "GuessSig = CustomDKISimulator(GuessDKI[:6],GuessDKI[6:],gtabSim,200)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k',label='true signal')\n",
    "plt.plot(GuessSig,lw=2,c=SBIFit,ls='--',label='SBI signal')\n",
    "plt.axis('off')\n",
    "plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1.09,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "if Save: plt.savefig(FigLoc+'FullReconSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4533f-aafe-430d-9b66-549992a737e8",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9297bb3a-5f74-4a8c-b54a-0da825073b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodel = dki.DiffusionKurtosisModel(gtabSim,fit_method='NLLS')\n",
    "tenfit = dkimodel.fit(tObs)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k',label='true signal')\n",
    "plt.plot(tenfit.predict(gtabSim,200),lw=2,c=WLSFit,ls='--',label='NLLS signal')\n",
    "plt.axis('off')\n",
    "plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1.09,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "if Save: plt.savefig(FigLoc+'FullReconWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b90aa2-db10-43e5-8e05-7b3f7b7a4096",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d532987-27d0-4e08-8589-81ff05571d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "Mets = []\n",
    "MetsSBI = []\n",
    "for i in tqdm.tqdm([20,10,5,2]):\n",
    "    m = []\n",
    "    m2 = []\n",
    "    for k in range(50):\n",
    "        tObs = CustomDKISimulator(np.squeeze(DT), np.squeeze(KT),gtabSim, S0=200, snr=i)#\n",
    "        dkimodel = dki.DiffusionKurtosisModel(gtabSim,fit_method='NLLS')\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        m.append(DKIMetrics(tenfit.lower_triangular(),tenfit.kt,False))\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessDKI = posterior_samples_1.mean(axis=0)\n",
    "        m2.append(DKIMetrics(GuessDKI[:6],GuessDKI[6:],False))\n",
    "    Mets.append(m)\n",
    "    MetsSBI.append(m2)\n",
    "Mets = np.array(Mets)\n",
    "MetsSBI = np.array(MetsSBI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5d470-157e-4c6d-9894-60385e4923b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,5,figsize=(22.5,3))\n",
    "for i in range(5):\n",
    "    plt.sca(ax[i])\n",
    "    viol_plot(Mets[:,:,i].T,WLSFit,)\n",
    "    viol_plot(MetsSBI[:,:,i].T,SBIFit,widths=0.3,positions=[1.3,2.3,3.3,4.3],)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.axhline(DKIMetrics(np.squeeze(DT),np.squeeze(KT),False)[i],lw=3,ls='--',c='k')\n",
    "    plt.yticks(fontsize=32)\n",
    "\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=WLSFit, edgecolor='k', label='NLLS')\n",
    "]\n",
    "ax[0].legend(handles=legend_elements, ncols=1,loc=1,bbox_to_anchor=(0.8,1),fontsize=32,columnspacing=0.5,handlelength=0.8,handletextpad=0.3)\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=SBIFit, edgecolor='k', label='SBI')\n",
    "]\n",
    "ax[1].legend(handles=legend_elements, ncols=1,loc=1,bbox_to_anchor=(0.8,1),fontsize=32,columnspacing=0.5,handlelength=0.8,handletextpad=0.3)\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='k', lw=3, ls='--', label='True value')\n",
    "]\n",
    "ax[3].legend(handles=legend_elements, ncols=1,loc=1,bbox_to_anchor=(0.95,1.1),fontsize=32,columnspacing=0.5,handlelength=0.8,handletextpad=0.3)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'EgSigMetricsFull.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf4eaa-1db0-4c54-bf74-51932a3334d8",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4ed0c-871e-408f-8973-55e21e3a6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],1,40)\n",
    "DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],1,40)\n",
    "DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],1,40)\n",
    "DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],1,40)\n",
    "DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,40)\n",
    "\n",
    "SampsDT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "SampsKT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "\n",
    "Samples  = []\n",
    "\n",
    "for Sd,Sk in zip(SampsDT,SampsKT):\n",
    "    Samples.append([CustomDKISimulator(Sd,Sk,gtabSim, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "\n",
    "Samples = np.array(Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00192bd5-9b97-446c-aef2-3b4af01f56e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "ErrorFull = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples[i,k,:]\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessSBI = posterior_samples_1.mean(axis=0)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(GuessSBI[:6],GuessSBI[6:],SampsDT[i],SampsKT[i]))\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "Error_s = []\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabSim,fit_method='NLLS')\n",
    "\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(tenfit.lower_triangular(),tenfit.kt,SampsDT[i],SampsKT[i]))\n",
    "    Error_s.append(ErrorN2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32064e59-73fa-4b8b-b399-bc2433d727ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorFull = np.array(ErrorFull)\n",
    "Error_s = np.array(Error_s)\n",
    "ErrorNames = ['MK Error', 'AK Error', 'RK Error', 'MKT Error', 'KFA Error']\n",
    "fig,ax = plt.subplots(1,5,figsize=(22.5,3))\n",
    "for i in range(5):\n",
    "    plt.sca(ax[i])\n",
    "    box_plot(Error_s[1:,:,i],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,showmeans=False,widths=0.3)\n",
    "    box_plot(ErrorFull[1:,:,i],SBIFit-0.2, np.clip(SBIFit+0.2,0,1),showfliers=False,widths=0.3,positions=[1.3,2.3,3.3,4.3])\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    plt.yticks(fontsize=32)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'ErrorsFull.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d711f3-ecab-4e8e-b514-d7295ae43846",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52920cf-19e4-40ba-bc54-96d26daebcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial15 = HemiSphere(xyz=bvecs[1:16])\n",
    "hsph_initial7 = HemiSphere(xyz=bvecs[1:7])\n",
    "hsph_updated15,_ = disperse_charges(hsph_initial15,5000)\n",
    "hsph_updated7,_ = disperse_charges(hsph_initial7,5000)\n",
    "gtabSimSub = gradient_table(np.array([0]+[1000]*6+[3000]*15).squeeze(), np.vstack([[0,0,0],hsph_updated7.vertices,hsph_updated15.vertices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d51d85c-a04d-4cd0-97b6-13d33e5e0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/7SampSim.pickle\"):\n",
    "    with open(f\"{network_path}/7SampSim.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*6000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*2000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*6000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*6000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(2.5*6000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabSimSub.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabSimSub,200,np.random.rand()*30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>800).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKISimMin.pickle\"):\n",
    "        with open(f\"{network_path}/DKISimMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6bdf39-6088-45ad-92c2-fa4a6d175318",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "j = 1\n",
    "vL = torch.tensor([0.2*j])\n",
    "vS = torch.tensor([0.01*j])  \n",
    "\n",
    "kk = np.random.randint(0,4)\n",
    "if(kk==0):\n",
    "    DT,KT = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],2,1)\n",
    "elif(kk==1):\n",
    "    DT,KT = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],2,1)\n",
    "elif(kk==2):\n",
    "    DT,KT = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],2,1)\n",
    "elif(kk==3):\n",
    "    DT,KT = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],2,1)\n",
    "\n",
    "tObs = CustomDKISimulator(np.squeeze(DT),np.squeeze(KT),gtabSimSub,200,20)\n",
    "tTrue = CustomDKISimulator(np.squeeze(DT),np.squeeze(KT),gtabSim,200,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b976b7-8ad0-4176-87ab-1a2a4770c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b087be-0056-42ab-93b1-5b0e7439c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "GuessDKI = posterior_samples_1.mean(axis=0)\n",
    "GuessSig = CustomDKISimulator(GuessDKI[:6],GuessDKI[6:],gtabSim,200)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k',label='true signal')\n",
    "plt.plot(GuessSig,lw=2,c=SBIFit,ls='--',label='SBI signal')\n",
    "plt.axis('off')\n",
    "plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1.09,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "plt.fill_betweenx(np.arange(0,500,50),0*np.ones(10),7*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.fill_betweenx(np.arange(0,500,50),64*np.ones(10),79*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.ylim(-9.996985449425491, 209.99985644997255)\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'7ReconSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8bd10e-73ea-497f-826f-d79f8bb76422",
   "metadata": {},
   "source": [
    "## f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae74aee-a22e-45f5-a3b5-49b5a69bd6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "tenfit = dkimodel.fit(tObs)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k',label='true signal')\n",
    "plt.plot(tenfit.predict(gtabSim,200),lw=2,c=WLSFit,ls='--',label='NLLS signal')\n",
    "plt.axis('off')\n",
    "plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1.09,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "plt.fill_betweenx(np.arange(0,500,50),0*np.ones(10),7*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.fill_betweenx(np.arange(0,500,50),64*np.ones(10),79*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.ylim(-9.996985449425491, 209.99985644997255)\n",
    "if Save: plt.savefig(FigLoc+'7ReconWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1bd02-c673-4fad-afd1-c54363126de5",
   "metadata": {},
   "source": [
    "## g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff16bf-9a5e-46a8-9453-2842544e3321",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "Mets = []\n",
    "MetsSBI = []\n",
    "for i in tqdm.tqdm([20,10,5,2]):\n",
    "    m = []\n",
    "    m2 = []\n",
    "    for k in range(50):\n",
    "        tObs = CustomDKISimulator(np.squeeze(DT), np.squeeze(KT),gtabSimSub, S0=200, snr=i)#\n",
    "        dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        m.append(DKIMetrics(tenfit.lower_triangular(),tenfit.kt,False))\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessDKI = posterior_samples_1.mean(axis=0)\n",
    "        m2.append(DKIMetrics(GuessDKI[:6],GuessDKI[6:],False))\n",
    "    Mets.append(m)\n",
    "    MetsSBI.append(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffc7d7-4601-4345-9048-72998e1ea099",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mets = np.array(Mets)\n",
    "MetsSBI = np.array(MetsSBI)\n",
    "fig,ax = plt.subplots(1,5,figsize=(22.5,3))\n",
    "for i in range(5):\n",
    "    plt.sca(ax[i])\n",
    "    viol_plot(Mets[:,:,i].T,WLSFit,)\n",
    "    viol_plot(MetsSBI[:,:,i].T,SBIFit,widths=0.3,positions=[1.3,2.3,3.3,4.3],)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.axhline(DKIMetrics(np.squeeze(DT),np.squeeze(KT),False)[i],lw=3,ls='--',c='k')\n",
    "    plt.yticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'EgSigMetrics7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f2d41-66ae-4b4c-b234-96fed0f022c2",
   "metadata": {},
   "source": [
    "## h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e098d3e-ecf5-4ee6-80f6-47081d8532a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "Samples7  = []\n",
    "\n",
    "for Sd,Sk in zip(SampsDT,SampsKT):\n",
    "    Samples7.append([CustomDKISimulator(Sd,Sk,gtabSimSub, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "\n",
    "Samples7 = np.array(Samples7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a331976-ed14-407a-8c1d-e2b31abc7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "ErrorFull = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples7[i,k,:]\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessSBI = posterior_samples_1.mean(axis=0)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(GuessSBI[:6],GuessSBI[6:],SampsDT[i],SampsKT[i]))\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "Error_s = []\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples7[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(tenfit.lower_triangular(),tenfit.kt,SampsDT[i],SampsKT[i]))\n",
    "    Error_s.append(ErrorN2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ef157-9dc7-4a2b-958d-d152f3ed5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorFull = np.array(ErrorFull)\n",
    "Error_s = np.array(Error_s)\n",
    "ErrorNames = ['MK Error', 'AK Error', 'RK Error', 'MKT Error', 'KFA Error']\n",
    "fig,ax = plt.subplots(1,5,figsize=(22.5,3))\n",
    "for i in range(5):\n",
    "    plt.sca(ax[i])\n",
    "    box_plot(Error_s[1:,:,i],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    box_plot(ErrorFull[1:,:,i],SBIFit-0.2, np.clip(SBIFit+0.2,0,1),showfliers=False,widths=0.3,positions=[1.3,2.3,3.3,4.3])\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    plt.yticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Errors7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b079e1-c405-462b-9bb9-39721d97377a",
   "metadata": {},
   "source": [
    "# Fig 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8f1ba-84f2-4343-abdf-84a506446661",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_4/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c18fa-4118-42ea-9a33-1f8995ad7dc5",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd27b2-186a-4907-bdbc-bf83f58585b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdwi = './HCP_data/Pat'+str(1)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(1)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(1)+'/bvecs_1k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = [0]+selected_indices\n",
    "\n",
    "bvalsHCP7 = bvalsHCP[selected_indices]\n",
    "bvecsHCP7 = bvecsHCP[selected_indices]\n",
    "gtabHCP7 = gradient_table(bvalsHCP7, bvecsHCP7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1861ad70-3e54-4b54-9a07-fc1659a6d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prior = DTIPriorS0Noise(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0,0,30)\n",
    "priorS0Noise, *_ = process_prior(custom_prior) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fac026-4927-4faf-b4c3-71a24acf8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIHCPFull.pickle\"):\n",
    "    with open(f\"{network_path}/DTIHCPFull.pickle\", \"rb\") as handle:\n",
    "        posterior2 = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    bvals = gtabHCP.bvals\n",
    "    bvecs = gtabHCP.bvecs\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorS0.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        if(np.random.rand()<0.2):\n",
    "            dt = ForceLowFA(dt)\n",
    "        Obs.append(CustomSimulator(dt,gtabHCP,params[-1],np.random.rand()*30))\n",
    "        Par.append(np.hstack([mat_to_vals(ComputeDTI(params)),params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior2 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIHCPFull.pickle\"):\n",
    "        with open(f\"{network_path}/DTIHCPFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior2, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f119a07e-f7b4-4e84-bc05-ec56593d0443",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArrShape = maskdata[:,:,axial_middle,0].shape\n",
    "NoiseEst = np.zeros([55,64,7])\n",
    "VarEst   = np.zeros([55,64])\n",
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        torch.manual_seed(10)\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            np.random.seed(1)\n",
    "            torch.manual_seed(1)\n",
    "            posterior_samples_1 = posterior2.sample((InferSamples,), x=maskdata[i,j,axial_middle,:],show_progress_bars=False)\n",
    "            NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb0973-b879-4d45-986b-8d143387ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(55):\n",
    "    for j in range(64):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1]])\n",
    "MD_SBIFull = np.zeros([55,64])\n",
    "FA_SBIFull = np.zeros([55,64])\n",
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "        MD_SBIFull[i,j] = np.mean(Eigs)\n",
    "        FA_SBIFull[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "FA_SBIFull[np.isnan(FA_SBIFull)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146872e2-8862-493e-aa15-630326e6513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenmodel = dti.TensorModel(gtabHCP,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(maskdata[:,:,axial_middle])\n",
    "FAFull = dti.fractional_anisotropy(tenfit.evals)\n",
    "MDFull = dti.mean_diffusivity(tenfit.evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a66437c-abd6-4ff9-9476-c5fe93432779",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            FAFull[i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c966ad9-e02b-4822-ae36-f037e2d6de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(MD_SBIFull.T,cmap='gray')\n",
    "plt.axis('off')\n",
    "vmin, vmax = img.get_clim()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_MD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c9f41b-7ba1-41e2-939c-d8e437671633",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MDFull.T,cmap='gray',vmin=vmin, vmax=vmax)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_MD.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d091f16e-561a-495a-a8b6-679abc97dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MDFull.T-MD_SBIFull.T\n",
    "norm = TwoSlopeNorm(vmin=np.nanmin(data), vcenter=0, vmax=np.nanmax(data))\n",
    "plt.imshow(data,cmap='seismic',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "ticks = [np.nanmin(data), 0, np.nanmax(data)]  # Adjust the number of ticks as needed\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_MD_Diff.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9b72f-45d8-41b3-b030-0de13cf00cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.abs(MD_SBIFull*mask[:,:,axial_middle]-MDFull*mask[:,:,axial_middle])))\n",
    "ssim_noise = ssim(MD_SBIFull, MDFull, data_range=np.max([MD_SBIFull.max(),MDFull.max()])-np.min([MD_SBIFull.min(),MDFull.min()]))\n",
    "print(ssim_noise)\n",
    "Num = 2*np.abs(MD_SBIFull*mask[:,:,axial_middle]-MDFull*mask[:,:,axial_middle])\n",
    "Den = np.abs(MDFull*mask[:,:,axial_middle])+np.abs(MD_SBIFull*mask[:,:,axial_middle])\n",
    "print(np.nanmedian(Num/Den))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5695e966-9ed5-4ae0-a22e-b998f0f296f6",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98661a9-7d30-4e64-8716-f6ed92053481",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIHCPMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTIHCPMin.pickle\", \"rb\") as handle:\n",
    "        posterior7_2 = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorS0.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        if(np.random.rand()<0.2):\n",
    "            dt = ForceLowFA(dt)\n",
    "        Obs.append(CustomSimulator(dt,gtabHCP7,params[-1],np.random.rand()*30)[:7])\n",
    "        Par.append(np.hstack([mat_to_vals(ComputeDTI(params)),params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior7_2= inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIHCPMin.pickle\"):\n",
    "        with open(f\"{network_path}/DTIHCPMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7_2, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fcd07-8d70-4dbf-93e4-b672ddcaba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArrShape = maskdata[:,:,axial_middle,0].shape\n",
    "NoiseEst = np.zeros([55,64,7])\n",
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        torch.manual_seed(10)\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            posterior_samples_1 = posterior7_2.sample((InferSamples,), x=maskdata[i,j,axial_middle,selected_indices],show_progress_bars=False)\n",
    "            NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742850a0-e986-48f0-a400-79624555eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(55):\n",
    "    for j in range(64):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1]])\n",
    "MD_SBI7 = np.zeros([55,64])\n",
    "FA_SBI7 = np.zeros([55,64])\n",
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "        MD_SBI7[i,j] = np.mean(Eigs)\n",
    "        FA_SBI7[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "FA_SBI7[np.isnan(FA_SBI7)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067bcc4-e8d9-408c-8d15-2e6ee1b9fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenmodel = dti.TensorModel(gtabHCP7,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(maskdata[:,:,axial_middle,selected_indices])\n",
    "FA7 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD7 = dti.mean_diffusivity(tenfit.evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529803a8-74e2-44a3-94c4-5bac3ad7ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            FA7[i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b987c-3dfc-4e83-b02d-f03f6801614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(MD_SBI7.T,cmap='gray')\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_MD_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d398649-abb6-4c4e-8b02-e575abf05c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MD7.T,cmap='gray',vmin=vmin, vmax=vmax)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_MD_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3ef39-b15d-4076-80a7-e9f030476c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs(MD_SBIFullNan.T-MD_SBI7Nan.T)\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.max(data)/2, vmax=np.max(data))\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'DTI_MDSBIErr.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.max(data)/2, vmax=np.max(data))\n",
    "ticks = [0, np.round(np.max(data),3)]  # Adjust the number of ticks as needed\n",
    "data = np.abs(MDFull.T-MD7.T)\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'DTI_MDWLSErr.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac25cad-973d-40a9-884b-7604e4bc0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.abs(MD_SBIFull*mask[:,:,axial_middle]-MD_SBI7*mask[:,:,axial_middle])))\n",
    "ssim_noise = ssim(MD_SBIFull*mask[:,:,axial_middle], MD_SBI7*mask[:,:,axial_middle], data_range=MD_SBIFull.max()-MD_SBIFull.min())\n",
    "print(ssim_noise)\n",
    "Num = 2*np.abs(MD_SBIFull*mask[:,:,axial_middle]-MD_SBI7*mask[:,:,axial_middle])\n",
    "Den = np.abs(MD_SBI7*mask[:,:,axial_middle])+np.abs(MD_SBIFull*mask[:,:,axial_middle])\n",
    "print(np.nanmedian(Num/Den))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb9453b-89fe-4acc-939d-b09887da7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.abs(MDFull*mask[:,:,axial_middle]-MD7*mask[:,:,axial_middle])))\n",
    "ssim_noise = ssim(MDFull*mask[:,:,axial_middle], MD7*mask[:,:,axial_middle], data_range=MDFull.max()-MDFull.min())\n",
    "print(ssim_noise)\n",
    "Num = 2*np.abs(MDFull*mask[:,:,axial_middle]-MD7*mask[:,:,axial_middle])\n",
    "Den = np.abs(MD7*mask[:,:,axial_middle])+np.abs(MDFull*mask[:,:,axial_middle])\n",
    "print(np.nanmedian(Num/Den))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2e8d4-affd-429f-90b1-38b63f0a5d4f",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a84cc-6f5b-4c4f-86d6-784b3448a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(FA_SBIFull.T,cmap='gray')\n",
    "plt.axis('off')\n",
    "vmin, vmax = img.get_clim()\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_FA.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46f2af-7467-413c-9423-ff3191dbd40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(FAFull.T,cmap='gray')\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "tick_labels = ['{:.2e}'.format(t) for t in cbar.get_ticks()]\n",
    "cbar.set_ticks(cbar.get_ticks())\n",
    "cbar.set_ticklabels(tick_labels)\n",
    "cbar.update_ticks()\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_FA.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ad1b7-a270-4e64-971e-b486b5d1178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FAFull.T-FA_SBIFull.T\n",
    "plt.imshow(data,cmap='seismic',vmin=-1, vmax=1)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "ticks = [-1, 0, 1]  # Adjust the number of ticks as needed\n",
    "cbar.set_ticks(ticks)\n",
    "if Save: plt.savefig(FigLoc+'HCP_FA_Diff.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480c563-007b-43db-9686-dab4a89c5d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.abs(FA_SBIFull*mask[:,:,axial_middle]-FAFull*mask[:,:,axial_middle])))\n",
    "ssim_noise = ssim(FA_SBIFull, FAFull, data_range=np.max([FA_SBIFull.max(),FAFull.max()])-np.min([FA_SBIFull.min(),FAFull.min()]))\n",
    "print(ssim_noise)\n",
    "Num = 2*np.abs(FA_SBIFull*mask[:,:,axial_middle]-FAFull*mask[:,:,axial_middle])\n",
    "Den = np.abs(FAFull*mask[:,:,axial_middle])+np.abs(FA_SBIFull*mask[:,:,axial_middle])\n",
    "print(np.nanmedian(Num/Den))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097378d3-bd31-4c46-a53d-f2d730bf2755",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d478309-a4cf-466c-8035-2b4e04936c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(FA_SBI7.T,cmap='gray')\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_FA_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5255eebc-507a-4e31-8abd-389236313d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(FA_SBI7.T,cmap='gray')\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "#if Save: plt.savefig(FigLoc+'HCP_SBI_FA_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43272bdc-b629-4b89-becc-2f884b8635df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(FA7.T,cmap='gray',vmin=vmin, vmax=vmax)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_FA_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f222f-b192-49e0-a2dc-f7c45bd2fb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import TwoSlopeNorm\n",
    "data = np.abs(FA_SBIFull.T-FA_SBI7.T)\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.max(data)/2, vmax=np.max(data))\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "if Save: plt.savefig(FigLoc+'DTI_FASBIErr.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.max(data)/2, vmax=np.max(data))\n",
    "ticks = [0, np.max(data)]  # Adjust the number of ticks as needed\n",
    "data = np.abs(FAFull.T-FA7.T)\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "\n",
    "cbar.set_ticks(ticks)\n",
    "if Save: plt.savefig(FigLoc+'DTI_FAWLSErr.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd934c0-2497-4c59-8fb1-d3ea59b68904",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.abs(FA_SBIFull*mask[:,:,axial_middle]-FA_SBI7*mask[:,:,axial_middle])))\n",
    "ssim_noise = ssim(FA_SBIFull*mask[:,:,axial_middle], FA_SBI7*mask[:,:,axial_middle], data_range=FA_SBIFull.max()-FA_SBIFull.min())\n",
    "print(ssim_noise)\n",
    "Num = 2*np.abs(FA_SBIFull*mask[:,:,axial_middle]-FA_SBI7*mask[:,:,axial_middle])\n",
    "Den = np.abs(FA_SBI7*mask[:,:,axial_middle])+np.abs(FA_SBIFull*mask[:,:,axial_middle])\n",
    "print(np.nanmedian(Num/Den))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105674a-f2c2-4e03-af6d-8e65334fad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.abs(FAFull*mask[:,:,axial_middle]-FA7*mask[:,:,axial_middle])))\n",
    "ssim_noise = ssim(FAFull*mask[:,:,axial_middle], FA7*mask[:,:,axial_middle], data_range=FAFull.max()-FAFull.min())\n",
    "print(ssim_noise)\n",
    "Num = 2*np.abs(FAFull*mask[:,:,axial_middle]-FA7*mask[:,:,axial_middle])\n",
    "Den = np.abs(FA7*mask[:,:,axial_middle])+np.abs(FAFull*mask[:,:,axial_middle])\n",
    "print(np.nanmedian(Num/Den))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5323b2-b1dd-4d92-b51a-e756092f4485",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c62af2-ae9f-42db-818e-899a571daa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prior = DTIPriorS0Direc(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0)\n",
    "priorDirec, *_ = process_prior(custom_prior) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a777b1-6e4e-4b7b-86c4-55443abda5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gTabsF = []\n",
    "gTabs20 = []\n",
    "gTabs7 = []\n",
    "\n",
    "Indices20 = []\n",
    "Indices7  = []\n",
    "FullDat   = []\n",
    "for i in tqdm.tqdm(range(1,6)):\n",
    "    fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    gTabsF.append(gtabHCP)\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    axial_middle = maskdata.shape[2] // 2\n",
    "    TestData = maskdata[:, :, axial_middle, :]\n",
    "    FlatTD = TestData.reshape(maskdata.shape[0]*maskdata.shape[1],69)\n",
    "    FlatTD = FlatTD[FlatTD.sum(axis=-1)>0]\n",
    "    FlatTD = FlatTD[~np.array(FlatTD<0).any(axis=-1)]\n",
    "    FullDat.append(FlatTD)\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    distance_matrix = squareform(pdist(bvecsHCP))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(6):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    bvalsHCP7 = bvalsHCP[selected_indices]\n",
    "    bvecsHCP7 = bvecsHCP[selected_indices]\n",
    "    gtabHCP7 = gradient_table(bvalsHCP7, bvecsHCP7)\n",
    "\n",
    "    gTabs7.append(gtabHCP7)\n",
    "    Indices7.append(selected_indices)\n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    distance_matrix = squareform(pdist(bvecsHCP))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(19):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    \n",
    "    bvalsHCP20 = bvalsHCP[selected_indices]\n",
    "    bvecsHCP20 = bvecsHCP[selected_indices]\n",
    "    gtabHCP20 = gradient_table(bvalsHCP20, bvecsHCP20)\n",
    "    gTabs20.append(gtabHCP20)\n",
    "    Indices20.append(selected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e193c87-aaa0-4d8e-bcd7-bb6d2c026eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gTabsPermF = []\n",
    "gTabsPerm20 = []\n",
    "gTabsPerm7 = []\n",
    "Perms     = []\n",
    "Perms20   = []\n",
    "Perms7    = []\n",
    "x = np.random.seed(116)\n",
    "for i in tqdm.tqdm(range(1,6)):\n",
    "    x = np.random.permutation(np.arange(69))\n",
    "    x20 = np.random.permutation(np.arange(20))\n",
    "    x7 = np.random.permutation(np.arange(7)) # Got to fix\n",
    "    Perms.append(x)\n",
    "    Perms20.append(x20)\n",
    "    Perms7.append(x7)\n",
    "    gTabsPermF.append(gradient_table(gTabsF[i-1].bvals[x], gTabsF[i-1].bvecs[x]))\n",
    "    gTabsPerm20.append(gradient_table(gTabs20[i-1].bvals[x20], gTabs20[i-1].bvecs[x20]))\n",
    "    gTabsPerm7.append(gradient_table(gTabs7[i-1].bvals[x7], gTabs7[i-1].bvecs[x7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa2ee7-57e9-41dd-ba6d-ae393e6d5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIMultiHCPFull.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMultiHCPFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-2])\n",
    "        if(np.random.rand()<0.2):\n",
    "            dt = ForceLowFA(dt)\n",
    "        cG = gTabsPermF[int(params[-1])]\n",
    "        Obs.append(CustomSimulator(dt,cG,params[-2],np.random.rand()*30))\n",
    "        Par.append(np.hstack([mat_to_vals(ComputeDTI(params)),params[-2],params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorDirec)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMultiHCPFull.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMultiHCPFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb311102-360c-47f2-a859-d749596f38d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIMultiHCPMid.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMultiHCPMid.pickle\", \"rb\") as handle:\n",
    "        posterior20 = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-2])\n",
    "        if(np.random.rand()<0.2):\n",
    "            dt = ForceLowFA(dt)\n",
    "        cG = gTabsPerm20[int(params[-1])]\n",
    "        Obs.append(CustomSimulator(dt,cG,params[-2],np.random.rand()*30)[:20])\n",
    "        Par.append(np.hstack([mat_to_vals(ComputeDTI(params)),params[-2],params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorDirec)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior20 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMultiHCPMid.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMultiHCPMid.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior20, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb6a97-ad5b-4247-adc4-36462cc9c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIMultiHCPMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTIMultiHCPMin.pickle\", \"rb\") as handle:\n",
    "        posterior7 = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorDirec.sample()\n",
    "        dt = ComputeDTI(params[:-2])\n",
    "        if(np.random.rand()<0.2):\n",
    "            dt = ForceLowFA(dt)\n",
    "        cG = gTabsPerm7[int(params[-1])]\n",
    "        Obs.append(CustomSimulator(dt,cG,params[-2],np.random.rand()*30)[:7])\n",
    "        Par.append(np.hstack([mat_to_vals(ComputeDTI(params)),params[-2],params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    \n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorDirec)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior7 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIMultiHCPMin.pickle\"):\n",
    "        with open(f\"{network_path}/DTIMultiHCPMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45204ae1-2283-490e-a1d7-9bbbc8a7bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "RandChoices = []\n",
    "for F in FullDat:\n",
    "    RandChoices.append(np.random.choice(len(F),200,replace=False))\n",
    "\n",
    "SubDatUnPerm = np.vstack([F[R] for F,R in zip(FullDat,RandChoices)])\n",
    "SubDatFull = np.vstack([F[R][:,P] for F,P,R in zip(FullDat,Perms,RandChoices)])\n",
    "SubDat20 = np.vstack([F[R][:,np.array(I)[P]] for F,I,P,R in zip(FullDat,Indices20,Perms20,RandChoices)])\n",
    "SubDat7 = np.vstack([F[R][:,np.array(I)[P]] for F,I,P,R in zip(FullDat,Indices7,Perms7,RandChoices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc24e6-b767-4aa1-848c-064a8e42d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBIGuessFull = []\n",
    "for S in tqdm.tqdm(SubDatFull):\n",
    "    SBIGuessFull.append(posteriorFull.sample((InferSamples,), x=S,show_progress_bars=False).mean(axis=0))\n",
    "SBIGuessFull = np.vstack(SBIGuessFull)\n",
    "SBIGuessFull_2 = np.vstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(S))) for S in SBIGuessFull])\n",
    "\n",
    "SBIGuess20 = []\n",
    "for S in tqdm.tqdm(SubDat20):\n",
    "    SBIGuess20.append(posterior20.sample((InferSamples,), x=S,show_progress_bars=False).mean(axis=0))\n",
    "SBIGuess20 = np.vstack(SBIGuess20)\n",
    "SBIGuess20_2 = np.vstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(S))) for S in SBIGuess20])\n",
    "\n",
    "SBIGuess7 = []\n",
    "for S in tqdm.tqdm(SubDat7):\n",
    "    SBIGuess7.append(posterior7.sample((InferSamples,), x=S,show_progress_bars=False).mean(axis=0))\n",
    "SBIGuess7 = np.vstack(SBIGuess7)\n",
    "SBIGuess7_2 = np.vstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(S))) for S in SBIGuess7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccb34c-04ed-4c80-ac3a-5d0d62efdea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "WLSGuess = []\n",
    "for i,gtab in enumerate(gTabsF):\n",
    "    tenmodel = dti.TensorModel(gtab,return_S0_hat = True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(SubDatUnPerm[i*200:(i+1)*200])\n",
    "    WLSGuess.append([mat_to_vals(t) for t in tenfit.quadratic_form])\n",
    "WLSGuess = np.vstack(WLSGuess)\n",
    "\n",
    "WLSGuess20 = []\n",
    "for i,gtab in enumerate(gTabsPerm20):\n",
    "    tenmodel = dti.TensorModel(gtab,return_S0_hat = True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(SubDat20[i*200:(i+1)*200])\n",
    "    WLSGuess20.append([mat_to_vals(t) for t in tenfit.quadratic_form])\n",
    "WLSGuess20 = np.vstack(WLSGuess20)\n",
    "\n",
    "WLSGuess7 = []\n",
    "for i,gtab in enumerate(gTabsPerm7):\n",
    "    tenmodel = dti.TensorModel(gtab,return_S0_hat = True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(SubDat7[i*200:(i+1)*200])\n",
    "    WLSGuess7.append([mat_to_vals(t) for t in tenfit.quadratic_form])\n",
    "WLSGuess7 = np.vstack(WLSGuess7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a572bf-567a-47c2-afae-174fbeed8660",
   "metadata": {},
   "outputs": [],
   "source": [
    "Errors20 = np.vstack([ErrorsMDFA(vals_to_mat(G),vals_to_mat(T)) for G,T in zip(SBIGuess20,SBIGuessFull)]) # SBI Truth\n",
    "Errors7 = np.vstack([ErrorsMDFA(vals_to_mat(G),vals_to_mat(T)) for G,T in zip(SBIGuess7,SBIGuessFull)])\n",
    "ErrorsFullW = np.vstack([ErrorsMDFA(vals_to_mat(G),vals_to_mat(T)) for G,T in zip(WLSGuess,SBIGuessFull)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b0cf1-f4af-46b0-bbf6-d54ea65d05b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Percs20 = np.vstack([PercsMDFA(vals_to_mat(G),vals_to_mat(T)) for G,T in zip(SBIGuess20,SBIGuessFull)]) # SBI Truth\n",
    "Percs7 = np.vstack([PercsMDFA(vals_to_mat(G),vals_to_mat(T)) for G,T in zip(SBIGuess7,SBIGuessFull)])\n",
    "PercsFullW = np.vstack([PercsMDFA(vals_to_mat(G),vals_to_mat(T)) for G,T in zip(WLSGuess,SBIGuessFull)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b52391a-22e3-4628-acac-da77cb479f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Percs20W_WLS = np.vstack([PercsMDFA(vals_to_mat(G),vals_to_mat(T)) for G,T in zip(WLSGuess20,WLSGuess)])\n",
    "Percs7W_WLS = np.vstack([PercsMDFA(vals_to_mat(G),vals_to_mat(T)) for G,T in zip(WLSGuess7,WLSGuess)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c418b1b3-880a-497a-a0de-644c714d2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Errors20W_WLS = np.vstack([ErrorsMDFA(vals_to_mat(G),vals_to_mat(T)) for G,T in zip(WLSGuess20,WLSGuess)])\n",
    "Errors7W_WLS = np.vstack([ErrorsMDFA(vals_to_mat(G),vals_to_mat(T)) for G,T in zip(WLSGuess7,WLSGuess)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b5134-3d17-4e75-9372-f42e3e5d81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "fig,ax = plt.subplots()\n",
    "box_plot(ErrorsFullW[:,i],'black', 'gray',positions=[0],showfliers=False,widths=0.3,)\n",
    "box_plot(np.array([Errors20[:,i],Errors7[:,i]]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.7,1],\n",
    "         showfliers=False,widths=0.3)    \n",
    "box_plot(np.array([Errors20W_WLS[:,i],Errors7W_WLS[:,i]]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),'/',positions=[1.8,2.1],\n",
    "         showfliers=False,widths=0.3)\n",
    "\n",
    "plt.xticks([0,0.7,1,1.8,2.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "if(i == 0):\n",
    "    handles = [\n",
    "        mpatches.Patch(facecolor=np.clip(SBIFit+0.2,0,1),edgecolor=SBIFit-0.2, label='Truth: SBI-full'),  # Adjust color as per the actual plot color\n",
    "    ]\n",
    "    plt.legend(handles=handles,loc=2, fontsize=32,bbox_to_anchor=(0.15,1.2),columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "if(i == 1):\n",
    "    handles = [\n",
    "            mpatches.Patch(facecolor=np.clip(WLSFit+0.2,0,1),edgecolor=WLSFit-0.2,hatch='/', label='Truth: NLLS-full') # Adjust color as per the actual plot color\n",
    "            ]\n",
    "    plt.legend(handles=handles,loc=2, fontsize=32,bbox_to_anchor=(0.15,1.2),columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "if Save: plt.savefig(FigLoc+'ErrorsHCP_'+str(i)+'.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "box_plot(PercsFullW[:,i],'black', 'gray',positions=[0],showfliers=False,widths=0.3,)\n",
    "box_plot(np.array([Percs20[:,i],Percs7[:,i]]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.7,1],\n",
    "         showfliers=False,widths=0.3)    \n",
    "box_plot(np.array([Percs20W_WLS[:,i],Percs7W_WLS[:,i]]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),'/',positions=[1.8,2.1],\n",
    "         showfliers=False,widths=0.3)\n",
    "\n",
    "plt.xticks([0,0.7,1,1.8,2.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "if(i == 0):\n",
    "    handles = [\n",
    "        mpatches.Patch(facecolor=np.clip(SBIFit+0.2,0,1),edgecolor=SBIFit-0.2, label='Truth: SBI-full'),  # Adjust color as per the actual plot color\n",
    "    ]\n",
    "    plt.legend(handles=handles,loc=2, fontsize=32,bbox_to_anchor=(0.15,1.2),columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "\n",
    "handles = [\n",
    "        mpatches.Patch(facecolor=np.clip(WLSFit+0.2,0,1),edgecolor=WLSFit-0.2,hatch='/', label='Truth: NLLS-full') # Adjust color as per the actual plot color\n",
    "        ]\n",
    "plt.legend(handles=handles,loc=2, fontsize=32,bbox_to_anchor=(0.15,1.2),columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.ylim([0,1])\n",
    "plt.axhspan(0,0.33,color='gray',alpha=0.25)\n",
    "plt.axhline(0.33,ls='--',color='k')\n",
    "if Save: plt.savefig(FigLoc+'PercsHCP_'+str(i)+'.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99d873-fb8f-4e60-9db9-9da8f2a2506c",
   "metadata": {},
   "source": [
    "## f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358588e6-774e-4e83-b6d5-fc9883c4e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "fig,ax = plt.subplots()\n",
    "box_plot(ErrorsFullW[:,i],'black', 'gray',positions=[0],showfliers=False,widths=0.3,)\n",
    "box_plot(np.array([Errors20[:,i],Errors7[:,i]]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.7,1],\n",
    "         showfliers=False,widths=0.3)    \n",
    "box_plot(np.array([Errors20W_WLS[:,i],Errors7W_WLS[:,i]]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),'/',positions=[1.8,2.1],\n",
    "         showfliers=False,widths=0.3)\n",
    "\n",
    "plt.xticks([0,0.7,1,1.8,2.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "handles = [\n",
    "    mpatches.Patch(facecolor=np.clip(SBIFit+0.2,0,1),edgecolor=SBIFit-0.2, label='Truth: SBI-full'),  # Adjust color as per the actual plot color\n",
    "]\n",
    "plt.legend(handles=handles,loc=2, fontsize=32,bbox_to_anchor=(0.15,1.2),columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "\n",
    "\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "if Save: plt.savefig(FigLoc+'ErrorsHCP_'+str(i)+'.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "box_plot(PercsFullW[:,i],'black', 'gray',positions=[0],showfliers=False,widths=0.3,)\n",
    "box_plot(np.array([Percs20[:,i],Percs7[:,i]]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.7,1],\n",
    "         showfliers=False,widths=0.3)    \n",
    "box_plot(np.array([Percs20W_WLS[:,i],Percs7W_WLS[:,i]]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),'/',positions=[1.8,2.1],\n",
    "         showfliers=False,widths=0.3)\n",
    "\n",
    "plt.xticks([0,0.7,1,1.8,2.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "if(i == 0):\n",
    "    handles = [\n",
    "        mpatches.Patch(facecolor=np.clip(SBIFit+0.2,0,1),edgecolor=SBIFit-0.2, label='Truth: SBI-full'),  # Adjust color as per the actual plot color\n",
    "    ]\n",
    "    plt.legend(handles=handles,loc=2, fontsize=32,bbox_to_anchor=(0.15,1.2),columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "\n",
    "handles = [\n",
    "        mpatches.Patch(facecolor=np.clip(WLSFit+0.2,0,1),edgecolor=WLSFit-0.2,hatch='/', label='Truth: NLLS-full') # Adjust color as per the actual plot color\n",
    "        ]\n",
    "plt.legend(handles=handles,loc=2, fontsize=32,bbox_to_anchor=(0.15,1.2),columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.ylim([0,1])\n",
    "plt.axhspan(0,0.33,color='gray',alpha=0.25)\n",
    "plt.axhline(0.33,ls='--',color='k')\n",
    "if Save: plt.savefig(FigLoc+'PercsHCP_'+str(i)+'.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47abe78-d1cc-4af0-82e1-38a49b53d5dc",
   "metadata": {},
   "source": [
    "# Fig 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b785687-7760-4a5c-b572-814d14641324",
   "metadata": {},
   "outputs": [],
   "source": [
    "InferSamples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d414e5c9-86c7-472e-88e6-470d239c39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_5/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794079f9-1bf8-481c-8b96-2af4415ae2d2",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b02eed-32af-450e-8510-f7d0cfd6180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "_, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "\n",
    "\n",
    "data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "# Get the indices of True values\n",
    "true_indices = np.argwhere(mask)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = true_indices.min(axis=0)\n",
    "max_coords = true_indices.max(axis=0)\n",
    "\n",
    "maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "axial_middle = maskdata.shape[2] // 2\n",
    "maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf93df-9b51-4a39-a131-25622ba07181",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIHCPFull.pickle\"):\n",
    "    with open(f\"{network_path}/DKIHCPFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(4*3000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(4*1000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(4*3000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(4*3000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(4*3000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([4*13000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabExt.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabExt,S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    if not os.path.exists(f\"{network_path}/DKIHCPFull.pickle\"):\n",
    "        with open(f\"{network_path}/DKIHCPFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n",
    "\n",
    "ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "NoiseEst = np.zeros([62, 68 ,22])\n",
    "torch.manual_seed(10)\n",
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            posterior_samples_1 = posteriorFull.sample((InferSamples,), x=TestData4D[i,j,axial_middle,:],show_progress_bars=False)\n",
    "            NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7193684-88ca-4c15-b9e0-b0bd8cdfad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae35385-9e3c-4f70-9de9-e486cb622612",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK_SBIFull  = np.zeros([62, 68])\n",
    "AK_SBIFull  = np.zeros([62, 68])\n",
    "RK_SBIFull  = np.zeros([62, 68])\n",
    "MKT_SBIFull = np.zeros([62, 68])\n",
    "KFA_SBIFull = np.zeros([62, 68])\n",
    "for i in tqdm.tqdm(range(62)):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "        MK_SBIFull[i,j] = Metrics[0]\n",
    "        AK_SBIFull[i,j] = Metrics[1]\n",
    "        RK_SBIFull[i,j] = Metrics[2]\n",
    "        MKT_SBIFull[i,j] = Metrics[3]\n",
    "        KFA_SBIFull[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8828df-b482-4538-ae17-7c8738b719ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodelNL = dki.DiffusionKurtosisModel(gtabExt,fit_method='NLLS')\n",
    "dkifitNL = dkimodelNL.fit(TestData[:,:,:])\n",
    "MK_NLFull  = np.zeros([62, 68])\n",
    "AK_NLFull  = np.zeros([62, 68])\n",
    "RK_NLFull  = np.zeros([62, 68])\n",
    "MKT_NLFull = np.zeros([62, 68])\n",
    "KFA_NLFull = np.zeros([62, 68])\n",
    "for i in range(62):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt)\n",
    "        MK_NLFull[i,j] = Metrics[0]\n",
    "        AK_NLFull[i,j] = Metrics[1]\n",
    "        RK_NLFull[i,j] = Metrics[2]\n",
    "        MKT_NLFull[i,j] = Metrics[3]\n",
    "        KFA_NLFull[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56944c23-031e-4725-8799-2d3189d4d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "KFA_SBIFull[np.isnan(KFA_SBIFull)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314332f4-0efd-4e54-8d6c-2af60ef20a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKSBIFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'AKSBIFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'RKSBIFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MKT_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKTSBIFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((KFA_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'KFASBIFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a103498-9d2d-4c8b-aaa4-ff79157ab4ce",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7b7b0-dc2b-4949-a685-8ea77370e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKNLFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'AKNLFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'RKNLFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MKT_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKTNLFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((KFA_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'KFANLFull.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c558de52-056a-4e88-bfc4-60dca119fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for S,N in zip([MK_NLFull,AK_NLFull,RK_NLFull,MKT_NLFull,KFA_NLFull],[MK_SBIFull,AK_SBIFull,RK_SBIFull,MKT_SBIFull,KFA_SBIFull]):\n",
    "    print('==')\n",
    "    print(np.mean(np.abs(S*mask2[:,:,axial_middle]-N*mask2[:,:,axial_middle])))\n",
    "    ssim_noise = ssim(S, N, data_range=np.max([S.max(),N.max()])-np.min([S.min(),N.min()]))\n",
    "    print(ssim_noise)\n",
    "    Num = 2*np.abs(S*mask2[:,:,axial_middle]-N*mask2[:,:,axial_middle])\n",
    "    Den = np.abs(N*mask2[:,:,axial_middle])+np.abs(S*mask2[:,:,axial_middle])\n",
    "    print(np.nanmean(Num/Den))\n",
    "    print('==')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e9e12-717e-45c5-96f8-337da3144797",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d688d-6d34-4dba-b97d-68bffe27b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices7 = [0]+selected_indices\n",
    "\n",
    "bvalsHCP7_1 = bvalsHCP[selected_indices7]\n",
    "bvecsHCP7_1 = bvecsHCP[selected_indices7]\n",
    "\n",
    "i=3\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(14):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx = []\n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "true_indx = selected_indices7+[t+69 for t in true_indx]\n",
    "gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297ace3-8f33-4a4c-ba27-fd10f66968f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIHCPMin.pickle\"):\n",
    "    with open(f\"{network_path}/DKIHCPMin.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(4*30000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(4*10000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(4*30000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(4*30000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(4*30000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([520000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabHCP7.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabHCP7,S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    if not os.path.exists(f\"{network_path}/DKIHCPMin.pickle\"):\n",
    "        with open(f\"{network_path}/DKIHCPMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n",
    "ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "NoiseEst7 = np.zeros([62, 68 ,22])\n",
    "torch.manual_seed(10)\n",
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            posterior_samples_1 = posteriorFull.sample((InferSamples,), x=TestData4D[i,j,axial_middle,true_indx],show_progress_bars=False)\n",
    "            NoiseEst7[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29e4b9-40a8-43dc-ae04-a77948486a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst7[i,j]))),NoiseEst7[i,j,6:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94935529-ca31-41e9-90b1-fbbf4cd5ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK_SBI7  = np.zeros([62, 68])\n",
    "AK_SBI7  = np.zeros([62, 68])\n",
    "RK_SBI7  = np.zeros([62, 68])\n",
    "MKT_SBI7 = np.zeros([62, 68])\n",
    "KFA_SBI7 = np.zeros([62, 68])\n",
    "for i in tqdm.tqdm(range(62)):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "        MK_SBI7[i,j] = Metrics[0]\n",
    "        AK_SBI7[i,j] = Metrics[1]\n",
    "        RK_SBI7[i,j] = Metrics[2]\n",
    "        MKT_SBI7[i,j] = Metrics[3]\n",
    "        KFA_SBI7[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae32349-bb89-4108-93a0-c3f2cf19b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodelNL = dki.DiffusionKurtosisModel(gtabHCP7,fit_method='NLLS')\n",
    "dkifitNL = dkimodelNL.fit(TestData[:,:,true_indx])\n",
    "MK_NL7  = np.zeros([62, 68])\n",
    "AK_NL7  = np.zeros([62, 68])\n",
    "RK_NL7 = np.zeros([62, 68])\n",
    "MKT_NL7 = np.zeros([62, 68])\n",
    "KFA_NL7 = np.zeros([62, 68])\n",
    "for i in range(62):\n",
    "    for j in range(68):\n",
    "        Metrics = DKIMetrics(dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt)\n",
    "        MK_NL7[i,j] = Metrics[0]\n",
    "        AK_NL7[i,j] = Metrics[1]\n",
    "        RK_NL7[i,j] = Metrics[2]\n",
    "        MKT_NL7[i,j] = Metrics[3]\n",
    "        KFA_NL7[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08637ae-0db4-4c24-b053-7ab5414078ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "KFA_SBIFull[np.isnan(KFA_SBIFull)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e0c80-c2bb-48f3-8532-1377243cf248",
   "metadata": {},
   "outputs": [],
   "source": [
    "KFA_SBI7[np.isnan(KFA_SBI7)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd1310-3161-4b5f-844c-09fea495b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_SBI7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKSBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_SBI7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'AKSBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_SBI7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'RKSBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MKT_SBI7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKTSBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((KFA_SBI7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'KFASBI7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c379557-12f4-4619-84b9-4bf188df4a06",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf037902-4952-4fc0-9fc4-e8e43824303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_NL7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKNL7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_NL7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'AKNL7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_NL7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'RKNL7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MKT_NL7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKTNL7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((KFA_NL7*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'KFANL7.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f211213a-75b2-4285-a4cd-e10979afa45e",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c3479-4701-4e37-9043-22d1760ac3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = [0,1,2]\n",
    "data = np.abs((MK_SBIFull-MK_SBI7)*mask2[:,:,axial_middle]).T\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=1, vmax=2)\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(ticks=ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'MKDiffSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = np.abs((AK_SBIFull-AK_SBI7)*mask2[:,:,axial_middle]).T\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.max(data)/2, vmax=np.max(data))\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(ticks=ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'AKDiffSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = np.abs((RK_SBIFull-RK_SBI7)*mask2[:,:,axial_middle]).T\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=1,vmax=2)\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'RKDiffSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "ticks = [0,1,2]\n",
    "data = np.abs((MKT_SBIFull-MKT_SBI7)*mask2[:,:,axial_middle]).T\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=1, vmax=2)\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()#ticks=ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'MKTDiffSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = np.abs((KFA_SBIFull-KFA_SBI7)*mask2[:,:,axial_middle]).T\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.nanmax(data)/2, vmax=np.nanmax(data))\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'KFADiffSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d41de81-1b73-40ad-9cc2-b5fc9699a297",
   "metadata": {},
   "source": [
    "## f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8955987-8d1a-4732-90e4-bfe2588178ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs((MK_NLFull-MK_NL7)*mask2[:,:,axial_middle]).T\n",
    "plt.imshow(data,cmap='Reds',vmin=0,vmax=2)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar(ticks=[0,1,2])\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'MKDiffWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "dat = np.abs((AK_SBIFull-AK_SBI7)*mask2[:,:,axial_middle]).T\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.max(dat)/2, vmax=np.max(dat))\n",
    "data = np.abs((AK_NLFull-AK_NL7)*mask2[:,:,axial_middle]).T\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()#ticks=ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'AKDiffWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=1,vmax=2)\n",
    "data = np.abs((RK_NLFull-RK_NL7)*mask2[:,:,axial_middle]).T\n",
    "#ticks = [0, np.round(np.max(data),10)]  #Adjust the number of ticks as needed\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'RKDiffWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = np.abs((MKT_NLFull-MKT_NL7)*mask2[:,:,axial_middle]).T\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'MKTDiffWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "dat = np.abs((KFA_SBIFull-KFA_SBI7)*mask2[:,:,axial_middle]).T\n",
    "norm = TwoSlopeNorm(vmin=0, vcenter=np.nanmax(dat)/2, vmax=np.nanmax(dat))\n",
    "data = np.abs((KFA_NLFull-KFA_NL7)*mask2[:,:,axial_middle]).T\n",
    "ticks = [0, np.round(np.max(data),10)]  #Adjust the number of ticks as needed\n",
    "plt.imshow(data,cmap='Reds',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'KFADiffWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4ce7e-743b-4c48-8e39-90ae590731cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for S,N in zip([MK_SBI7,AK_SBI7,RK_SBI7,MKT_SBI7,KFA_SBI7],[MK_SBIFull,AK_SBIFull,RK_SBIFull,MKT_SBIFull,KFA_SBIFull]):\n",
    "    print('==')\n",
    "    print(np.mean(np.abs(S*mask2[:,:,axial_middle]-N*mask2[:,:,axial_middle])))\n",
    "    ssim_noise = ssim(S, N, data_range=N.max()-N.min())\n",
    "    print(ssim_noise)\n",
    "    Num = 2*np.abs(S*mask2[:,:,axial_middle]-N*mask2[:,:,axial_middle])\n",
    "    Den = np.abs(N*mask2[:,:,axial_middle])+np.abs(S*mask2[:,:,axial_middle])\n",
    "    print(np.nanmean(Num/Den))\n",
    "    print('==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ad512-e8f2-4f3f-87e2-a2f57039f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "for S,N in zip([MK_NL7,AK_NL7,RK_NL7,MKT_NL7,KFA_NL7],[MK_NLFull,AK_NLFull,RK_NLFull,MKT_NLFull,KFA_NLFull]):\n",
    "    print('==')\n",
    "    print(np.mean(np.abs(S*mask2[:,:,axial_middle]-N*mask2[:,:,axial_middle])))\n",
    "    ssim_noise = ssim(S, N, data_range=N.max()-N.min())\n",
    "    print(ssim_noise)\n",
    "    Num = 2*np.abs(S*mask2[:,:,axial_middle]-N*mask2[:,:,axial_middle])\n",
    "    Den = np.abs(N*mask2[:,:,axial_middle])+np.abs(S*mask2[:,:,axial_middle])\n",
    "    print(np.nanmean(Num/Den))\n",
    "    print('==')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84152946-277c-49e9-b554-bfa0ee803742",
   "metadata": {},
   "source": [
    "## g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ea91b-f3b5-4147-9e82-f9175a04d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrueIndxs = []\n",
    "gTabs7 = []\n",
    "for i in range(1,6):\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [1]\n",
    "    distance_matrix = squareform(pdist(bvecsHCP))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(5):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices7 = [0]+selected_indices\n",
    "    \n",
    "    bvalsHCP7_1 = bvalsHCP[selected_indices7]\n",
    "    bvecsHCP7_1 = bvecsHCP[selected_indices7]\n",
    "    \n",
    "    i=3\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    \n",
    "    temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "    temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "    distance_matrix = squareform(pdist(temp_bvecs))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(14):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "    bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "    \n",
    "    gtabHCP7 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "    \n",
    "    true_indx = []\n",
    "    for b in bvecsHCP7_3:\n",
    "        true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "    true_indx = selected_indices7+[t+69 for t in true_indx]\n",
    "    TrueIndxs.append(true_indx)\n",
    "    gTabs7.append(gtabHCP7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb585e-6456-4c01-8a56-5aa81f435084",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrueIndxs20 = []\n",
    "gTabs20 = []\n",
    "for i in range(1,6):\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [1]\n",
    "    distance_matrix = squareform(pdist(bvecsHCP))\n",
    "\n",
    "    temp_bvecs = bvecsHCP[bvalsHCP>0]\n",
    "    temp_bvals = bvalsHCP[bvalsHCP>0]\n",
    "    distance_matrix = squareform(pdist(temp_bvecs))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(18):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices7 = selected_indices\n",
    "    \n",
    "    bvalsHCP7_1 = np.insert(temp_bvals[selected_indices7],0,0)\n",
    "    bvecsHCP7_1 = np.insert(temp_bvecs[selected_indices7],0,[0,0,0],axis=0)\n",
    "    \n",
    "    i=3\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    \n",
    "    temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "    temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "    distance_matrix = squareform(pdist(temp_bvecs))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(27):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "    bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "    \n",
    "    gtabHCP20 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "    \n",
    "    true_indx_one = []\n",
    "    for b in bvecsHCP7_1:\n",
    "        true_indx_one.append(np.linalg.norm(b-bvecsHCP,axis=1).argmin())\n",
    "    true_indx = []        \n",
    "    for b in bvecsHCP7_3:\n",
    "        true_indx.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "    true_indx = true_indx_one+[t+69 for t in true_indx]\n",
    "    TrueIndxs20.append(true_indx)\n",
    "    gTabs20.append(gtabHCP20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c27000-4679-40a4-b71f-e0dc25b73405",
   "metadata": {},
   "outputs": [],
   "source": [
    "gTabsE = []\n",
    "\n",
    "for i in tqdm.tqdm(range(1,6)):\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "    bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "    bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "    gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "    \n",
    "    gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "    gTabsE.append(gtabExt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b127a32d-b24b-42ff-ab22-295656b33277",
   "metadata": {},
   "outputs": [],
   "source": [
    "gTabsPermE = []\n",
    "gTabsPerm7 = []\n",
    "gTabsPerm20 = []\n",
    "Perms     = []\n",
    "Perms7    = []\n",
    "Perms20    = []\n",
    "x = np.random.seed(116)\n",
    "for i in tqdm.tqdm(range(1,6)):\n",
    "    x = np.random.permutation(np.arange(138))\n",
    "    x7 = np.random.permutation(np.arange(22)) # Got to fix\n",
    "    x20 = np.random.permutation(np.arange(48)) # Got to fix\n",
    "    Perms.append(x)\n",
    "    Perms7.append(x7)\n",
    "    Perms20.append(x20)\n",
    "    gTabsPermE.append(gradient_table(gTabsE[i-1].bvals[x], gTabsE[i-1].bvecs[x]))\n",
    "    gTabsPerm7.append(gradient_table(gTabs7[i-1].bvals[x7], gTabs7[i-1].bvecs[x7]))\n",
    "    gTabsPerm20.append(gradient_table(gTabs20[i-1].bvals[x20], gTabs20[i-1].bvecs[x20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7bdee7-722f-4a25-9aa1-1dc3ecbe0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIMultiHCPFull.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiHCPFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(6000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(6000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(6000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(6000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([26000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    A  = np.random.choice(5,26000)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabsPermE[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabsPermE[A[i]],S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    A = np.array(A).reshape(len(A),1)\n",
    "    Par = np.hstack([DT,KT,S0])#,A])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiHCPFull.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiHCPFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e26efa-e62f-4611-9ab1-8fc2c7727fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIMultiHCPMin.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiHCPMin.pickle\", \"rb\") as handle:\n",
    "        posterior7 = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*6000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*2000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*6000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*6000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(2.5*6000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([65000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    A  = np.random.choice(5,65000)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabsPerm7[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabsPerm7[A[i]],S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    A = np.array(A).reshape(len(A),1)\n",
    "    Par = np.hstack([DT,KT,S0])#,A])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior7 = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiHCPMin.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiHCPMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3fac70-d36e-41e2-8300-01811bab71d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIMultiHCPMid.pickle\"):\n",
    "    with open(f\"{network_path}/DKIMultiHCPMid.pickle\", \"rb\") as handle:\n",
    "        posterior20 = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*6000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*2000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*6000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*6000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(2.5*6000))\n",
    "        \n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([65000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    A  = np.random.choice(5,650000)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gTabsPerm20[0].bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gTabsPerm20[A[i]],S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    A = np.array(A).reshape(len(A),1)\n",
    "    Par = np.hstack([DT,KT,S0])#,A])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posterior20 = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKIMultiHCPMid.pickle\"):\n",
    "        with open(f\"{network_path}/DKIMultiHCPMid.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior20, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40301c1-3023-4174-9846-1b1833d2251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FullDat = []\n",
    "for i in range(1,6):\n",
    "    fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "    fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    \n",
    "    data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "    data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "    # Get the indices of True values\n",
    "    true_indices = np.argwhere(mask)\n",
    "    \n",
    "    # Determine the minimum and maximum indices along each dimension\n",
    "    min_coords = true_indices.min(axis=0)\n",
    "    max_coords = true_indices.max(axis=0)\n",
    "    \n",
    "    maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "    \n",
    "    TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "    FlatTD = TestData.reshape(maskdata.shape[0]*maskdata.shape[1],138)\n",
    "    FlatTD = FlatTD[FlatTD[:,:69].sum(axis=-1)>0]\n",
    "    FlatTD = FlatTD[~np.array(FlatTD<0).any(axis=-1)]\n",
    "    FullDat.append(FlatTD)\n",
    "\n",
    "np.random.seed(1)\n",
    "RandChoices = []\n",
    "for F in FullDat:\n",
    "    RandChoices.append(np.random.choice(len(F),200,replace=False))\n",
    "    \n",
    "SubDatUnPerm = np.vstack([F[R] for F,R in zip(FullDat,RandChoices)])\n",
    "SubDatFull = np.vstack([F[R][:,P] for F,P,R in zip(FullDat,Perms,RandChoices)])\n",
    "SubDat20 = np.vstack([F[R][:,np.array(T)[P]] for F,T,P,R in zip(FullDat,TrueIndxs20,Perms20,RandChoices)])\n",
    "SubDat7 = np.vstack([F[R][:,np.array(T)[P]] for F,T,P,R in zip(FullDat,TrueIndxs,Perms7,RandChoices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c7b0b-acc7-48bb-a5d6-962c8589f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBIGuessFull = []\n",
    "for S in tqdm.tqdm(SubDatFull):\n",
    "    SBIGuessFull.append(posteriorFull.sample((InferSamples,), x=S,show_progress_bars=False).mean(axis=0))\n",
    "SBIGuessFull = np.vstack(SBIGuessFull)\n",
    "\n",
    "SBIGuess20 = []\n",
    "for S in tqdm.tqdm(SubDat20):\n",
    "    SBIGuess20.append(posterior20.sample((InferSamples,), x=S,show_progress_bars=False).mean(axis=0))\n",
    "SBIGuess20 = np.vstack(SBIGuess20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede1599-0f52-44bb-96bb-014a4a83a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBIGuess7 = []\n",
    "for S in tqdm.tqdm(SubDat7):\n",
    "    SBIGuess7.append(posterior7.sample((InferSamples,), x=S,show_progress_bars=False).mean(axis=0))\n",
    "SBIGuess7 = np.vstack(SBIGuess7)\n",
    "\n",
    "SBIGuessFull_2 = np.array([np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(S))),S[6:]]) for S in SBIGuessFull])\n",
    "SBIGuess7_2 = np.array([np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(S))),S[6:]]) for S in SBIGuess7])\n",
    "SBIGuess20_2 = np.array([np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(S))),S[6:]]) for S in SBIGuess20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454961d2-b3d7-48b7-87eb-04422eec060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WLSGuess = []\n",
    "for i,gtab in enumerate(gTabsE):\n",
    "    print(i)\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gtab,fit_method='NLLS')\n",
    "    tenfit = dkimodelNL.fit(SubDatUnPerm[i*200:(i+1)*200])\n",
    "    WLSGuess.append([np.hstack((d.lower_triangular(),d.kt)) for d in tenfit])\n",
    "WLSGuess = np.vstack(WLSGuess)\n",
    "\n",
    "\n",
    "WLSGuess20 = []\n",
    "for i,gtab in enumerate(gTabsPerm20):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gtab,fit_method='NLLS')\n",
    "    tenfit = dkimodelNL.fit(SubDat20[i*200:(i+1)*200])\n",
    "    WLSGuess20.append([np.hstack((d.lower_triangular(),d.kt)) for d in tenfit])\n",
    "WLSGuess20 = np.vstack(WLSGuess20)\n",
    "\n",
    "WLSGuess7 = []\n",
    "for i,gtab in enumerate(gTabsPerm7):\n",
    "    dkimodelNL = dki.DiffusionKurtosisModel(gtab,fit_method='NLLS')\n",
    "    tenfit = dkimodelNL.fit(SubDat7[i*200:(i+1)*200])\n",
    "    WLSGuess7.append([np.hstack((d.lower_triangular(),d.kt)) for d in tenfit])\n",
    "WLSGuess7 = np.vstack(WLSGuess7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a6565-f86c-4e7e-a43b-890bc8e37b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Errors20 = np.vstack([DKIErrors(G[:6],G[6:],T[:6],T[6:]) for G,T in zip(SBIGuess20_2,SBIGuessFull_2)]) # SBI Truth\n",
    "Errors7 = np.vstack([DKIErrors(G[:6],G[6:],T[:6],T[6:]) for G,T in zip(SBIGuess7_2,SBIGuessFull_2)])\n",
    "ErrorsFullW = np.vstack([DKIErrors(G[:6],G[6:],T[:6],T[6:]) for G,T in zip(WLSGuess,SBIGuessFull_2)])\n",
    "\n",
    "Errors20W_WLS = np.vstack([DKIErrors(G[:6],G[6:],T[:6],T[6:]) for G,T in zip(WLSGuess20,WLSGuess)])\n",
    "Errors7W_WLS = np.vstack([DKIErrors(G[:6],G[6:],T[:6],T[6:]) for G,T in zip(WLSGuess7,WLSGuess)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111c879-6e2f-46ee-ab9a-44680c788af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Percs20 = np.vstack([Percs(G[:6],G[6:],T[:6],T[6:]) for G,T in zip(SBIGuess20_2,SBIGuessFull_2)]) # SBI Truth\n",
    "Percs7 = np.vstack([Percs(G[:6],G[6:],T[:6],T[6:]) for G,T in zip(SBIGuess7_2,SBIGuessFull_2)])\n",
    "PercsFullW = np.vstack([Percs(G[:6],G[6:],T[:6],T[6:]) for G,T in zip(WLSGuess,SBIGuessFull_2)])\n",
    "\n",
    "Percs20W_WLS = np.vstack([Percs(G[:6],G[6:],T[:6],T[6:]) for G,T in zip(WLSGuess20,WLSGuess)])\n",
    "Percs7W_WLS = np.vstack([Percs(G[:6],G[6:],T[:6],T[6:]) for G,T in zip(WLSGuess7,WLSGuess)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d2c5a-6a24-44c9-bd6b-36dd6e8daf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    fig,ax = plt.subplots()\n",
    "    box_plot(PercsFullW[:,i],'black', 'gray',positions=[0],showfliers=False,widths=0.3,)\n",
    "    box_plot(np.array([Percs20[:,i],Percs7[:,i]]),SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[0.7,1],\n",
    "             showfliers=False,widths=0.3)    \n",
    "    box_plot(np.array([Percs20W_WLS[:,i],Percs7W_WLS[:,i]]),WLSFit-0.2, np.clip(WLSFit+0.2,0,1),'/',positions=[1.8,2.1],\n",
    "             showfliers=False,widths=0.3)\n",
    "    \n",
    "    plt.xticks([0,0.7,1,1.8,2.1],['Full','Mid','Min','Mid','Min'],fontsize=32,rotation=90)\n",
    "\n",
    "    if(i == 0):\n",
    "        handles = [\n",
    "            mpatches.Patch(facecolor=np.clip(SBIFit+0.2,0,1),edgecolor=SBIFit-0.2, label='Truth: SBI-full'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        plt.legend(handles=handles,loc=2, fontsize=32,bbox_to_anchor=(0.0,0.8),columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(i == 1):\n",
    "        handles = [\n",
    "                mpatches.Patch(facecolor=np.clip(WLSFit+0.2,0,1),edgecolor=WLSFit-0.2,hatch='/', label='Truth: NLLS-full') # Adjust color as per the actual plot color\n",
    "                ]\n",
    "        plt.legend(handles=handles,loc=2, fontsize=32,bbox_to_anchor=(0.0,0.8),columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.ylim([0,1])\n",
    "    plt.axhspan(0,0.33,color='gray',alpha=0.25)\n",
    "    plt.axhline(0.33,ls='--',color='k')\n",
    "    if Save: plt.savefig(FigLoc+'PercsHCP_'+str(i)+'.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e18650-1b9f-4183-968b-37a2bb03cba5",
   "metadata": {},
   "source": [
    "# Supplemental Fig 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958a748-9a23-4402-ba1e-0c0be6e35a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "InferSamples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3801716-05b9-4af7-8a5d-f1d45e89ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S1/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b68c1b-38ba-4a9e-9a55-0d142227b712",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dcf0ce-e7ae-4e56-8971-42d000ca782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial = HemiSphere(xyz=bvecs[1:])\n",
    "hsph_initial20 = HemiSphere(xyz=bvecs[1:20])\n",
    "hsph_initial7 = HemiSphere(xyz=bvecs[1:7])\n",
    "hsph_updated,potentials = disperse_charges(hsph_initial,5000)\n",
    "hsph_updated20,potentials = disperse_charges(hsph_initial20,5000)\n",
    "hsph_updated7,potentials = disperse_charges(hsph_initial7,5000)\n",
    "\n",
    "gtabSimF = gradient_table(np.array([0]+[1000]*64).squeeze(), np.vstack([[0,0,0],hsph_updated.vertices]))\n",
    "gtabSim20 = gradient_table(np.array([0]+[1000]*19).squeeze(), np.vstack([[0,0,0],hsph_updated20.vertices]))\n",
    "gtabSim7 = gradient_table(np.array([0]+[1000]*6).squeeze(), np.vstack([[0,0,0],hsph_updated7.vertices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b3bc9-5186-45de-9167-fe7986b01ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FullDat = []\n",
    "S0Full  = []\n",
    "DTIFull = []\n",
    "for i in tqdm.tqdm(range(1,6)):\n",
    "    fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "    bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "    bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "    \n",
    "    bvalsHCP = np.loadtxt(bvalloc)\n",
    "    bvecsHCP = np.loadtxt(bvecloc)\n",
    "    gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "    \n",
    "    data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                                 numpass=1, autocrop=True, dilate=2)\n",
    "    print('maskdata.shape (%d, %d, %d, %d)' % maskdata.shape)\n",
    "    \n",
    "    TestData = maskdata[:, :, axial_middle, :]\n",
    "    FlatTD = TestData.reshape(maskdata.shape[0]*maskdata.shape[1],69)\n",
    "    FlatTD = FlatTD[FlatTD.sum(axis=-1)>0]\n",
    "    FlatTD = FlatTD[~np.array(FlatTD<0).any(axis=-1)]\n",
    "    FullDat.append(FlatTD)\n",
    "    # Fit the tensor model to the DWI data with return_S0_hat=True\n",
    "    tenmodel = dti.TensorModel(gtabHCP, return_S0_hat=True,fit_method='NLLS')\n",
    "    tenfit = tenmodel.fit(FlatTD)\n",
    "    DTIHCP = tenfit.quadratic_form\n",
    "    DTIFull.append(DTIHCP)\n",
    "    # Get the estimated S0_hat values\n",
    "    S0HCP = tenfit.S0_hat\n",
    "    S0Full.append(S0HCP)\n",
    "DTIFull = np.concatenate(DTIFull)\n",
    "FullDat = np.concatenate(FullDat)\n",
    "S0Full = np.hstack(S0Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa5ef6-a06f-45be-9f2b-d8826c2b1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "Samples  = []\n",
    "DTISim = []\n",
    "S0Sim    = []\n",
    "# Define the lower and upper bounds\n",
    "\n",
    "lower_abs,upper_abs = -0.07,0.07\n",
    "lower_rest,upper_rest = -0.015,0.015\n",
    "lower_S0 = 25\n",
    "upper_S0 = 2000\n",
    "\n",
    "custom_prior = DTIPriorS0(lower_abs,upper_abs,lower_rest,upper_rest,lower_S0,upper_S0)\n",
    "prior, *_ = process_prior(custom_prior) \n",
    "\n",
    "params = prior.sample([5000])\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim.append(dt)\n",
    "    S0Sim.append(params[i,-1])\n",
    "    Samples.append([CustomSimulator(dt,gtabSimF, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples = np.array(Samples).squeeze()\n",
    "Samples = np.moveaxis(Samples, 0, -1)\n",
    "\n",
    "DTISim = np.array(DTISim)\n",
    "\n",
    "MDSim = [np.mean(np.linalg.eigh(B)[0]) for B in DTISim]\n",
    "MDHCP = [np.mean(np.linalg.eigh(B)[0]) for B in DTIFull]\n",
    "\n",
    "FASim = [FracAni(np.linalg.eigh(B)[0],m) for B,m in zip(DTISim,MDSim)]\n",
    "FAHCP = [FracAni(np.linalg.eigh(B)[0],m) for B,m in zip(DTIFull,MDHCP)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d5b6b-418d-400d-9dd3-2f6ee6f55064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a7176c-c965-4336-8219-30b36f8a14f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(S0Sim,density=True,stacked=True,alpha=0.75,label='Simulated',color=SBIFit)\n",
    "plt.hist(S0Full,density=True,stacked=True,alpha=0.75,label='HCP',color='gray')\n",
    "plt.legend(fontsize=32,loc=1,bbox_to_anchor=(1,1))\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=32)\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'S0Dist.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589eb927-dca9-4964-bb7d-978e6ca9845d",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932280ec-76ac-4ca5-ae9e-fabb83cd6ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(MDSim,density=True,stacked=True,label='Simulated samples',color=SBIFit)\n",
    "plt.hist(MDHCP,density=True,stacked=True,alpha=0.75,label='HPC subset',color='gray')\n",
    "plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.yticks(fontsize=32)\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'MDDist.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74040569-c9e1-4a56-a304-1cd6f998845f",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221d6e5-0a0d-4803-ac35-a250ca3ec7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(FASim,density=True,label='Simulated samples',color=SBIFit)\n",
    "plt.hist(FAHCP,density=True,alpha=0.75,label='HPC subset',color='gray')\n",
    "plt.yticks(fontsize=32)\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'FADist.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82722113-e886-4bb9-b8fb-fc72f92755f3",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af6507-b9a6-4949-b45f-7c368fb7b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(3,3,figsize=(12,12))\n",
    "ax = axs.ravel()\n",
    "ax[0].hist(DTISim[:,0,0],density=True,color=SBIFit)\n",
    "ax[1].hist(DTISim[:,0,1],density=True,color=SBIFit)\n",
    "ax[2].hist(DTISim[:,0,2],density=True,color=SBIFit)\n",
    "ax[4].hist(DTISim[:,1,1],density=True,color=SBIFit)\n",
    "ax[5].hist(DTISim[:,1,2],density=True,color=SBIFit)\n",
    "ax[-1].hist(DTISim[:,2,2],density=True,color=SBIFit)\n",
    "\n",
    "\n",
    "ax[0].hist(DTIFull[:,0,0],density=True,alpha=0.75,color='gray')\n",
    "ax[1].hist(DTIFull[:,0,1],density=True,alpha=0.75,color='gray')\n",
    "ax[2].hist(DTIFull[:,0,2],density=True,alpha=0.75,color='gray')\n",
    "ax[4].hist(DTIFull[:,1,1],density=True,alpha=0.75,color='gray')\n",
    "ax[5].hist(DTIFull[:,1,2],density=True,alpha=0.75,color='gray')\n",
    "ax[-1].hist(DTIFull[:,2,2],density=True,alpha=0.75,color='gray')\n",
    "ax[3].axis('off')\n",
    "ax[-2].axis('off')\n",
    "ax[-3].axis('off')\n",
    "\n",
    "for a in ax:\n",
    "    a.tick_params(axis='x', labelsize=32)\n",
    "    a.tick_params(axis='y', labelsize=32)\n",
    "    a.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'DTDist.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d809d11-fe9d-4629-90f7-ddc62701f3cd",
   "metadata": {},
   "source": [
    "# Supplemental Fig 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a0661-c403-43ea-a6d1-111c685258da",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S2/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d135e4-accf-455f-92f3-442bef8dfa12",
   "metadata": {},
   "source": [
    "## a-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330daecb-a855-4ada-9b5f-9c5381dcfe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DTIFilt[:,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[:,5],bins=30,density=True,color=WLSFit)\n",
    "plt.hist(DTISim[:,0,0],bins=30,density=True,alpha=0.5,color='gray')\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.0014,600,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Normal1.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "#DT_rest\n",
    "data = DTIFilt[:,1]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti2_fitted = stats.norm(loc=loc, scale=scale)\n",
    "\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.norm(loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[:,1],bins=30,density=True,color=WLSFit,label='HCP data')\n",
    "plt.hist(DTISim[:,1,0],bins=30,density=True,alpha=0.5,color='gray',label='DTI prior')\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit,label='stat. fit')\n",
    "plt.text(0.00011,2000,\"Normal, \\n mean = {:.2f},\\n S.D. = {:.2e} \\n\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "plt.legend(fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1,loc=1,bbox_to_anchor=(0.5,1))\n",
    "if Save: plt.savefig(FigLoc+'Normal2.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = DKIFilt[:,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "x4_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DKIFilt[:,0],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,x4_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(1,0.8,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Normal3.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Fitting R1\n",
    "data = DKIFilt[:,3]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "R1_fitted = stats.norm(loc,scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "plt.hist(DKIFilt[:,3],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,R1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.1,3,\"Normal, \\n mean = {:.2f},\\n S.D. = {:.2e} \\n\".format(shape,loc),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'Normal4.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fbe689-69d0-4824-9f53-9b5b88f5b079",
   "metadata": {},
   "source": [
    "## e-h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d4187-d809-48f6-824b-0a58ee501c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = TrueMets[:,-1]<0.3\n",
    "data = DTIFilt[mask,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[mask,5],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.0014,600,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'lowFA1.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "#DT_rest\n",
    "data = DTIFilt[mask,1]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti2_fitted = stats.norm(loc=loc, scale=scale)\n",
    "\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.norm(loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[mask,1],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.00011,2000,\"Normal, \\n mean = {:.2f},\\n S.D. = {:.2e} \\n\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'lowFA2.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = DKIFilt[mask,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "x4_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DKIFilt[mask,0],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,x4_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(1,0.8,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'lowFA3.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Fitting R1\n",
    "data = DKIFilt[mask,3]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "R1_fitted = stats.norm(loc,scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "plt.hist(DKIFilt[mask,3],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,R1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.05,3,\"Normal, \\n mean = {:.2f},\\n S.D. = {:.2e} \\n\".format(shape,loc),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'lowFA4.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b054f-6dda-48f3-9861-dc1057947a96",
   "metadata": {},
   "source": [
    "## i-l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8aa79-feec-4b85-8b63-48e5c04c0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = TrueMets[:,-1]>0.7\n",
    "data = DTIFilt[mask,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[mask,0],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.0008,1400,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'HighFA1.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "#DT_rest\n",
    "data = DTIFilt[mask,1]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti2_fitted = stats.norm(loc=loc, scale=scale)\n",
    "\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.norm(loc=loc, scale=scale)\n",
    "plt.hist(DTIFilt[mask,1],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,dti1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.00013,1600,\"Normal, \\n mean = {:.2f},\\n S.D. = {:.2e} \\n\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'HighFA2.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "data = DKIFilt[mask,0]\n",
    "shape,loc,scale = lognorm.fit(data)\n",
    "x4_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "# Compute the fitted PDF\n",
    "dti1_fitted = stats.lognorm(shape, loc=loc, scale=scale)\n",
    "plt.hist(DKIFilt[mask,0],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,x4_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(1.3,0.5,\"Lognormal, \\n shape = {:.2f}, \\n location = {:.2e} \\n scale = {:.2e}\".format(shape,loc,scale),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'HighFA3.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Fitting R1\n",
    "data = DKIFilt[mask,3]\n",
    "loc,scale = stats.norm.fit(data)\n",
    "R1_fitted = stats.norm(loc,scale)\n",
    "    \n",
    "# Generate x-values for plotting\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "\n",
    "plt.hist(DKIFilt[mask,3],bins=30,density=True,color=WLSFit)\n",
    "plt.plot(x,R1_fitted.pdf(x),lw=3,c=SBIFit)\n",
    "plt.text(0.3,1,\"Normal, \\n mean = {:.2f},\\n S.D. = {:.2e} \\n\".format(shape,loc),\n",
    "        fontsize=32)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=32)\n",
    "if Save: plt.savefig(FigLoc+'HighFA4.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af576c3-5642-463e-b549-ee4fa7a7e76e",
   "metadata": {},
   "source": [
    "## m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b85a0-6c6f-453b-9f39-31df9d6842d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j=0,0\n",
    "mask = (TrueMets[:,-1]<0.7)*(TrueMets[:,-1]>0.3)\n",
    "plt.scatter(DTIFilt[mask,i],DKIFilt[mask,j],color=np.clip(WLSFit-0.5,0,1),label='HCP data')\n",
    "mask = TrueMets[:,-1]>0.7\n",
    "plt.scatter(DTIFilt[mask,i],DKIFilt[mask,j],color=np.clip(WLSFit,0,1),marker='v'\n",
    "            ,label='HCP data (KFA$>$0.7)')\n",
    "mask = TrueMets[:,-1]<0.3\n",
    "plt.scatter(DTIFilt[mask,i],DKIFilt[mask,j],color=np.clip(WLSFit-0.3,0,1),marker='^'\n",
    "            ,label='HCP data (KFA$<$0.3)')\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.legend(fontsize=24,loc=1,bbox_to_anchor=(1.1,1.1),handlelength=0.4,handletextpad=0.4,markerscale=2)\n",
    "if Save: plt.savefig(FigLoc+'Scatter1Dat.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20aca88-7d7e-488e-a75c-9c02434e3c8f",
   "metadata": {},
   "source": [
    "## n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796e8c26-f229-4820-bc24-8413fb46caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,200)\n",
    "DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,200)\n",
    "DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,200)\n",
    "DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,200)\n",
    "DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,800)\n",
    "\n",
    "\n",
    "DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT1,KT1)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest1 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT2,KT2)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest2 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT3,KT3)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest3 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT4,KT4)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest4 = np.array(ParMets)\n",
    "\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT5,KT5)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest5 = np.array(ParMets)\n",
    "ParMets = []\n",
    "for d,k in tqdm.tqdm(zip(DT,KT)):\n",
    "    ParMets.append(DKIMetrics(d,k))\n",
    "ParTest = np.array(ParMets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9bd5a-3942-4b2d-a664-e710af07b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j=0,0\n",
    "mask = (ParTest[:,-1]<0.7)*(ParTest[:,-1]>0.3)\n",
    "plt.scatter(DT[mask,i],KT[mask,j],color=np.clip(SBIFit-0.5,0,1),label='Sim. data')\n",
    "mask = ParTest[:,-1]>0.7\n",
    "plt.scatter(DT[mask,i],KT[mask,j],color=np.clip(SBIFit+0.2,0,1),marker='v',label='Sim. data (KFA$>$0.7)')\n",
    "mask = ParTest[:,-1]<0.3\n",
    "plt.scatter(DT[mask,i],KT[mask,j],color=np.clip(SBIFit-0.3,0,1),marker='^',label='Sim. data (KFA$>$0.7)')\n",
    "plt.xlim((2.6217407288099334e-05, 0.004342572522996818))\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.legend(fontsize=24,loc=1,bbox_to_anchor=(1.1,1.1),handlelength=0.4,handletextpad=0.4,markerscale=2)\n",
    "if Save: plt.savefig(FigLoc+'Scatter1Sim.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23390ab8-d734-4579-a531-60c650c20fd5",
   "metadata": {},
   "source": [
    "## o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b61b59-167b-41c7-bbfc-df12ce3aa23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j=9,0\n",
    "mask = (TrueMets[:,-1]<0.7)*(TrueMets[:,-1]>0.3)\n",
    "plt.scatter(DKIFilt[mask,i],DKIFilt[mask,j],color=np.clip(WLSFit-0.5,0,1))\n",
    "mask = TrueMets[:,-1]>0.7\n",
    "plt.scatter(DKIFilt[mask,i],DKIFilt[mask,j],color=np.clip(WLSFit+0.2,0,1),marker='v')\n",
    "mask = TrueMets[:,-1]<0.3\n",
    "plt.scatter(DKIFilt[mask,i],DKIFilt[mask,j],color=np.clip(WLSFit-0.3,0,1),marker='^')\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "if Save: plt.savefig(FigLoc+'Scatter2Dat.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d40334-0607-4d53-89b2-a777259b19d2",
   "metadata": {},
   "source": [
    "## p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ab6a6-b013-4a25-8a6c-cbd5fc28e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j=9,0\n",
    "mask = (ParTest[:,-1]<0.7)*(ParTest[:,-1]>0.3)\n",
    "plt.scatter(KT[mask,i],KT[mask,j],color=np.clip(SBIFit-0.5,0,1))\n",
    "mask = ParTest[:,-1]>0.7\n",
    "plt.scatter(KT[mask,i],KT[mask,j],color=np.clip(SBIFit+0.2,0,1),marker='v')\n",
    "mask = ParTest[:,-1]<0.3\n",
    "plt.scatter(KT[mask,i],KT[mask,j],color=np.clip(SBIFit-0.3,0,1),marker='^')\n",
    "plt.xlim((-0.07662077262433849, 1.0282445059644276))\n",
    "plt.ylim((-0.5425279356172327, 4.178757181686705))\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "if Save: plt.savefig(FigLoc+'Scatter2Sim.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a2ca0-6bcc-45a2-a7ac-87e1e7ec456b",
   "metadata": {},
   "source": [
    "# Supplemental Fig 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81ee19-bcee-4743-b5b7-343339e3c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S3/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fece82-06d8-42d8-93b1-5d9b6db2f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimMid.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimMid.pickle\", \"rb\") as handle:\n",
    "        posterior20 = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSim20,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior20 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTISimMid.pickle\"):\n",
    "        with open(f\"{network_path}/DTISimMid.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior20, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc27b3-b17e-48f6-b74e-f5185445fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "Error20 = []\n",
    "NoiseApprox20 = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim20[i])\n",
    "        tObs = Samples20[k,:,i]\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSim20, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posterior20.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSim20,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApprox20.append(ENoise)\n",
    "    Error20.append(ErrorN2)\n",
    "\n",
    "NoiseApprox20 = np.array(NoiseApprox20)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15f5fc-86fd-4f28-8576-34f49cedd22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k,gtab,Samps,DTIS = 20,gtabSim20,Samples20,DTISim20\n",
    "tenmodel = dti.TensorModel(gtab,fit_method='NLLS')\n",
    "Error_n = []\n",
    "for S,Noise in zip(Samps,NoiseLevels):\n",
    "    Error = []\n",
    "    for i in range(500):\n",
    "        tenfit = tenmodel.fit(S[:,i])\n",
    "        tensor_vals = dti.lower_triangular(tenfit.quadratic_form)\n",
    "        DT_test = vals_to_mat(tensor_vals)\n",
    "        Error.append(Errors(DT_test,DTIS[i],gtab,Samps[0][:,i],S[:,i]))\n",
    "    Error_n.append(Error)\n",
    "Error_n = np.array(Error_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e604e-b904-44b4-87cb-c68ea28e15c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,4,figsize=(18,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(Error20).T,Errors_name)):\n",
    "    plt.sca(a) \n",
    "    bp = box_plot(E[:,1:].T,SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3,2.3,3.3,4.3],showfliers=False,widths=0.3)\n",
    "    bp2 = box_plot(Error_n[1:,:,ll],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    plt.sca(a)\n",
    "    if(ll>3):\n",
    "        plt.xlabel('SNR', fontsize=24)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    plt.yticks(fontsize=32)\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'SimDatDTIErrors1_20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "\n",
    "fig,axs = plt.subplots(1,4,figsize=(18,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(Error20).T[4:],Errors_name[4:])):\n",
    "    plt.sca(a) \n",
    "    bp = box_plot(E[:,1:].T,SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3,2.3,3.3,4.3],showfliers=False,widths=0.3)\n",
    "    bp2 = box_plot(Error_n[1:,:,ll+4],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    plt.sca(a)\n",
    "    if(ll>3):\n",
    "        plt.xlabel('SNR', fontsize=24)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    plt.yticks(fontsize=32)\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'SimDatDTIErrors2_20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63518d1-daa7-449b-824a-10a049e0584f",
   "metadata": {},
   "source": [
    "# Supplemental Fig 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc566e78-1e2c-4a4c-8150-f53533f9fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S4/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbcbd1-a293-4dba-9e03-402954b86243",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93adef-ef95-4c78-9484-bc22e83e58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "Samples  = []\n",
    "DTISim = []\n",
    "S0Sim    = []\n",
    "\n",
    "params = priorS0.sample([5000])\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim.append(dt)\n",
    "    S0Sim.append(params[i,-1])\n",
    "    Samples.append([CustomSimulator(dt,gtabSimF, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples = np.array(Samples).squeeze()\n",
    "Samples = np.moveaxis(Samples, 0, -1)\n",
    "\n",
    "Samples20  = []\n",
    "DTISim20 = []\n",
    "S0Sim20    = []\n",
    "\n",
    "params = priorS0.sample([5000])\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim20.append(dt)\n",
    "    S0Sim20.append(params[i,-1])\n",
    "    Samples20.append([CustomSimulator(dt,gtabSim20, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples20 = np.array(Samples20).squeeze()\n",
    "Samples20 = np.moveaxis(Samples20, 0, -1)\n",
    "\n",
    "Samples7  = []\n",
    "DTISim7 = []\n",
    "S0Sim7    = []\n",
    "\n",
    "params = priorS0.sample([5000])\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    dt = ComputeDTI(params[i])\n",
    "    dt = ForceLowFA(dt)\n",
    "    DTISim7.append(dt)\n",
    "    S0Sim7.append(params[i,-1])\n",
    "    Samples7.append([CustomSimulator(dt,gtabSim7, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "    \n",
    "Samples7 = np.array(Samples7).squeeze()\n",
    "Samples7 = np.moveaxis(Samples7, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60dfef-1fc0-47df-8b59-c6f2d27f73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimFull.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimFull.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSimF,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{save_path}/DTISimFull.pickle\"):\n",
    "        with open(f\"{save_path}/DTISimFull.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d536c-98f3-417c-8cb1-94ae931fc728",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "ErrorFull = []\n",
    "NoiseApproxFull = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim[i])\n",
    "        tObs = Samples[k,:,i]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSimF, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        mat_guess = clip_negative_eigenvalues(mat_guess)\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSimF,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApproxFull.append(ENoise)\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "NoiseApproxFull = np.array(NoiseApproxFull)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f91f0-535f-4a43-98ba-d0754cbfa186",
   "metadata": {},
   "outputs": [],
   "source": [
    "k,gtab,Samps,DTIS = 65,gtabSimF,Samples,DTISim\n",
    "tenmodel = dti.TensorModel(gtab,fit_method='NLLS')\n",
    "Error_n = []\n",
    "for S,Noise in zip(Samps,NoiseLevels):\n",
    "    Error = []\n",
    "    for i in range(500):\n",
    "        tenfit = tenmodel.fit(S[:,i])\n",
    "        tensor_vals = dti.lower_triangular(tenfit.quadratic_form)\n",
    "        DT_test = vals_to_mat(tensor_vals)\n",
    "        Error.append(Errors(DT_test,DTIS[i],gtab,Samps[0][:,i],S[:,i]))\n",
    "    Error_n.append(Error)\n",
    "Error_n = np.array(Error_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0850f-ab5a-48fb-a86f-b7f73dc57a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,6,figsize=(27,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(ErrorFull).T[2:],Errors_name[2:])):\n",
    "    plt.sca(a) \n",
    "    bp = box_plot(E[:,1:].T,SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3,2.3,3.3,4.3],showfliers=False,widths=0.3)\n",
    "    bp2 = box_plot(Error_n[1:,:,ll+2],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    plt.sca(a)\n",
    "    if(ll>3):\n",
    "        plt.xlabel('SNR', fontsize=24)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    plt.yticks(fontsize=32)\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    if(ll==1):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.05),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(ll==0):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=WLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'SimDatDTIErrors2.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337fb9d6-fe66-468d-9d77-260bf06836da",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08490819-b1bd-49af-b904-2125038bd4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DTISimMin.pickle\"):\n",
    "    with open(f\"{network_path}/DTISimMin.pickle\", \"rb\") as handle:\n",
    "        posterior7 = pickle.load(handle)\n",
    "else:\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorNoise.sample()\n",
    "        dt = ComputeDTI(params)\n",
    "        dt = ForceLowFA(dt)\n",
    "        a = params[-1]\n",
    "        Obs.append(CustomSimulator(dt,gtabSim7,200,a))\n",
    "        Par.append(np.hstack([mat_to_vals(dt),a]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorNoise)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior7 = inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTISimMin.pickle\"):\n",
    "        with open(f\"{network_path}/DTISimMin.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior7, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f27ab-7be6-415d-864e-b1f927cc1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "SNR = NoiseLevels\n",
    "Error7 = []\n",
    "NoiseApprox7 = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(500):\n",
    "        tparams = mat_to_vals(DTISim7[i])\n",
    "        tObs = Samples7[k,:,i]\n",
    "        mat_true = vals_to_mat(tparams)\n",
    "        evals_true,evecs_true = np.linalg.eigh(mat_true)\n",
    "        true_signal_dti = single_tensor(gtabSim7, S0=200, evals=evals_true, evecs=evecs_true,\n",
    "                           snr=None)\n",
    "        posterior_samples_1 = posterior7.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        mat_guess = vals_to_mat(np.array(posterior_samples_1.mean(axis=0)))\n",
    "        ErrorN2.append(Errors(mat_guess,mat_true,gtabSim7,true_signal_dti,tObs))\n",
    "        ENoise.append(posterior_samples_1[:,-1].mean())\n",
    "    NoiseApprox7.append(ENoise)\n",
    "    Error7.append(ErrorN2)\n",
    "\n",
    "NoiseApprox7 = np.array(NoiseApprox7)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7911f26-40c8-4651-8596-8fd01ecc6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "k,gtab,Samps,DTIS = 7,gtabSim7,Samples7,DTISim7\n",
    "tenmodel = dti.TensorModel(gtab,fit_method='NLLS')\n",
    "Error_n = []\n",
    "for S,Noise in zip(Samps,NoiseLevels):\n",
    "    Error = []\n",
    "    for i in range(500):\n",
    "        tenfit = tenmodel.fit(S[:,i])\n",
    "        tensor_vals = dti.lower_triangular(tenfit.quadratic_form)\n",
    "        DT_test = vals_to_mat(tensor_vals)\n",
    "        Error.append(Errors(DT_test,DTIS[i],gtab,Samps[0][:,i],S[:,i]))\n",
    "    Error_n.append(Error)\n",
    "Error_n = np.array(Error_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d072e4c-cd58-40ed-a3de-1b40bfcf3ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,6,figsize=(27,3))\n",
    "ax = axs.ravel()\n",
    "for ll,(a,E,t) in enumerate(zip(ax,np.array(Error7).T[2:],Errors_name[2:])):\n",
    "    plt.sca(a) \n",
    "    bp = box_plot(E[:,1:].T,SBIFit-0.2, np.clip(SBIFit+0.2,0,1),positions=[1.3,2.3,3.3,4.3],showfliers=False,widths=0.3)\n",
    "    bp2 = box_plot(Error_n[1:,:,ll+2],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    plt.sca(a)\n",
    "    if(ll>3):\n",
    "        plt.xlabel('SNR', fontsize=24)\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,], NoiseLevels[1:],fontsize=32)\n",
    "    ymax = max(bp2['whiskers'][1].get_ydata()[1],bp['whiskers'][1].get_ydata()[1])*1.2\n",
    "    plt.yticks(fontsize=32)\n",
    "    # Create custom legend handles\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    if(ll==1):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=SBIFit, lw=4, label='SBI'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.05),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "    if(ll==0):\n",
    "        handles = [\n",
    "            Line2D([0], [0], color=WLSFit, lw=4, label='NLLS'),  # Adjust color as per the actual plot color\n",
    "        ]\n",
    "        # Add the legenda\n",
    "        plt.legend(handles=handles,loc=2, bbox_to_anchor=(0,1.15),\n",
    "                   fontsize=36,columnspacing=0.3,handlelength=0.6,handletextpad=0.3)\n",
    "plt.tight_layout()\n",
    "if Save: plt.savefig(FigLoc+'SimDatDTIErrors2_7.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa34f3-e36a-4980-b8b9-a47b99065d3a",
   "metadata": {},
   "source": [
    "# Supplemental Fig 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52169c-f600-40a8-be70-20e63a46317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S5/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40953b36-523c-4d29-ab4a-2299740a7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimg_init, fbvals, fbvecs = get_fnames('small_64D')\n",
    "bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\n",
    "hsph_initial28 = HemiSphere(xyz=bvecs[1:29])\n",
    "hsph_initial20 = HemiSphere(xyz=bvecs[1:20])\n",
    "hsph_updated28,_ = disperse_charges(hsph_initial28,5000)\n",
    "hsph_updated20,_ = disperse_charges(hsph_initial20,5000)\n",
    "gtabSimSub = gradient_table(np.array([0]+[1000]*19+[3000]*28).squeeze(), np.vstack([[0,0,0],hsph_updated20.vertices,hsph_updated28.vertices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9def9-d7d4-472a-921f-8c52b5f57bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DKISimMid.pickle\"):\n",
    "    with open(f\"{network_path}/DKISimMid.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*60000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*20000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*60000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*60000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(2.5*60000))\n",
    "    \n",
    "    \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabSimSub.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabSimSub,200,np.random.rand()*30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>800).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKISimMid.pickle\"):\n",
    "        with open(f\"{network_path}/DKISimMid.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d9c2b4-85a8-4f17-a581-20d56900acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "j = 1\n",
    "vL = torch.tensor([0.2*j])\n",
    "vS = torch.tensor([0.01*j])  \n",
    "\n",
    "kk = np.random.randint(0,4)\n",
    "if(kk==0):\n",
    "    DT,KT = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],2,1)\n",
    "elif(kk==1):\n",
    "    DT,KT = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],2,1)\n",
    "elif(kk==2):\n",
    "    DT,KT = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],2,1)\n",
    "elif(kk==3):\n",
    "    DT,KT = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],2,1)\n",
    "\n",
    "tObs = CustomDKISimulator(np.squeeze(DT),np.squeeze(KT),gtabSimSub,200,20)\n",
    "tTrue = CustomDKISimulator(np.squeeze(DT),np.squeeze(KT),gtabSim,200,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea594d1a-e8b8-46d6-9b28-90c7dc7ac6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c578a3-d3fd-44bb-ac28-7284c8167d7e",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7423528-c4bf-44ef-8bfa-5837f2a9e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GuessDKI = posterior_samples_1.mean(axis=0)\n",
    "GuessSig = CustomDKISimulator(GuessDKI[:6],GuessDKI[6:],gtabSim,200)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k',label='true signal')\n",
    "plt.plot(GuessSig,lw=2,c=SBIFit,ls='--',label='SBI signal')\n",
    "plt.axis('off')\n",
    "plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1.09,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "plt.fill_betweenx(np.arange(0,500,50),0*np.ones(10),20*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.fill_betweenx(np.arange(0,500,50),64*np.ones(10),92*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.ylim(-9.996985449425491, 209.99985644997255)\n",
    "\n",
    "plt.savefig(FigLoc+'20ReconSBI.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5e543-7bad-45c7-acfd-3e24c83bae90",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01d2b1-eabd-41a0-8faa-8919e8b1c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "tenfit = dkimodel.fit(tObs)\n",
    "plt.subplots(figsize=(6,1))\n",
    "plt.plot(tTrue,lw=2,c='k',label='true signal')\n",
    "plt.plot(tenfit.predict(gtabSim,200),lw=2,c=WLSFit,ls='--',label='NLLS signal')\n",
    "plt.axis('off')\n",
    "plt.legend(ncols=2,loc=1,bbox_to_anchor =  (1.09,1.95),fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "plt.fill_betweenx(np.arange(0,500,50),0*np.ones(10),20*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.fill_betweenx(np.arange(0,500,50),64*np.ones(10),92*np.ones(10),color='gray',alpha=0.5)\n",
    "plt.ylim(-9.996985449425491, 209.99985644997255)\n",
    "plt.savefig(FigLoc+'20ReconWLS.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f93c9c-b4d2-40cd-8102-b9e2bd0541b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "Mets = []\n",
    "MetsSBI = []\n",
    "for i in tqdm.tqdm([20,10,5,2]):\n",
    "    m = []\n",
    "    m2 = []\n",
    "    for k in range(50):\n",
    "        tObs = CustomDKISimulator(np.squeeze(DT), np.squeeze(KT),gtabSimSub, S0=200, snr=i)#\n",
    "        dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        m.append(DKIMetrics(tenfit.lower_triangular(),tenfit.kt,False))\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessDKI = posterior_samples_1.mean(axis=0)\n",
    "        m2.append(DKIMetrics(GuessDKI[:6],GuessDKI[6:],False))\n",
    "    Mets.append(m)\n",
    "    MetsSBI.append(m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1294fe-2ec8-49ea-a0d5-3d4351ee9aaa",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33ae9f-a4bd-4121-82ec-2c6822545f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mets = np.array(Mets)\n",
    "MetsSBI = np.array(MetsSBI)\n",
    "fig,ax = plt.subplots(1,5,figsize=(22.5,3))\n",
    "for i in range(5):\n",
    "    plt.sca(ax[i])\n",
    "    viol_plot(Mets[:,:,i].T,WLSFit,)\n",
    "    viol_plot(MetsSBI[:,:,i].T,SBIFit,widths=0.3,positions=[1.3,2.3,3.3,4.3])\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.axhline(DKIMetrics(np.squeeze(DT),np.squeeze(KT),False)[i],lw=3,ls='--',c='k')\n",
    "    plt.yticks(fontsize=32)\n",
    "plt.savefig(FigLoc+'EgSigMetrics20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496d2ad-395d-4948-b9ab-1bf8be4e8356",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e91dc49-1daf-4e96-bbf3-22ab6c226425",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],1,40)\n",
    "DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],1,40)\n",
    "DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],1,40)\n",
    "DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],1,40)\n",
    "DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,40)\n",
    "\n",
    "SampsDT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "SampsKT = np.vstack([KT1,KT2,KT3,KT4,KT5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8df768-566a-4dc7-be32-e0dbf9202d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "Samples20  = []\n",
    "\n",
    "for Sd,Sk in zip(SampsDT,SampsKT):\n",
    "    Samples20.append([CustomDKISimulator(Sd,Sk,gtabSimSub, S0=200,snr=scale) for scale in NoiseLevels])\n",
    "\n",
    "Samples20 = np.array(Samples20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720048c-bd32-42b5-8a25-229928a85092",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "ErrorFull = []\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples20[i,k,:]\n",
    "        posterior_samples_1 = posteriorFull.sample((InferSamples,), x=tObs,show_progress_bars=False)\n",
    "        GuessSBI = posterior_samples_1.mean(axis=0)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(GuessSBI[:6],GuessSBI[6:],SampsDT[i],SampsKT[i]))\n",
    "    ErrorFull.append(ErrorN2)\n",
    "\n",
    "Error_s = []\n",
    "dkimodel = dki.DiffusionKurtosisModel(gtabSimSub,fit_method='NLLS')\n",
    "\n",
    "for k in tqdm.tqdm(range(5)):\n",
    "    ErrorN2 = []\n",
    "    ENoise = []\n",
    "    for i in range(200):\n",
    "        tObs = Samples20[i,k,:]#Simulator(bvals,bvecs,200,params,Noise)\n",
    "        tenfit = dkimodel.fit(tObs)\n",
    "        \n",
    "        ErrorN2.append(DKIErrors(tenfit.lower_triangular(),tenfit.kt,SampsDT[i],SampsKT[i]))\n",
    "    Error_s.append(ErrorN2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f6fbef-7d99-4e79-8a38-20401710df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorFull = np.array(ErrorFull)\n",
    "Error_s = np.array(Error_s)\n",
    "ErrorNames = ['MK Error', 'AK Error', 'RK Error', 'MKT Error', 'KFA Error']\n",
    "fig,ax = plt.subplots(1,5,figsize=(22.5,3))\n",
    "for i in range(5):\n",
    "    plt.sca(ax[i])\n",
    "    box_plot(Error_s[1:,:,i],WLSFit-0.2, np.clip(WLSFit+0.2,0,1),showfliers=False,widths=0.3)\n",
    "    box_plot(ErrorFull[1:,:,i],SBIFit-0.2, np.clip(SBIFit+0.2,0,1),showfliers=False,widths=0.3,positions=[1.3,2.3,3.3,4.3])\n",
    "    plt.xticks([1.15, 2.15, 3.15, 4.15,],[20,10,5,2],fontsize=32)\n",
    "    plt.gca().ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "    plt.grid(axis='y')\n",
    "    plt.yticks(fontsize=32)\n",
    "plt.savefig(FigLoc+'Errors20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f33129-d4d5-421d-a278-621ee3313ded",
   "metadata": {},
   "source": [
    "## e-g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d382e-e8f3-44c4-8d25-4deb5dfeb5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [1]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "\n",
    "temp_bvecs = bvecsHCP[bvalsHCP>0]\n",
    "temp_bvals = bvalsHCP[bvalsHCP>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(18):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices7 = selected_indices\n",
    "\n",
    "bvalsHCP7_1 = np.insert(temp_bvals[selected_indices7],0,0)\n",
    "bvecsHCP7_1 = np.insert(temp_bvecs[selected_indices7],0,[0,0,0],axis=0)\n",
    "\n",
    "i=3\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc)\n",
    "gtabHCP3 = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "\n",
    "temp_bvecs = bvecsHCP3[bvalsHCP3>0]\n",
    "temp_bvals = bvalsHCP3[bvalsHCP3>0]\n",
    "distance_matrix = squareform(pdist(temp_bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(27):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(temp_bvecs))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "bvalsHCP7_3 = temp_bvals[selected_indices]\n",
    "bvecsHCP7_3 = temp_bvecs[selected_indices]\n",
    "\n",
    "gtabHCP20 = gradient_table(np.hstack((bvalsHCP7_1,bvalsHCP7_3)), np.vstack((bvecsHCP7_1,bvecsHCP7_3)))\n",
    "\n",
    "true_indx_one = []\n",
    "for b in bvecsHCP7_1:\n",
    "    true_indx_one.append(np.linalg.norm(b-bvecsHCP,axis=1).argmin())\n",
    "true_indx20 = []        \n",
    "for b in bvecsHCP7_3:\n",
    "    true_indx20.append(np.linalg.norm(b-bvecsHCP3,axis=1).argmin())\n",
    "true_indx20 = true_indx_one+[t+69 for t in true_indx20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459cbae-182c-4812-a658-13865f8fb228",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if os.path.exists(f\"{network_path}/DKIHCPMid.pickle\"):\n",
    "    with open(f\"{network_path}/DKIHCPMid.pickle\", \"rb\") as handle:\n",
    "        posteriorFull = pickle.load(handle)\n",
    "else:\n",
    "    DT = []\n",
    "    KT = []\n",
    "    S0 = []\n",
    "    DT1,KT1 = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],12,int(2.5*60000))\n",
    "    DT2,KT2 = GenDTKT([DT1_lfa,DT2_lfa],[x4_lfa,R1_lfa,x2_lfa,R2_lfa],12,int(2.5*20000))\n",
    "    DT3,KT3 = GenDTKT([DT1_hfa,DT2_hfa],[x4_hfa,R1_hfa,x2_hfa,R2_hfa],12,int(2.5*60000))\n",
    "    DT4,KT4 = GenDTKT([DT1_ulfa,DT2_ulfa],[x4_ulfa,R1_ulfa,x2_ulfa,R2_ulfa],12,int(2.5*60000))\n",
    "    DT5,KT5 = GenDTKT([DT1_hak,DT2_hak],[x4_hak,R1_hak,x2_hak,R2_hak],12,int(2.5*60000))\n",
    "        \n",
    "    DT = np.vstack([DT1,DT2,DT3,DT4,DT5])\n",
    "    KT = np.vstack([KT1,KT2,KT3,KT4,KT5])\n",
    "    \n",
    "    S0Dist = BoxUniform(low=torch.tensor([lower_S0]), high=torch.tensor([upper_S0]))\n",
    "    \n",
    "    S0 = S0Dist.sample([650000])\n",
    "    \n",
    "    S0 = np.array(S0).reshape(len(S0),1)\n",
    "    \n",
    "    indx = np.arange(len(KT))\n",
    "    Obs = np.zeros([len(KT),len(gtabHCP20.bvecs)])\n",
    "    kk = 0\n",
    "    while len(indx)>0:\n",
    "        for i in tqdm.tqdm(indx): \n",
    "            Obs[i] = CustomDKISimulator(DT[i],KT[i],gtabHCP20,S0[i],np.random.rand()*20 + 30)\n",
    "        \n",
    "        indxNew = []\n",
    "        for i,O in enumerate(Obs):\n",
    "            if (O>4*np.array(S0[i])).any() or (O<0).any():\n",
    "                indxNew.append(i)\n",
    "        KT[indxNew] = KT[indxNew]/2\n",
    "        DT[indxNew] = GenDTKT([DT1_full,DT2_full],[x4_full,R1_full,x2_full,R2_full],kk,1)[0]\n",
    "    \n",
    "        indx = indxNew\n",
    "        kk+=1\n",
    "    Par = np.hstack([DT,KT,S0])\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par = torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs= 100)\n",
    "    posteriorFull = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    os.system('say \"Network done.\"')\n",
    "    if not os.path.exists(f\"{network_path}/DKIHCPMid.pickle\"):\n",
    "        with open(f\"{network_path}/DKIHCPMid.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posteriorFull, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af7d639-5c8f-475f-8cd1-0be159b61650",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "fdwi = './HCP_data/Pat'+str(i)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(i)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(i)+'/bvecs_1k.txt'\n",
    "\n",
    "fdwi3 = './HCP_data/Pat'+str(i)+'/diff_3k.nii.gz'\n",
    "bvalloc3 = './HCP_data/Pat'+str(i)+'/bvals_3k.txt'\n",
    "bvecloc3 = './HCP_data/Pat'+str(i)+'/bvecs_3k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "bvalsHCP3 = np.loadtxt(bvalloc3)\n",
    "bvecsHCP3 = np.loadtxt(bvecloc3)\n",
    "gtabHCP3 = gradient_table(bvalsHCP3, bvecsHCP3)\n",
    "\n",
    "gtabExt  = gradient_table(np.hstack((bvalsHCP,bvalsHCP3)), np.vstack((bvecsHCP,bvecsHCP3)))\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=False, dilate=2)\n",
    "_, mask2 = median_otsu(data, vol_idx=range(10, 50), median_radius=4,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "\n",
    "\n",
    "data3, affine, img = load_nifti(fdwi3, return_img=True)\n",
    "data3, affine = reslice(data3, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "# Get the indices of True values\n",
    "true_indices = np.argwhere(mask)\n",
    "\n",
    "# Determine the minimum and maximum indices along each dimension\n",
    "min_coords = true_indices.min(axis=0)\n",
    "max_coords = true_indices.max(axis=0)\n",
    "\n",
    "maskdata  = maskdata[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "axial_middle = maskdata.shape[2] // 2\n",
    "maskdata3 = data3[min_coords[0]:max_coords[0]+1,min_coords[1]:max_coords[1]+1,min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "TestData = np.concatenate([maskdata[:, :, axial_middle, :],maskdata3[:, :, axial_middle, :]],axis=-1)\n",
    "TestData4D = np.concatenate([maskdata,maskdata3],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0507825a-2fc7-45d4-b6a1-3d3a041c0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArrShape = TestData4D[:,:,axial_middle,0].shape\n",
    "NoiseEst = np.zeros([62, 68 ,22])\n",
    "torch.manual_seed(10)\n",
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            posterior_samples_1 = posteriorFull.sample((InferSamples,), x=TestData4D[i,j,axial_middle,true_indx20],show_progress_bars=False)\n",
    "            NoiseEst[i,j] = posterior_samples_1.mean(axis=0)\n",
    "os.system('say \"Finished sampling.\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a2b65-a2b2-4b90-94c7-bc121734148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(62):\n",
    "    for j in range(68):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,6:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb70632-ab92-4871-8f5b-264535210a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MK_SBIFull  = np.zeros([62, 68])\n",
    "AK_SBIFull  = np.zeros([62, 68])\n",
    "RK_SBIFull  = np.zeros([62, 68])\n",
    "MKT_SBIFull = np.zeros([62, 68])\n",
    "KFA_SBIFull = np.zeros([62, 68])\n",
    "for i in tqdm.tqdm(range(62)):\n",
    "    for j in range(68):\n",
    "        if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            Metrics = DKIMetrics(NoiseEst2[i,j][:6],NoiseEst2[i,j][6:21])\n",
    "            MK_SBIFull[i,j] = Metrics[0]\n",
    "            AK_SBIFull[i,j] = Metrics[1]\n",
    "            RK_SBIFull[i,j] = Metrics[2]\n",
    "            MKT_SBIFull[i,j] = Metrics[3]\n",
    "            KFA_SBIFull[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471bc6b8-d661-479c-a505-c97518051288",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkimodelNL = dki.DiffusionKurtosisModel(gtabHCP20,fit_method='NLLS')\n",
    "dkifitNL = dkimodelNL.fit(TestData[:,:,true_indx20])\n",
    "MK_NLFull  = np.zeros([62, 68])\n",
    "AK_NLFull  = np.zeros([62, 68])\n",
    "RK_NLFull  = np.zeros([62, 68])\n",
    "MKT_NLFull = np.zeros([62, 68])\n",
    "KFA_NLFull = np.zeros([62, 68])\n",
    "for i in range(62):\n",
    "    for j in range(68):\n",
    "        if(np.sum(TestData4D[i,j,axial_middle,:69],axis=-1) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            Metrics = DKIMetrics(dkifitNL[i,j].lower_triangular(),dkifitNL[i,j].kt)\n",
    "            MK_NLFull[i,j] = Metrics[0]\n",
    "            AK_NLFull[i,j] = Metrics[1]\n",
    "            RK_NLFull[i,j] = Metrics[2]\n",
    "            MKT_NLFull[i,j] = Metrics[3]\n",
    "            KFA_NLFull[i,j] = Metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3adcd9-19f1-451d-8168-cfe65c04a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "KFA_SBIFull[np.isnan(KFA_SBIFull)] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c72c8e-2405-49e9-8757-277e344ef67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKSBI20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'AKSBI20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'RKSBI20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MKT_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKTSBI20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((KFA_SBIFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'KFASBI20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575af5de-88e9-41e5-b0aa-59d38c37f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKNL20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((AK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'AKNL20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((RK_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'RKNL20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((MKT_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'MKTNL20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "tnorm = TwoSlopeNorm(vmin=-0,vcenter = 0.5,vmax=1)\n",
    "plt.imshow((KFA_NLFull*mask2[:,:,axial_middle]).T,norm=tnorm,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "if Save: plt.savefig(FigLoc+'KFANL20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4492f464-2463-40f0-8141-f670e0a8f56c",
   "metadata": {},
   "source": [
    "# Supplemental Fig 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5f5c98-c5a8-4bd0-bcd9-b95d89f6a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S6/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410c919-729e-4474-a493-71b4c798e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.hist(ParTest[:,i],density=True,range=[0,1],color=SBIFit,label='Simulated')\n",
    "    plt.hist(TrueMets[:,i],alpha=0.8,density=True,range=[0,1],color='gray',label='HCP')\n",
    "    if(i==0):\n",
    "        plt.legend(fontsize=32,columnspacing=0.3,handlelength=0.4,handletextpad=0.1)\n",
    "    if Save: plt.savefig(FigLoc+'EgMetricDKI_'+str(i)+'.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45214278-3a58-405f-a6ac-3743a7f03665",
   "metadata": {},
   "source": [
    "# Supplemental Fig 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28909426-c6b4-4dc8-9628-55758ac1e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './SBI_Weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd6427-9be2-43ce-997e-ac324752b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigLoc = image_path + 'Fig_S7/'\n",
    "if not os.path.exists(FigLoc):\n",
    "    os.makedirs(FigLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee080db6-0437-4e50-a1d7-4cc6660949ed",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3d8db-e493-4a26-91b9-cda52ec00823",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdwi = './HCP_data/Pat'+str(1)+'/diff_1k.nii.gz'\n",
    "bvalloc = './HCP_data/Pat'+str(1)+'/bvals_1k.txt'\n",
    "bvecloc = './HCP_data/Pat'+str(1)+'/bvecs_1k.txt'\n",
    "\n",
    "bvalsHCP = np.loadtxt(bvalloc)\n",
    "bvecsHCP = np.loadtxt(bvecloc)\n",
    "gtabHCP = gradient_table(bvalsHCP, bvecsHCP)\n",
    "\n",
    "data, affine, img = load_nifti(fdwi, return_img=True)\n",
    "data, affine = reslice(data, affine, (1.5,1.5,1.5), (2.5,2.5,2.5))\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10, 50), median_radius=3,\n",
    "                             numpass=1, autocrop=True, dilate=2)\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "distance_matrix = squareform(pdist(bvecsHCP))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(19):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecsHCP))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "\n",
    "bvalsHCP20 = bvalsHCP[selected_indices]\n",
    "bvecsHCP20 = bvecsHCP[selected_indices]\n",
    "gtabHCP20 = gradient_table(bvalsHCP20, bvecsHCP20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccffaf0-68ca-4ad4-bcca-646298249eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/DTIHCPMid.pickle\"):\n",
    "    with open(f\"{network_path}/DTIHCPMid.pickle\", \"rb\") as handle:\n",
    "        posterior20 = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    bvals = gtabHCP.bvals\n",
    "    bvecs = gtabHCP.bvecs\n",
    "    Obs = []\n",
    "    Par = []\n",
    "    for i in tqdm.tqdm(range(TrainingSamples)):\n",
    "        params = priorS0.sample()\n",
    "        dt = ComputeDTI(params[:-1])\n",
    "        if(np.random.rand()<0.2):\n",
    "            dt = ForceLowFA(dt)\n",
    "        Obs.append(CustomSimulator(dt,gtabHCP20,params[-1],np.random.rand()*30))\n",
    "        Par.append(np.hstack([mat_to_vals(ComputeDTI(params)),params[-1]]))\n",
    "    \n",
    "    Obs = np.array(Obs)\n",
    "    Par = np.array(Par)\n",
    "    Obs = torch.tensor(Obs).float()\n",
    "    Par= torch.tensor(Par).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE(prior=priorS0)\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior20= inference.build_posterior(density_estimator)\n",
    "    if not os.path.exists(f\"{network_path}/DTIHCPMid.pickle\"):\n",
    "        with open(f\"{network_path}/DTIHCPMid.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(posterior20, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892a341-8f16-456d-99fc-5ef32bf1ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArrShape = maskdata[:,:,axial_middle,0].shape\n",
    "NoiseEst = np.zeros([55,64,7])\n",
    "for i in tqdm.tqdm(range(ArrShape[0])):\n",
    "    for j in range(ArrShape[1]):\n",
    "        torch.manual_seed(10)\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            pass\n",
    "        else:\n",
    "            posterior_samples_1 = posterior20.sample((InferSamples,), x=maskdata[i,j,axial_middle,selected_indices],show_progress_bars=False)\n",
    "            NoiseEst[i,j] = np.array([histogram_mode(p) for p in posterior_samples_1.T])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea7a4f-917d-4769-ab65-b4ce221c550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 =  np.zeros_like(NoiseEst)\n",
    "for i in range(55):\n",
    "    for j in range(64):    \n",
    "        NoiseEst2[i,j] = np.hstack([mat_to_vals(clip_negative_eigenvalues(vals_to_mat(NoiseEst[i,j]))),NoiseEst[i,j,-1]])\n",
    "MD_SBI20 = np.zeros([55,64])\n",
    "FA_SBI20 = np.zeros([55,64])\n",
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        Eigs = np.linalg.eigh(vals_to_mat(NoiseEst2[i,j,:6]))[0]\n",
    "        MD_SBI20[i,j] = np.mean(Eigs)\n",
    "        FA_SBI20[i,j] = FracAni(Eigs,np.mean(Eigs))\n",
    "FA_SBI20[np.isnan(FA_SBI20)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceb0db2-8684-4368-bc34-cd5c132a88d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtab2 = gtabHCP20\n",
    "tenmodel = dti.TensorModel(gtab2,return_S0_hat = True,fit_method='NLLS')\n",
    "tenfit = tenmodel.fit(maskdata[:,:,axial_middle,selected_indices])\n",
    "FA20 = dti.fractional_anisotropy(tenfit.evals)\n",
    "MD20 = dti.mean_diffusivity(tenfit.evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db4950-9e80-4d4e-bd95-ab6f3df23030",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(MD_SBI20.T,cmap='gray')\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_MD_20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3751b8-2e70-470c-9b09-2c6c8c666bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MD20.T,cmap='gray',vmin=vmin, vmax=vmax)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_MD_20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969bf42-24fd-4b83-a036-4565fff8dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MD20.T-MD_SBI20.T\n",
    "norm = TwoSlopeNorm(vmin=np.min(data), vcenter=0, vmax=np.max(data))\n",
    "plt.imshow(data,cmap='seismic',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "ticks = [np.min(data), 0, np.max(data)]  # Adjust the number of ticks as needed\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'HCP_MD_Diff_20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30ef8a-510d-4040-b54b-e5a45b86a1d2",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49481fbb-6020-4155-92ff-ef5dd6925de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(55):\n",
    "    for j in range(64):\n",
    "        if(np.sum(maskdata[i,j,axial_middle,:]) == 0):\n",
    "            FA20[i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f67417-30ef-4ae5-9e16-38f82474a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(FA_SBI20.T,cmap='gray')\n",
    "plt.axis('off')\n",
    "#cbar = plt.colorbar()\n",
    "#cbar.formatter.set_powerlimits((0, 0))\n",
    "vmin, vmax = img.get_clim()\n",
    "if Save: plt.savefig(FigLoc+'HCP_SBI_FA_20.pdf',format='pdf',bbox_inches='tight',transparent=True)\n",
    "plt.show()\n",
    "plt.imshow(FA20.T,cmap='gray',vmin=vmin, vmax=vmax)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'HCP_WLS_FA_20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d5c22-ca1b-4a97-9116-96149e3924a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import TwoSlopeNorm\n",
    "data = FAFull.T-FA_SBIFull.T\n",
    "norm = TwoSlopeNorm(vmin=np.min(data),vcenter=0, vmax=np.max(data))\n",
    "plt.imshow(data,cmap='seismic',norm=norm)\n",
    "plt.axis('off')\n",
    "cbar = plt.colorbar()\n",
    "ticks = [np.min(data), 0, np.max(data)]  # Adjust the number of ticks as needed\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "if Save: plt.savefig(FigLoc+'HCP_FA_Diff_20.pdf',format='pdf',bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35447299-9e6a-4136-b108-8bb9c124f6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
